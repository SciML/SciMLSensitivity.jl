<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Partial Differential Equation (PDE) Constrained Optimization Â· DiffEqSensitivity.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://sensitivity.sciml.ai/stable/pde_fitting/pde_constrained/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DiffEqSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DiffEqSensitivity.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Differentiating Ordinary Differential Equations (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation</a></li><li><a class="tocitem" href="../../ad_examples/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../ad_examples/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../ad_examples/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Fitting Ordinary Differential Equation (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ode_fitting/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../ode_fitting/stiff_ode_fit/">Parameter Estimation on Highly Stiff Systems</a></li><li><a class="tocitem" href="../../ode_fitting/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../../ode_fitting/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../ode_fitting/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../../ode_fitting/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../../ode_fitting/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Neural Ordinary Differential Equation (Neural ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../../neural_ode/mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../../neural_ode/mnist_conv_neural_ode/">Convolutional Neural ODE MNIST Classifier on GPU</a></li><li><a class="tocitem" href="../../neural_ode/GPUs/">Neural ODEs on GPUs</a></li><li><a class="tocitem" href="../../neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../../neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Stochastic Differential Equation (SDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sde_fitting/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../sde_fitting/neural_sde/">Neural Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox"/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Delay Differential Equation (DDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dde_fitting/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Differential-Algebraic Equation (DAE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dae_fitting/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox" checked/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Partial Differential Equation (PDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Partial Differential Equation (PDE) Constrained Optimization</a><ul class="internal"><li><a class="tocitem" href="#Step-by-step-Description"><span>Step-by-step Description</span></a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-9" type="checkbox"/><label class="tocitem" for="menuitem-2-9"><span class="docs-label">Hybrid and Jump Equation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../hybrid_jump_fitting/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-10" type="checkbox"/><label class="tocitem" for="menuitem-2-10"><span class="docs-label">Bayesian Estimation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li><li><a class="tocitem" href="../../bayesian/BayesianNODE_NUTS/">Bayesian Neural ODEs: NUTS</a></li><li><a class="tocitem" href="../../bayesian/BayesianNODE_SGLD/">Bayesian Neural ODEs: SGLD</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-11" type="checkbox"/><label class="tocitem" for="menuitem-2-11"><span class="docs-label">Optimal and Model Predictive Control Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../../optimal_control/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../../manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../../manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Partial Differential Equation (PDE) Tutorials</a></li><li class="is-active"><a href>Partial Differential Equation (PDE) Constrained Optimization</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/master/docs/src/pde_fitting/pde_constrained.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Partial-Differential-Equation-(PDE)-Constrained-Optimization"><a class="docs-heading-anchor" href="#Partial-Differential-Equation-(PDE)-Constrained-Optimization">Partial Differential Equation (PDE) Constrained Optimization</a><a id="Partial-Differential-Equation-(PDE)-Constrained-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Partial-Differential-Equation-(PDE)-Constrained-Optimization" title="Permalink"></a></h1><p>This example uses a prediction model to optimize the one-dimensional Heat Equation. (Step-by-step description below)</p><pre><code class="language-julia hljs">using DelimitedFiles,Plots
using DifferentialEquations, Optimization, OptimizationPolyalgorithms, OptimizationOptimJL

# Problem setup parameters:
Lx = 10.0
x  = 0.0:0.01:Lx
dx = x[2] - x[1]
Nx = size(x)

u0 = exp.(-(x.-3.0).^2) # I.C

## Problem Parameters
p        = [1.0,1.0]    # True solution parameters
xtrs     = [dx,Nx]      # Extra parameters
dt       = 0.40*dx^2    # CFL condition
t0, tMax = 0.0 ,1000*dt
tspan    = (t0,tMax)
t        = t0:dt:tMax;

## Definition of Auxiliary functions
function ddx(u,dx)
    &quot;&quot;&quot;
    2nd order Central difference for 1st degree derivative
    &quot;&quot;&quot;
    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]
end


function d2dx(u,dx)
    &quot;&quot;&quot;
    2nd order Central difference for 2nd degree derivative
    &quot;&quot;&quot;
    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]
end

## ODE description of the Physics:
function heat(u,p,t)
    # Model parameters
    a0, a1 = p
    dx,Nx = xtrs #[1.0,3.0,0.125,100]
    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)
end

# Testing Solver on linear PDE
prob = ODEProblem(heat,u0,tspan,p)
sol = solve(prob,Tsit5(), dt=dt,saveat=t);

plot(x, sol.u[1], lw=3, label=&quot;t0&quot;, size=(800,500))
plot!(x, sol.u[end],lw=3, ls=:dash, label=&quot;tMax&quot;)

ps  = [0.1, 0.2];   # Initial guess for model parameters
function predict(Î¸)
    Array(solve(prob,Tsit5(),p=Î¸,dt=dt,saveat=t))
end

## Defining Loss function
function loss(Î¸)
    pred = predict(Î¸)
    l = predict(Î¸)  - sol
    return sum(abs2, l), pred # Mean squared error
end

l,pred   = loss(ps)
size(pred), size(sol), size(t) # Checking sizes

LOSS  = []                              # Loss accumulator
PRED  = []                              # prediction accumulator
PARS  = []                              # parameters accumulator

callback = function (Î¸,l,pred) #callback function to observe training
  display(l)
  append!(PRED, [pred])
  append!(LOSS, l)
  append!(PARS, [Î¸])
  false
end

callback(ps,loss(ps)...) # Testing callback function

# Let see prediction vs. Truth
scatter(sol[:,end], label=&quot;Truth&quot;, size=(800,500))
plot!(PRED[end][:,end], lw=2, label=&quot;Prediction&quot;)

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x,p)-&gt;loss(x), adtype)

optprob = Optimization.OptimizationProblem(optf, ps)
res = Optimization.solve(optprob, PolyOpt(), callback = callback)
@show res.u # returns [0.999999999613485, 0.9999999991343996]</code></pre><h2 id="Step-by-step-Description"><a class="docs-heading-anchor" href="#Step-by-step-Description">Step-by-step Description</a><a id="Step-by-step-Description-1"></a><a class="docs-heading-anchor-permalink" href="#Step-by-step-Description" title="Permalink"></a></h2><h3 id="Load-Packages"><a class="docs-heading-anchor" href="#Load-Packages">Load Packages</a><a id="Load-Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Load-Packages" title="Permalink"></a></h3><pre><code class="language-julia hljs">using DelimitedFiles,Plots
using DifferentialEquations, DiffEqFlux</code></pre><h3 id="Parameters"><a class="docs-heading-anchor" href="#Parameters">Parameters</a><a id="Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Parameters" title="Permalink"></a></h3><p>First, we setup the 1-dimensional space over which our equations will be evaluated. <code>x</code> spans <strong>from 0.0 to 10.0</strong> in steps of <strong>0.01</strong>; <code>t</code> spans <strong>from 0.00 to 0.04</strong> in steps of <strong>4.0e-5</strong>.</p><pre><code class="language-julia hljs"># Problem setup parameters:
Lx = 10.0
x  = 0.0:0.01:Lx
dx = x[2] - x[1]
Nx = size(x)

u0 = exp.(-(x.-3.0).^2) # I.C

## Problem Parameters
p        = [1.0,1.0]    # True solution parameters
xtrs     = [dx,Nx]      # Extra parameters
dt       = 0.40*dx^2    # CFL condition
t0, tMax = 0.0 ,1000*dt
tspan    = (t0,tMax)
t        = t0:dt:tMax;</code></pre><p>In plain terms, the quantities that were defined are:</p><ul><li><code>x</code> (to <code>Lx</code>) spans the specified 1D space</li><li><code>dx</code> = distance between two points</li><li><code>Nx</code> = total size of space</li><li><code>u0</code> = initial condition</li><li><code>p</code> = true solution</li><li><code>xtrs</code> = convenient grouping of <code>dx</code> and <code>Nx</code> into Array</li><li><code>dt</code> = time distance between two points</li><li><code>t</code> (<code>t0</code> to <code>tMax</code>) spans the specified time frame</li><li><code>tspan</code> = span of <code>t</code></li></ul><h3 id="Auxiliary-Functions"><a class="docs-heading-anchor" href="#Auxiliary-Functions">Auxiliary Functions</a><a id="Auxiliary-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Auxiliary-Functions" title="Permalink"></a></h3><p>We then define two functions to compute the derivatives numerically. The <strong>Central Difference</strong> is used in both the 1st and 2nd degree derivatives.</p><pre><code class="language-julia hljs">## Definition of Auxiliary functions
function ddx(u,dx)
    &quot;&quot;&quot;
    2nd order Central difference for 1st degree derivative
    &quot;&quot;&quot;
    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]
end


function d2dx(u,dx)
    &quot;&quot;&quot;
    2nd order Central difference for 2nd degree derivative
    &quot;&quot;&quot;
    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]
end</code></pre><h3 id="Heat-Differential-Equation"><a class="docs-heading-anchor" href="#Heat-Differential-Equation">Heat Differential Equation</a><a id="Heat-Differential-Equation-1"></a><a class="docs-heading-anchor-permalink" href="#Heat-Differential-Equation" title="Permalink"></a></h3><p>Next, we setup our desired set of equations in order to define our problem.</p><pre><code class="language-julia hljs">## ODE description of the Physics:
function heat(u,p,t)
    # Model parameters
    a0, a1 = p
    dx,Nx = xtrs #[1.0,3.0,0.125,100]
    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)
end</code></pre><h3 id="Solve-and-Plot-Ground-Truth"><a class="docs-heading-anchor" href="#Solve-and-Plot-Ground-Truth">Solve and Plot Ground Truth</a><a id="Solve-and-Plot-Ground-Truth-1"></a><a class="docs-heading-anchor-permalink" href="#Solve-and-Plot-Ground-Truth" title="Permalink"></a></h3><p>We then solve and plot our partial differential equation. This is the true solution which we will compare to further on.</p><pre><code class="language-julia hljs"># Testing Solver on linear PDE
prob = ODEProblem(heat,u0,tspan,p)
sol = solve(prob,Tsit5(), dt=dt,saveat=t);

plot(x, sol.u[1], lw=3, label=&quot;t0&quot;, size=(800,500))
plot!(x, sol.u[end],lw=3, ls=:dash, label=&quot;tMax&quot;)</code></pre><h3 id="Building-the-Prediction-Model"><a class="docs-heading-anchor" href="#Building-the-Prediction-Model">Building the Prediction Model</a><a id="Building-the-Prediction-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Building-the-Prediction-Model" title="Permalink"></a></h3><p>Now we start building our prediction model to try to obtain the values <code>p</code>. We make an initial guess for the parameters and name it <code>ps</code> here. The <code>predict</code> function is a non-linear transformation in one layer using <code>solve</code>. If unfamiliar with the concept, refer to <a href="https://julialang.org/blog/2019/01/fluxdiffeq/">here</a>.</p><pre><code class="language-julia hljs">ps  = [0.1, 0.2];   # Initial guess for model parameters
function predict(Î¸)
    Array(solve(prob,Tsit5(),p=Î¸,dt=dt,saveat=t))
end</code></pre><h3 id="Train-Parameters"><a class="docs-heading-anchor" href="#Train-Parameters">Train Parameters</a><a id="Train-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Train-Parameters" title="Permalink"></a></h3><p>Training our model requires a <strong>loss function</strong>, an <strong>optimizer</strong> and a <strong>callback function</strong> to display the progress.</p><h4 id="Loss"><a class="docs-heading-anchor" href="#Loss">Loss</a><a id="Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Loss" title="Permalink"></a></h4><p>We first make our predictions based on the current values of our parameters <code>ps</code>, then take the difference between the predicted solution and the truth above. For the loss, we use the <strong>Mean squared error</strong>.</p><pre><code class="language-julia hljs">## Defining Loss function
function loss(Î¸)
    pred = predict(Î¸)
    l = predict(Î¸)  - sol
    return sum(abs2, l), pred # Mean squared error
end

l,pred   = loss(ps)
size(pred), size(sol), size(t) # Checking sizes</code></pre><h4 id="Optimizer"><a class="docs-heading-anchor" href="#Optimizer">Optimizer</a><a id="Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#Optimizer" title="Permalink"></a></h4><p>The optimizers <code>ADAM</code> with a learning rate of 0.01 and <code>BFGS</code> are directly passed in training (see below)</p><h4 id="Callback"><a class="docs-heading-anchor" href="#Callback">Callback</a><a id="Callback-1"></a><a class="docs-heading-anchor-permalink" href="#Callback" title="Permalink"></a></h4><p>The callback function displays the loss during training. We also keep a history of the loss, the previous predictions and the previous parameters with <code>LOSS</code>, <code>PRED</code> and <code>PARS</code> accumulators.</p><pre><code class="language-julia hljs">LOSS  = []                              # Loss accumulator
PRED  = []                              # prediction accumulator
PARS  = []                              # parameters accumulator

callback = function (Î¸,l,pred) #callback function to observe training
  display(l)
  append!(PRED, [pred])
  append!(LOSS, l)
  append!(PARS, [Î¸])
  false
end

callback(ps,loss(ps)...) # Testing callback function</code></pre><h3 id="Plotting-Prediction-vs-Ground-Truth"><a class="docs-heading-anchor" href="#Plotting-Prediction-vs-Ground-Truth">Plotting Prediction vs Ground Truth</a><a id="Plotting-Prediction-vs-Ground-Truth-1"></a><a class="docs-heading-anchor-permalink" href="#Plotting-Prediction-vs-Ground-Truth" title="Permalink"></a></h3><p>The scatter points plotted here are the ground truth obtained from the actual solution we solved for above. The solid line represents our prediction. The goal is for both to overlap almost perfectly when the PDE finishes its training and the loss is close to 0.</p><pre><code class="language-julia hljs"># Let see prediction vs. Truth
scatter(sol[:,end], label=&quot;Truth&quot;, size=(800,500))
plot!(PRED[end][:,end], lw=2, label=&quot;Prediction&quot;)</code></pre><h3 id="Train"><a class="docs-heading-anchor" href="#Train">Train</a><a id="Train-1"></a><a class="docs-heading-anchor-permalink" href="#Train" title="Permalink"></a></h3><p>The parameters are trained using <code>Optimization.solve</code> and adjoint sensitivities. The resulting best parameters are stored in <code>res</code> and <code>res.u</code> returns the parameters that minimizes the cost function.</p><pre><code class="language-julia hljs">adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x,p)-&gt;loss(x), adtype)

optprob = Optimization.OptimizationProblem(optf, ps)
res = Optimization.solve(optprob, PolyOpt(), callback = callback)
@show res.u # returns [0.999999999613485, 0.9999999991343996]</code></pre><p>We successfully predict the final <code>ps</code> to be equal to <strong>[0.999999999999975, 1.0000000000000213]</strong> vs the true solution of <code>p</code> = <strong>[1.0, 1.0]</strong></p><h3 id="Expected-Output"><a class="docs-heading-anchor" href="#Expected-Output">Expected Output</a><a id="Expected-Output-1"></a><a class="docs-heading-anchor-permalink" href="#Expected-Output" title="Permalink"></a></h3><pre><code class="language-julia hljs">153.74716386883014
153.74716386883014
150.31001476832154
146.91327105278128
143.55759898759374
140.24363496931753
136.97198347241257
133.7432151677673
130.55786524987215
127.4164319720337
124.31937540894337
121.26711645161134
118.26003603654628
115.29847461603427
112.3827318609633
109.51306659138356
106.68969692777314
103.9128006498965
101.18251574195561
98.4989411191655
95.8621374998964
93.27212842357801
90.7289013677808
88.23240896985287
85.7825703121191
83.37927225399383
81.02237079935475
78.71169247246975
76.44703568540336
74.22817209335733
72.05484791455291
69.92678520204167
67.84368308185877
65.80521891873633
63.81104944163126
61.860811797059554
59.95412455791812
58.090588663826914
56.26978832428055
54.491291863817686
52.75465253618253
51.05940929392087
49.405087540342564
47.79119984816457
46.217246667009626
44.68271701552145
43.18708916553295
41.729831330086824
40.310402328506555
38.928252289762675
37.58282331100446
36.27355015737786
34.99986094007708
33.76117780641769
32.55691762379305
31.386492661205562
30.249311268822595
29.144778544729924
28.07229699202965
27.031267166855155
26.0210883069299
25.041158938495613
24.09087747422764
23.169642780270983
22.276854715336583
21.411914664407295
20.57422602075309
19.76319467338999
18.978229434706996
18.218742481097735
17.48414972880479
16.773871221320032
16.087331469276343
15.423959781047255
14.78319057598673
14.164463661389682
13.567224508247984
12.990924508800399
12.435021204904853
11.898978515303417
11.382266943971572
10.884363779196345
10.404753276294088
9.942926832732251
9.49838314770057
9.070628379941386
8.659176278010788
8.263548334737965
7.883273889583058
7.517890250788576
7.1669427976429585
6.829985075319055
6.506578881124348
6.19629433688754
5.898709957062298
5.613412692266443
5.339997993203038
5.078069839645422
4.827240754206443
4.587131834698446
4.357372763056912
4.357372763056912
4.137601774726927
1.5254536025963588
0.0023707487489687726
4.933077457357198e-7
8.157805551380282e-14
1.6648677430325974e-16
res.u = [0.999999999999975, 1.0000000000000213]
2-element Array{Float64,1}:
 0.999999999999975
 1.0000000000000213</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../dae_fitting/physical_constraints/">Â« Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a><a class="docs-footer-nextpage" href="../../hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Monday 20 June 2022 15:15">Monday 20 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
