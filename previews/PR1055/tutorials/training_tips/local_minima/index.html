<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Strategies to Avoid Local Minima · SciMLSensitivity.jl</title><meta name="title" content="Strategies to Avoid Local Minima · SciMLSensitivity.jl"/><meta property="og:title" content="Strategies to Avoid Local Minima · SciMLSensitivity.jl"/><meta property="twitter:title" content="Strategies to Avoid Local Minima · SciMLSensitivity.jl"/><meta name="description" content="Documentation for SciMLSensitivity.jl."/><meta property="og:description" content="Documentation for SciMLSensitivity.jl."/><meta property="twitter:description" content="Documentation for SciMLSensitivity.jl."/><meta property="og:url" content="https://docs.sciml.ai/SciMLSensitivity/stable/tutorials/training_tips/local_minima/"/><meta property="twitter:url" content="https://docs.sciml.ai/SciMLSensitivity/stable/tutorials/training_tips/local_minima/"/><link rel="canonical" href="https://docs.sciml.ai/SciMLSensitivity/stable/tutorials/training_tips/local_minima/"/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">SciMLSensitivity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">SciMLSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><a class="tocitem" href="../../../getting_started/">Getting Started with SciMLSensitivity: Differentiating ODE Solutions</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../parameter_estimation_ode/">Parameter Estimation of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li><li><input class="collapse-toggle" id="menuitem-3-6" type="checkbox" checked/><label class="tocitem" for="menuitem-3-6"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Strategies to Avoid Local Minima</a><ul class="internal"><li><a class="tocitem" href="#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima"><span>Iterative Growing Of Fits to Reduce Probability of Bad Local Minima</span></a></li><li><a class="tocitem" href="#Training-both-the-initial-conditions-and-the-parameters-to-start"><span>Training both the initial conditions and the parameters to start</span></a></li></ul></li><li><a class="tocitem" href="../divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Ordinary Differential Equations (ODEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/ode/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../../../examples/ode/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../../../examples/ode/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../../../examples/ode/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox"/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Neural Ordinary Differential Equations (Neural ODE)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../../../examples/neural_ode/simplechains/">Faster Neural Ordinary Differential Equations with SimpleChains</a></li><li><a class="tocitem" href="../../../examples/neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Stochastic Differential Equations (SDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/sde/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../../examples/sde/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Delay Differential Equations (DDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/dde/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Partial Differential Equations (PDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/pde/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-6" type="checkbox"/><label class="tocitem" for="menuitem-4-6"><span class="docs-label">Hybrid and Jump Equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/hybrid_jump/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../../examples/hybrid_jump/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-7" type="checkbox"/><label class="tocitem" for="menuitem-4-7"><span class="docs-label">Bayesian Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-8" type="checkbox"/><label class="tocitem" for="menuitem-4-8"><span class="docs-label">Optimal and Model Predictive Control</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../examples/optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../../examples/optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../../../manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../../manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../../manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../../../manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Training Techniques and Tips</a></li><li class="is-active"><a href>Strategies to Avoid Local Minima</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Strategies to Avoid Local Minima</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/SciMLSensitivity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/SciMLSensitivity.jl/blob/master/docs/src/tutorials/training_tips/local_minima.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Strategies-to-Avoid-Local-Minima"><a class="docs-heading-anchor" href="#Strategies-to-Avoid-Local-Minima">Strategies to Avoid Local Minima</a><a id="Strategies-to-Avoid-Local-Minima-1"></a><a class="docs-heading-anchor-permalink" href="#Strategies-to-Avoid-Local-Minima" title="Permalink"></a></h1><p>Local minima can be an issue with fitting neural differential equations. However, there are many strategies to avoid local minima:</p><ol><li>Insert stochasticity into the loss function through minibatching</li><li>Weigh the loss function to allow for fitting earlier portions first</li><li>Iteratively grow the fit</li><li>Training both the initial conditions and parameters at first, followed by just the parameters</li></ol><h2 id="Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima"><a class="docs-heading-anchor" href="#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima">Iterative Growing Of Fits to Reduce Probability of Bad Local Minima</a><a id="Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima-1"></a><a class="docs-heading-anchor-permalink" href="#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima" title="Permalink"></a></h2><p>In this example, we will show how to use strategy (3) in order to increase the robustness of the fit. Let&#39;s start with the same neural ODE example we&#39;ve used before, except with one small twist: we wish to find the neural ODE that fits on <code>(0,5.0)</code>. Naively, we use the same training strategy as before:</p><pre><code class="language-julia hljs">using SciMLSensitivity
using OrdinaryDiffEq,
      ComponentArrays, SciMLSensitivity, Optimization, OptimizationOptimisers
using Lux, Plots, Random, Zygote

rng = Random.default_rng()
u0 = Float32[2.0; 0.0]
datasize = 30
tspan = (0.0f0, 5.0f0)
tsteps = range(tspan[1], tspan[2], length = datasize)

function trueODEfunc(du, u, p, t)
    true_A = Float32[-0.1 2.0; -2.0 -0.1]
    du .= ((u .^ 3)&#39;true_A)&#39;
end

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))

dudt2 = Lux.Chain(x -&gt; x .^ 3,
    Lux.Dense(2, 16, tanh),
    Lux.Dense(16, 2))

pinit, st = Lux.setup(rng, dudt2)
pinit = ComponentArray(pinit)

function neuralode_f(u, p, t)
    dudt2(u, p, st)[1]
end

function predict_neuralode(p)
    prob = ODEProblem(neuralode_f, u0, tspan, p)
    sol = solve(prob, Vern7(), saveat = tsteps, abstol = 1e-6, reltol = 1e-6)
    Array(sol)
end

function loss_neuralode(p)
    pred = predict_neuralode(p)
    loss = sum(abs2, (ode_data[:, 1:size(pred, 2)] .- pred))
    return loss, pred
end

iter = 0
callback = function (state, l, pred; doplot = false)
    global iter
    iter += 1

    println(l)
    if doplot
        # plot current prediction against data
        plt = scatter(tsteps[1:size(pred, 2)], ode_data[1, 1:size(pred, 2)], label = &quot;data&quot;)
        scatter!(plt, tsteps[1:size(pred, 2)], pred[1, :], label = &quot;prediction&quot;)
        display(plot(plt))
    end

    return false
end

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x, p) -&gt; loss_neuralode(x), adtype)

optprob = Optimization.OptimizationProblem(optf, pinit)
result_neuralode = Optimization.solve(optprob,
    Adam(0.05), callback = callback,
    maxiters = 300)

pred = predict_neuralode(result_neuralode.u)
plt = scatter(tsteps[1:size(pred, 2)], ode_data[1, 1:size(pred, 2)], label = &quot;data&quot;)
scatter!(plt, tsteps[1:size(pred, 2)], pred[1, :], label = &quot;prediction&quot;)</code></pre><img src="709dc4ff.svg" alt="Example block output"/><p>However, we&#39;ve now fallen into a trap of a local minimum. If the optimizer changes the parameters, so it dips early, it will increase the loss because there will be more error in the later parts of the time series. Thus it tends to just stay flat and never fit perfectly. This thus suggests strategies (2) and (3): do not allow the later parts of the time series to influence the fit until the later stages. Strategy (3) seems more robust, so this is what will be demonstrated.</p><p>Let&#39;s start by reducing the timespan to <code>(0,1.5)</code>:</p><pre><code class="language-julia hljs">function predict_neuralode(p)
    prob = ODEProblem(neuralode_f, u0, (0.0f0, 1.5f0), p)
    sol = solve(prob, Vern7(), saveat = tsteps, abstol = 1e-6, reltol = 1e-6)
    Array(sol)
end

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x, p) -&gt; loss_neuralode(x), adtype)

optprob = Optimization.OptimizationProblem(optf, pinit)
result_neuralode2 = Optimization.solve(optprob,
    Adam(0.05), callback = callback,
    maxiters = 300)

pred = predict_neuralode(result_neuralode2.u)
plt = scatter(tsteps[1:size(pred, 2)], ode_data[1, 1:size(pred, 2)], label = &quot;data&quot;)
scatter!(plt, tsteps[1:size(pred, 2)], pred[1, :], label = &quot;prediction&quot;)</code></pre><img src="1f8aadd6.svg" alt="Example block output"/><p>This fits beautifully. Now let&#39;s grow the timespan and utilize the parameters from our <code>(0,1.5)</code> fit as the initial condition to our next fit:</p><pre><code class="language-julia hljs">function predict_neuralode(p)
    prob = ODEProblem(neuralode_f, u0, (0.0f0, 3.0f0), p)
    sol = solve(prob, Vern7(), saveat = tsteps, abstol = 1e-6, reltol = 1e-6)
    Array(sol)
end

optprob = Optimization.OptimizationProblem(optf, result_neuralode2.u)
result_neuralode3 = Optimization.solve(optprob,
    Adam(0.05), maxiters = 300,
    callback = callback)

pred = predict_neuralode(result_neuralode3.u)
plt = scatter(tsteps[1:size(pred, 2)], ode_data[1, 1:size(pred, 2)], label = &quot;data&quot;)
scatter!(plt, tsteps[1:size(pred, 2)], pred[1, :], label = &quot;prediction&quot;)</code></pre><img src="18e9c3f0.svg" alt="Example block output"/><p>Once again, a great fit. Now we utilize these parameters as the initial condition to the full fit:</p><pre><code class="language-julia hljs">function predict_neuralode(p)
    prob = ODEProblem(neuralode_f, u0, (0.0f0, 5.0f0), p)
    sol = solve(prob, Vern7(), saveat = tsteps, abstol = 1e-6, reltol = 1e-6)
    Array(sol)
end

optprob = Optimization.OptimizationProblem(optf, result_neuralode3.u)
result_neuralode4 = Optimization.solve(optprob,
    Adam(0.01), maxiters = 500,
    callback = callback)

pred = predict_neuralode(result_neuralode4.u)
plt = scatter(tsteps[1:size(pred, 2)], ode_data[1, 1:size(pred, 2)], label = &quot;data&quot;)
scatter!(plt, tsteps[1:size(pred, 2)], pred[1, :], label = &quot;prediction&quot;)</code></pre><img src="eb3460ed.svg" alt="Example block output"/><h2 id="Training-both-the-initial-conditions-and-the-parameters-to-start"><a class="docs-heading-anchor" href="#Training-both-the-initial-conditions-and-the-parameters-to-start">Training both the initial conditions and the parameters to start</a><a id="Training-both-the-initial-conditions-and-the-parameters-to-start-1"></a><a class="docs-heading-anchor-permalink" href="#Training-both-the-initial-conditions-and-the-parameters-to-start" title="Permalink"></a></h2><p>In this example, we will show how to use strategy (4) in order to accomplish the same goal, except rather than growing the trajectory iteratively, we can train on the whole trajectory. We do this by allowing the neural ODE to learn both the initial conditions and parameters to start, and then reset the initial conditions back and train only the parameters. Note: this strategy is demonstrated for the (0, 5) time span and (0, 10), any longer and more iterations will be required. Alternatively, one could use a mix of (3) and (4), or breaking up the trajectory into chunks and just (4).</p><pre><code class="language-julia hljs">using SciMLSensitivity
using OrdinaryDiffEq,
      ComponentArrays, SciMLSensitivity, Optimization, OptimizationOptimisers
using Lux, Plots, Random, Zygote

#Starting example with tspan (0, 5)
u0 = Float32[2.0; 0.0]
datasize = 30
tspan = (0.0f0, 5.0f0)
tsteps = range(tspan[1], tspan[2], length = datasize)

function trueODEfunc(du, u, p, t)
    true_A = [-0.1 2.0; -2.0 -0.1]
    du .= ((u .^ 3)&#39;true_A)&#39;
end

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))

dudt2 = Chain(Dense(2, 16, tanh), Dense(16, 2))

ps, st = Lux.setup(Random.default_rng(), dudt2)
p = ComponentArray(ps)
dudt(u, p, t) = first(dudt2(u, p, st))
prob = ODEProblem(dudt, u0, tspan)

function predict_n_ode(pu0)
    u0 = pu0.u0
    p = pu0.p
    Array(solve(prob, Tsit5(), u0 = u0, p = p, saveat = tsteps))
end

function loss_n_ode(pu0, _)
    pred = predict_n_ode(pu0)
    sqnorm(x) = sum(abs2, x)
    loss = sum(abs2, ode_data .- pred)
    loss
end

function callback(state, l; doplot = true) #callback function to observe training
    pred = predict_n_ode(state.u)
    display(sum(abs2, ode_data .- pred))
    if doplot
        # plot current prediction against data
        pl = plot(tsteps, ode_data[1, :], label = &quot;data&quot;)
        plot!(pl, tsteps, pred[1, :], label = &quot;prediction&quot;)
        display(plot(pl))
    end
    return false
end

p_init = ComponentArray(; u0 = u0, p = p)

predict_n_ode(p_init)
loss_n_ode(p_init, nothing)

res = solve(OptimizationProblem(OptimizationFunction(loss_n_ode, AutoZygote()), p_init),
    Adam(0.05); callback = callback, maxiters = 1000)

function predict_n_ode2(p)
    Array(solve(prob, Tsit5(), u0 = u0, p = p, saveat = tsteps))
end

function loss_n_ode2(p, _)
    pred = predict_n_ode2(p)
    sqnorm(x) = sum(abs2, x)
    loss = sum(abs2, ode_data .- pred)
    loss
end

function callback2(state, l; doplot = true) #callback function to observe training
    pred = predict_n_ode2(state.u)
    display(sum(abs2, ode_data .- pred))
    if doplot
        # plot current prediction against data
        pl = plot(tsteps, ode_data[1, :], label = &quot;data&quot;)
        plot!(pl, tsteps, pred[1, :], label = &quot;prediction&quot;)
        display(plot(pl))
    end
    return false
end

#Here we reset the IC back to the original and train only the NODE parameters
u0 = Float32[2.0; 0.0]
res = solve(OptimizationProblem(OptimizationFunction(loss_n_ode2, AutoZygote()), p_init.p),
    Adam(0.05); callback = callback2, maxiters = 1000)

#Now use the same technique for a longer tspan (0, 10)
datasize = 30
tspan = (0.0f0, 10.0f0)
tsteps = range(tspan[1], tspan[2], length = datasize)

prob_trueode = ODEProblem(trueODEfunc, u0, tspan)
ode_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))

dudt2 = Chain(Dense(2, 16, tanh), Dense(16, 2))

ps, st = Lux.setup(Random.default_rng(), dudt2)
p = ComponentArray(ps)
dudt(u, p, t) = first(dudt2(u, p, st))
prob = ODEProblem(dudt, u0, tspan)

p_init = ComponentArray(; u0 = u0, p = p)
res = solve(OptimizationProblem(OptimizationFunction(loss_n_ode, AutoZygote()), p_init),
    Adam(0.05); callback = callback, maxiters = 1000)

res = solve(OptimizationProblem(OptimizationFunction(loss_n_ode2, AutoZygote()), p_init.p),
    Adam(0.05); callback = callback2, maxiters = 1000)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Default
u: ComponentVector{Float32}(layer_1 = (weight = Float32[0.86358213 0.4825756; -0.091008976 -0.39799267; … ; -0.17974809 -0.64563113; -0.5895323 -1.2058624], bias = Float32[-0.067548126; -0.20738967; … ; -0.29910597; 0.4732953;;]), layer_2 = (weight = Float32[-0.84792227 0.24151467 … 0.32020646 0.38877714; 0.93666404 -0.03780652 … 0.35303974 -0.242653], bias = Float32[-0.36630347; 0.15947655;;]))</code></pre><p>And there we go, a set of robust strategies for fitting an equation that would otherwise get stuck in local optima.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../chaotic_ode/">« Sensitivity analysis for chaotic systems (shadowing methods)</a><a class="docs-footer-nextpage" href="../divergence/">Handling Divergent and Unstable Trajectories »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Tuesday 21 May 2024 05:54">Tuesday 21 May 2024</span>. Using Julia version 1.10.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
