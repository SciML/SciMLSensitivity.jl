<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers Â· SciMLSensitivity.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://sensitivity.sciml.ai/stable/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>SciMLSensitivity.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a><ul class="internal"><li><a class="tocitem" href="#High-Level-Interface:-sensealg"><span>High Level Interface: <code>sensealg</code></span></a></li><li><a class="tocitem" href="#Equation-Scope"><span>Equation Scope</span></a></li><li><a class="tocitem" href="#SciMLSensitivity-and-Universal-Differential-Equations"><span>SciMLSensitivity and Universal Differential Equations</span></a></li><li><a class="tocitem" href="#Note-on-Modularity-and-Composability-with-Solvers"><span>Note on Modularity and Composability with Solvers</span></a></li><li><a class="tocitem" href="#Citation"><span>Citation</span></a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Differentiating Ordinary Differential Equations (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation</a></li><li><a class="tocitem" href="ad_examples/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="ad_examples/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="ad_examples/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Fitting Ordinary Differential Equation (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="ode_fitting/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="ode_fitting/stiff_ode_fit/">Parameter Estimation on Highly Stiff Systems</a></li><li><a class="tocitem" href="ode_fitting/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="ode_fitting/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="ode_fitting/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="ode_fitting/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="ode_fitting/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Neural Ordinary Differential Equation (Neural ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Stochastic Differential Equation (SDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="sde_fitting/optimization_sde/">Optimization of Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox"/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Delay Differential Equation (DDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="dde_fitting/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Differential-Algebraic Equation (DAE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="dae_fitting/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox"/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Partial Differential Equation (PDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="pde_fitting/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-9" type="checkbox"/><label class="tocitem" for="menuitem-2-9"><span class="docs-label">Hybrid and Jump Equation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="hybrid_jump_fitting/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-10" type="checkbox"/><label class="tocitem" for="menuitem-2-10"><span class="docs-label">Bayesian Estimation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-11" type="checkbox"/><label class="tocitem" for="menuitem-2-11"><span class="docs-label">Optimal and Model Predictive Control Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="optimal_control/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/SciMLSensitivity.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SciMLSensitivity:-Automatic-Differentiation-and-Adjoints-for-(Differential)-Equation-Solvers"><a class="docs-heading-anchor" href="#SciMLSensitivity:-Automatic-Differentiation-and-Adjoints-for-(Differential)-Equation-Solvers">SciMLSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a><a id="SciMLSensitivity:-Automatic-Differentiation-and-Adjoints-for-(Differential)-Equation-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#SciMLSensitivity:-Automatic-Differentiation-and-Adjoints-for-(Differential)-Equation-Solvers" title="Permalink"></a></h1><p>SciMLSensitivity.jl is the automatic differentiation and adjoints system for the SciML ecosystem. Also known as local sensitivity analysis, these methods allow for calculation of fast derivatives of SciML problem types which are commonly used to analyze model sensitivities, callibrate models to data, train neural ODEs, perform automated model discovery via universal differential equations, and more. SciMLSensitivity.jl is a high level interface that pulls together all of the tools with heuristics and helper functions to make solving inverse problems and inferring models as easy as possible without losing efficiency.</p><p>Thus, what SciMLSensitivity.jl provides is:</p><ul><li>Automatic differentiation overloads for improving the performance and flexibility of AD calls over <code>solve</code>.</li><li>A bunch of tutorials, documentation, and test cases for this combination with parameter estimation (data fitting / model calibration), neural network  libraries and GPUs.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This documentation assumes familiarity with the solver packages for the respective problem types. If one is not familiar with the solver packages, please consult the documentation for pieces like <a href="https://diffeq.sciml.ai/stable/">DifferentialEquations.jl</a>,  <a href="https://nonlinearsolve.sciml.ai/dev/">NonlinearSolve.jl</a>,  <a href="http://linearsolve.sciml.ai/dev/">LinearSolve.jl</a>, etc. first.</p></div></div><h2 id="High-Level-Interface:-sensealg"><a class="docs-heading-anchor" href="#High-Level-Interface:-sensealg">High Level Interface: <code>sensealg</code></a><a id="High-Level-Interface:-sensealg-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-Interface:-sensealg" title="Permalink"></a></h2><p>The highest level interface is provided by the function <code>solve</code>:</p><pre><code class="language-julia hljs">solve(prob,args...;sensealg=InterpolatingAdjoint(),
      checkpoints=sol.t,kwargs...)</code></pre><p><code>solve</code> is fully compatible with automatic differentiation libraries like:</p><ul><li><a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a></li><li><a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a></li><li><a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a></li><li><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a></li></ul><p>and will automatically replace any calculations of the solution&#39;s derivative with a fast method. The keyword argument <code>sensealg</code> controls the dispatch to the <code>AbstractSensitivityAlgorithm</code> used for the sensitivity calculation. Note that <code>solve</code> in an AD context does not allow higher order interpolations unless <code>sensealg=DiffEqBase.SensitivityADPassThrough()</code> is used, i.e. going back to the AD mechanism.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The behavior of ForwardDiff.jl is different from the other automatic differentiation libraries mentioned above. The <code>sensealg</code> keyword is ignored. Instead, the differential equations are solved using <code>Dual</code> numbers for <code>u0</code> and <code>p</code>. If only <code>p</code> is perturbed in the sensitivity analysis, but not <code>u0</code>, the state is still implemented as a <code>Dual</code> number. ForwardDiff.jl will thus not dispatch into continuous forward nor adjoint sensitivity analysis even if a <code>sensealg</code> is provided.</p></div></div><h2 id="Equation-Scope"><a class="docs-heading-anchor" href="#Equation-Scope">Equation Scope</a><a id="Equation-Scope-1"></a><a class="docs-heading-anchor-permalink" href="#Equation-Scope" title="Permalink"></a></h2><p>SciMLSensitivity.jl supports all of the equation types of the  <a href="https://scimlbase.sciml.ai/dev/">SciML Common Interface</a>, extending the problem types by adding overloads for automatic differentiation to improve the performance and flexibility of the differentiation system. This includes:</p><ul><li>Linear systems (<code>LinearProblem</code>)<ul><li>Direct methods for dense and sparse</li><li>Iterative solvers with preconditioning</li></ul></li><li>Nonlinear Systems (<code>NonlinearProblem</code>)<ul><li>Systems of nonlinear equations</li><li>Scalar bracketing systems</li></ul></li><li>Integrals (quadrature) (<code>QuadratureProblem</code>)</li><li>Differential Equations<ul><li>Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (<code>DiscreteProblem</code>)</li><li>Ordinary differential equations (ODEs) (<code>ODEProblem</code>)</li><li>Split and Partitioned ODEs (Symplectic integrators, IMEX Methods) (<code>SplitODEProblem</code>)</li><li>Stochastic ordinary differential equations (SODEs or SDEs) (<code>SDEProblem</code>)</li><li>Stochastic differential-algebraic equations (SDAEs) (<code>SDEProblem</code> with mass matrices)</li><li>Random differential equations (RODEs or RDEs) (<code>RODEProblem</code>)</li><li>Differential algebraic equations (DAEs) (<code>DAEProblem</code> and <code>ODEProblem</code> with mass matrices)</li><li>Delay differential equations (DDEs) (<code>DDEProblem</code>)</li><li>Neutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)</li><li>Stochastic delay differential equations (SDDEs) (<code>SDDEProblem</code>)</li><li>Experimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)</li><li>Mixed discrete and continuous equations (Hybrid Equations, Jump Diffusions) (<code>DEProblem</code>s with callbacks)</li></ul></li><li>Optimization (<code>OptimizationProblem</code>)<ul><li>Nonlinear (constrained) optimization</li></ul></li><li>(Stochastic/Delay/Differential-Algebraic) Partial Differential Equations (<code>PDESystem</code>)<ul><li>Finite difference and finite volume methods</li><li>Interfaces to finite element methods</li><li>Physics-Informed Neural Networks (PINNs)</li><li>Integro-Differential Equations</li><li>Fractional Differential Equations</li></ul></li></ul><h2 id="SciMLSensitivity-and-Universal-Differential-Equations"><a class="docs-heading-anchor" href="#SciMLSensitivity-and-Universal-Differential-Equations">SciMLSensitivity and Universal Differential Equations</a><a id="SciMLSensitivity-and-Universal-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#SciMLSensitivity-and-Universal-Differential-Equations" title="Permalink"></a></h2><p>SciMLSensitivity is for universal differential equations, where these can include delays, physical constraints, stochasticity, events, and all other kinds of interesting behavior that shows up in scientific simulations. Neural networks can be all or part of the model. They can be around the differential equation, in the cost function, or inside of the differential equation. Neural networks representing unknown portions of the model or functions can go anywhere you have uncertainty in the form of the scientific simulator. Forward sensitivity and adjoint equations are automatically generated with checkpointing and stabilization to ensure it works for large stiff equations, while specializations on static objects allows for high efficiency on small equations. For an overview of the topic with applications, consult the paper <a href="https://arxiv.org/abs/2001.04385">Universal Differential Equations for Scientific Machine Learning</a>.</p><p>You can efficiently use the package for:</p><ul><li>Parameter estimation of scientific models (ODEs, SDEs, DDEs, DAEs, etc.)</li><li>Neural ODEs, Neural SDE, Neural DAEs, Neural DDEs, etc.</li><li>Nonlinear optimal control, including training neural controllers</li><li>(Stiff) universal ordinary differential equations (universal ODEs)</li><li>Universal stochastic differential equations (universal SDEs)</li><li>Universal delay differential equations (universal DDEs)</li><li>Universal partial differential equations (universal PDEs)</li><li>Universal jump stochastic differential equations (universal jump diffusions)</li><li>Hybrid universal differential equations (universal DEs with event handling)</li></ul><p>with high order, adaptive, implicit, GPU-accelerated, Newton-Krylov, etc. methods. For examples, please refer to <a href="https://julialang.org/blog/2019/01/fluxdiffeq">the DiffEqFlux release blog post</a> (which we try to keep updated for changes to the libraries). Additional demonstrations, like neural PDEs and neural jump SDEs, can be found <a href="http://www.stochasticlifestyle.com/neural-jump-sdes-jump-diffusions-and-neural-pdes/">at this blog post</a> (among many others!). All of these features are only part of the advantage, as this library <a href="Benchmark/#Benchmarks">routinely benchmarks orders of magnitude faster than competing libraries like torchdiffeq</a>. Use with GPUs is highly optimized by <a href="https://www.stochasticlifestyle.com/solving-systems-stochastic-pdes-using-gpus-julia/">recompiling the solvers to GPUs to remove all CPU-GPU data transfers</a>, while use with CPUs uses specialized kernels for accelerating differential equation solves.</p><p>Many different training techniques are supported by this package, including:</p><ul><li>Optimize-then-discretize (backsolve adjoints, checkpointed adjoints, quadrature adjoints)</li><li>Discretize-then-optimize (forward and reverse mode discrete sensitivity analysis)<ul><li>This is a generalization of <a href="https://arxiv.org/pdf/1902.10298.pdf">ANODE</a> and <a href="https://arxiv.org/pdf/1906.04596.pdf">ANODEv2</a> to all <a href="https://diffeq.sciml.ai/latest/solvers/ode_solve/">DifferentialEquations.jl ODE solvers</a></li></ul></li><li>Hybrid approaches (adaptive time stepping + AD for adaptive discretize-then-optimize)</li><li>O(1) memory backprop of ODEs via BacksolveAdjoint, and Virtual Brownian Trees for O(1) backprop of SDEs</li><li><a href="ad_examples/adjoint_continuous_functional/#continuous_loss">Continuous adjoints for integral loss functions</a></li><li>Probabilistic programming and variational inference on ODEs/SDEs/DAEs/DDEs/hybrid equations etc. is provided by integration with <a href="https://turing.ml/dev/">Turing.jl</a> and <a href="https://github.com/probcomp/Gen.jl">Gen.jl</a>. Reproduce <a href="https://arxiv.org/abs/2001.01328">variational loss functions</a> by plugging <a href="https://turing.ml/dev/tutorials/9-variationalinference/">composible libraries together</a>.</li></ul><p>all while mixing forward mode and reverse mode approaches as appropriate for the most speed. For more details on the adjoint sensitivity analysis methods for computing fast gradients, see the <a href="manual/differential_equation_sensitivities/#sensitivity_diffeq">adjoints details page</a>.</p><p>With this package, you can explore various ways to integrate the two methodologies:</p><ul><li>Neural networks can be defined where the âactivationsâ are nonlinear functions described by differential equations</li><li>Neural networks can be defined where some layers are ODE solves</li><li>ODEs can be defined where some terms are neural networks</li><li>Cost functions on ODEs can define neural networks</li></ul><h2 id="Note-on-Modularity-and-Composability-with-Solvers"><a class="docs-heading-anchor" href="#Note-on-Modularity-and-Composability-with-Solvers">Note on Modularity and Composability with Solvers</a><a id="Note-on-Modularity-and-Composability-with-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Note-on-Modularity-and-Composability-with-Solvers" title="Permalink"></a></h2><p>Note that SciMLSensitivity.jl purely built on composable and modular infrastructure.  SciMLSensitivity provides high level helper functions and documentation for the user, but the code generation stack is modular and composes in many different ways. For example, one can use and swap out the ODE solver between any common interface compatible library, like:</p><ul><li>Sundials.jl</li><li>OrdinaryDiffEq.jl</li><li>LSODA.jl</li><li><a href="https://github.com/mikelehu/IRKGaussLegendre.jl">IRKGaussLegendre.jl</a></li><li><a href="https://github.com/SciML/SciPyDiffEq.jl">SciPyDiffEq.jl</a></li><li><a href="https://diffeq.sciml.ai/stable/solvers/ode_solve/">... etc. many other choices!</a></li></ul><p>In addition, due to the composability of the system, none of the components are directly tied to the Flux.jl machine learning framework. For example, you can <a href="https://youtu.be/n2MwJ1guGVQ?t=284">use SciMLSensitivity.jl to generate TensorFlow graphs and train the neural network with TensorFlow.jl</a>, <a href="https://github.com/FluxML/Torch.jl">use PyTorch arrays via Torch.jl</a>, and more all with single line code changes by utilizing the underlying code generation. The tutorials shown here are thus mostly a guide on how to use the ecosystem as a whole, only showing a small snippet of the possible ways to compose the thousands of differentiable libraries together! Swap out ODEs for SDEs, DDEs, DAEs, etc., put quadrature libraries or  <a href="https://github.com/mcabbott/Tullio.jl">Tullio.jl</a> in the loss function, the world is your  oyster!</p><p>As a proof of composability, note that the implementation of Bayesian neural ODEs required zero code changes to the library, and instead just relied on the composability with other Julia packages.</p><h2 id="Citation"><a class="docs-heading-anchor" href="#Citation">Citation</a><a id="Citation-1"></a><a class="docs-heading-anchor-permalink" href="#Citation" title="Permalink"></a></h2><p>If you use SciMLSensitivity.jl or are influenced by its ideas, please cite:</p><pre><code class="nohighlight hljs">@article{rackauckas2020universal,
  title={Universal differential equations for scientific machine learning},
  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},
  journal={arXiv preprint arXiv:2001.04385},
  year={2020}
}</code></pre></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.21 on <span class="colophon-date" title="Sunday 17 July 2022 14:04">Sunday 17 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
