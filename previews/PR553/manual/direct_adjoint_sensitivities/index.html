<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Direct Adjoint Sensitivities of Differential Equations · DiffEqSensitivity.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://sensitivity.sciml.ai/stable/manual/direct_adjoint_sensitivities/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DiffEqSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DiffEqSensitivity.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Differentiating Ordinary Differential Equations (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation</a></li><li><a class="tocitem" href="../../ad_examples/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../ad_examples/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../ad_examples/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Fitting Ordinary Differential Equation (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ode_fitting/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../ode_fitting/stiff_ode_fit/">Parameter Estimation on Highly Stiff Systems</a></li><li><a class="tocitem" href="../../ode_fitting/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../../ode_fitting/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../ode_fitting/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../../ode_fitting/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../../ode_fitting/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Neural Ordinary Differential Equation (Neural ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../neural_ode/neural_ode_galacticoptim/">Neural Ordinary Differential Equations with GalacticOptim.jl</a></li><li><a class="tocitem" href="../../neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../../neural_ode/mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../../neural_ode/mnist_conv_neural_ode/">Convolutional Neural ODE MNIST Classifier on GPU</a></li><li><a class="tocitem" href="../../neural_ode/GPUs/">Neural ODEs on GPUs</a></li><li><a class="tocitem" href="../../neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../../neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Stochastic Differential Equation (SDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sde_fitting/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../sde_fitting/neural_sde/">Neural Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox"/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Delay Differential Equation (DDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dde_fitting/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Differential-Algebraic Equation (DAE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dae_fitting/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox"/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Partial Differential Equation (PDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../pde_fitting/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-9" type="checkbox"/><label class="tocitem" for="menuitem-2-9"><span class="docs-label">Hybrid and Jump Equation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../hybrid_jump_fitting/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-10" type="checkbox"/><label class="tocitem" for="menuitem-2-10"><span class="docs-label">Bayesian Estimation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li><li><a class="tocitem" href="../../bayesian/BayesianNODE_NUTS/">Bayesian Neural ODEs: NUTS</a></li><li><a class="tocitem" href="../../bayesian/BayesianNODE_SGLD/">Bayesian Neural ODEs: SGLD</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-11" type="checkbox"/><label class="tocitem" for="menuitem-2-11"><span class="docs-label">Optimal and Model Predictive Control Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../../optimal_control/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li class="is-active"><a class="tocitem" href>Direct Adjoint Sensitivities of Differential Equations</a><ul class="internal"><li><a class="tocitem" href="#First-Order-Adjoint-Sensitivities"><span>First Order Adjoint Sensitivities</span></a></li><li><a class="tocitem" href="#Second-Order-Adjoint-Sensitivities"><span>Second Order Adjoint Sensitivities</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual and APIs</a></li><li class="is-active"><a href>Direct Adjoint Sensitivities of Differential Equations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Direct Adjoint Sensitivities of Differential Equations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/master/docs/src/manual/direct_adjoint_sensitivities.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="adjoint_sense"><a class="docs-heading-anchor" href="#adjoint_sense">Direct Adjoint Sensitivities of Differential Equations</a><a id="adjoint_sense-1"></a><a class="docs-heading-anchor-permalink" href="#adjoint_sense" title="Permalink"></a></h1><h2 id="First-Order-Adjoint-Sensitivities"><a class="docs-heading-anchor" href="#First-Order-Adjoint-Sensitivities">First Order Adjoint Sensitivities</a><a id="First-Order-Adjoint-Sensitivities-1"></a><a class="docs-heading-anchor-permalink" href="#First-Order-Adjoint-Sensitivities" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiffEqSensitivity.adjoint_sensitivities" href="#DiffEqSensitivity.adjoint_sensitivities"><code>DiffEqSensitivity.adjoint_sensitivities</code></a> — <span class="docstring-category">Function</span></header><section><div><p>adjoint<em>sensitivities(sol,alg,g,t=nothing,dg=nothing;                             abstol=1e-6,reltol=1e-3,                             checkpoints=sol.t,                             corfunc</em>analytical=nothing,                             callback = nothing,                             sensealg=InterpolatingAdjoint(),                             kwargs...)</p><p>Adjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution. In many cases this is used in an optimization problem to return the gradient with respect to some cost function. It is equivalent to &quot;backpropagation&quot; or reverse-mode automatic differentiation of a differential equation.</p><p>Using <code>adjoint_sensitivities</code> directly let&#39;s you do three things. One it can allow you to be more efficient, since the sensitivity calculation can be done directly on a cost function, avoiding the overhead of building the derivative of the full concretized solution. It can also allow you to be more efficient by directly controlling the forward solve that is then reversed over. Lastly, it allows one to define a continuous cost function on the continuous solution, instead of just at discrete data points.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Adjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct <code>p</code>. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with adjoint sensitivity analysis requires that <code>p</code> could be a valid   type for being the initial condition <code>u0</code> of an array. This means that   many simple types, such as <code>Tuple</code>s and <code>NamedTuple</code>s, will work as   parameters in normal contexts but will fail during adjoint differentiation.   To work around this issue for complicated cases like nested structs, look   into defining <code>p</code> using <code>AbstractArray</code> libraries such as RecursiveArrayTools.jl   or ComponentArrays.jl so that <code>p</code> is an <code>AbstractArray</code> with a concrete element type.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Non-checkpointed InterpolatingAdjoint and QuadratureAdjoint sensealgs   require that the forward solution <code>sol(t)</code> has an accurate dense   solution unless checkpointing is used. This means that you should   not use <code>solve(prob,alg,saveat=ts)</code> unless checkpointing. If specific   saving is required, one should solve dense <code>solve(prob,alg)</code>, use the   solution in the adjoint, and then <code>sol(ts)</code> interpolate.</p></div></div><p><strong>Syntax</strong></p><p>There are two forms. For discrete adjoints, the form is:</p><pre><code class="language-julia hljs">du0,dp = adjoint_sensitivities(sol,alg,dg,ts;sensealg=InterpolatingAdjoint(),
                               checkpoints=sol.t,kwargs...)</code></pre><p>where <code>alg</code> is the ODE algorithm to solve the adjoint problem, <code>dg</code> is the jump function, <code>sensealg</code> is the sensitivity algorithm, and <code>ts</code> is the time points for data. <code>dg</code> is given by:</p><pre><code class="language-julia hljs">dg(out,u,p,t,i)</code></pre><p>which is the in-place gradient of the cost functional <code>g</code> at time point <code>ts[i]</code> with <code>u=u(t)</code>.</p><p>For continuous functionals, the form is:</p><pre><code class="language-julia hljs">du0,dp = adjoint_sensitivities(sol,alg,g,nothing,(dgdu,dgdp);sensealg=InterpolatingAdjoint(),
                               checkpoints=sol.t,,kwargs...)</code></pre><p>for the cost functional</p><pre><code class="language-julia hljs">g(u,p,t)</code></pre><p>with in-place gradient</p><pre><code class="language-julia hljs">dgdu(out,u,p,t)
dgdp(out,u,p,t)</code></pre><p>If the gradient is omitted, i.e.</p><pre><code class="language-julia hljs">du0,dp = adjoint_sensitivities(sol,alg,g,nothing;kwargs...)</code></pre><p>then we assume <code>dgdp</code> is zero and <code>dgdu</code> will be computed automatically using ForwardDiff or finite differencing, depending on the <code>autodiff</code> setting in the <code>AbstractSensitivityAlgorithm</code>. Note that the keyword arguments are passed to the internal ODE solver for solving the adjoint problem.</p><p><strong>Example discrete adjoints on a cost function</strong></p><p>In this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let&#39;s solve the ODE and get a high quality continuous solution:</p><pre><code class="language-julia hljs">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + u[1]*u[2]
end

p = [1.5,1.0,3.0]
prob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)
sol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)</code></pre><p>Now let&#39;s calculate the sensitivity of the <span>$\ell_2$</span> error against 1 at evenly spaced points in time, that is:</p><p class="math-container">\[L(u,p,t)=\sum_{i=1}^{n}\frac{\Vert1-u(t_{i},p)\Vert^{2}}{2}\]</p><p>for <span>$t_i = 0.5i$</span>. This is the assumption that the data is <code>data[i]=1.0</code>. For this function, notice we have that:</p><p class="math-container">\[\begin{aligned}
dg_{1}&amp;=1-u_{1} \\
dg_{2}&amp;=1-u_{2} \\
&amp; \quad \vdots
\end{aligned}\]</p><p>and thus:</p><pre><code class="language-julia hljs">dg(out,u,p,t,i) = (out.=1.0.-u)</code></pre><p>Also, we can omit <code>dgdp</code>, because the cost function doesn&#39;t dependent on <code>p</code>. If we had data, we&#39;d just replace <code>1.0</code> with <code>data[i]</code>. To get the adjoint sensitivities, call:</p><pre><code class="language-julia hljs">ts = 0:0.5:10
res = adjoint_sensitivities(sol,Vern9(),dg,ts,abstol=1e-14,
                            reltol=1e-14)</code></pre><p>This is super high accuracy. As always, there&#39;s a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:</p><pre><code class="language-julia hljs">using ForwardDiff,Calculus,Tracker
function G(p)
  tmp_prob = remake(prob,u0=convert.(eltype(p),prob.u0),p=p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=ts,
              sensealg=SensitivityADPassThrough())
  A = convert(Array,sol)
  sum(((1 .- A).^2)./2)
end
G([1.5,1.0,3.0])
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])
res4 = Tracker.gradient(G,[1.5,1.0,3.0])
res5 = ReverseDiff.gradient(G,[1.5,1.0,3.0])</code></pre><p>and see this gives the same values.</p><p><strong>Example controlling adjoint method choices and checkpointing</strong></p><p>In the previous examples, all calculations were done using the interpolating method. This maximizes speed but at a cost of requiring a dense <code>sol</code>. If it is not possible to hold a dense forward solution in memory, then one can use checkpointing. For example:</p><pre><code class="language-julia hljs">ts = [0.0,0.2,0.5,0.7]
sol = solve(prob,Vern9(),saveat=ts)</code></pre><p>Creates a non-dense solution with checkpoints at <code>[0.0,0.2,0.5,0.7]</code>. Now we can do:</p><pre><code class="language-julia hljs">res = adjoint_sensitivities(sol,Vern9(),dg,ts,
                            sensealg=InterpolatingAdjoint(checkpointing=true))</code></pre><p>When grabbing a Jacobian value during the backwards solution, it will no longer interpolate to get the value. Instead, it will start a forward solution at the nearest checkpoint to build local interpolants in a way that conserves memory. By default the checkpoints are at <code>sol.t</code>, but we can override this:</p><pre><code class="language-julia hljs">res = adjoint_sensitivities(sol,Vern9(),dg,ts,
                            sensealg=InterpolatingAdjoint(checkpointing=true),
                            checkpoints = [0.0,0.5])</code></pre><p><strong>Example continuous adjoints on an energy functional</strong></p><p>In this case we&#39;d like to calculate the adjoint sensitivity of the scalar energy functional:</p><p class="math-container">\[G(u,p)=\int_{0}^{T}\frac{\sum_{i=1}^{n}u_{i}^{2}(t)}{2}dt\]</p><p>which is:</p><pre><code class="language-julia hljs">g(u,p,t) = (sum(u).^2) ./ 2</code></pre><p>Notice that the gradient of this function with respect to the state <code>u</code> is:</p><pre><code class="language-julia hljs">function dg(out,u,p,t)
  out[1]= u[1] + u[2]
  out[2]= u[1] + u[2]
end</code></pre><p>To get the adjoint sensitivities, we call:</p><pre><code class="language-julia hljs">res = adjoint_sensitivities(sol,Vern9(),g,nothing,dg,abstol=1e-8,
                                 reltol=1e-8,iabstol=1e-8,ireltol=1e-8)</code></pre><p>Notice that we can check this against autodifferentiation and numerical differentiation as follows:</p><pre><code class="language-julia hljs">using QuadGK
function G(p)
  tmp_prob = remake(prob,p=p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)
  res,err = quadgk((t)-&gt; (sum(sol(t)).^2)./2,0.0,10.0,atol=1e-14,rtol=1e-10)
  res
end
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/eef7521ce15eb5f8f595967d54bff957ad91804a/src/sensitivity_interface.jl#L23">source</a></section></article><h2 id="Second-Order-Adjoint-Sensitivities"><a class="docs-heading-anchor" href="#Second-Order-Adjoint-Sensitivities">Second Order Adjoint Sensitivities</a><a id="Second-Order-Adjoint-Sensitivities-1"></a><a class="docs-heading-anchor-permalink" href="#Second-Order-Adjoint-Sensitivities" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="DiffEqSensitivity.second_order_sensitivities" href="#DiffEqSensitivity.second_order_sensitivities"><code>DiffEqSensitivity.second_order_sensitivities</code></a> — <span class="docstring-category">Function</span></header><section><div><p>H = second<em>order</em>sensitivities(loss,prob,alg,args...;                                sensealg=ForwardDiffOverAdjoint(InterpolatingAdjoint(autojacvec=ReverseDiffVJP())),                                kwargs...)</p><p>Second order sensitivity analysis is used for the fast calculation of Hessian matrices.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Adjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct <code>p</code>. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with adjoint sensitivity analysis requires that <code>p</code> could be a valid   type for being the initial condition <code>u0</code> of an array. This means that   many simple types, such as <code>Tuple</code>s and <code>NamedTuple</code>s, will work as   parameters in normal contexts but will fail during adjoint differentiation.   To work around this issue for complicated cases like nested structs, look   into defining <code>p</code> using <code>AbstractArray</code> libraries such as RecursiveArrayTools.jl   or ComponentArrays.jl so that <code>p</code> is an <code>AbstractArray</code> with a concrete element type.</p></div></div><p><strong>Example second order sensitivity analysis calculation</strong></p><pre><code class="language-julia hljs">using DiffEqSensitivity, OrdinaryDiffEq, ForwardDiff
using Test

function lotka!(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end

p = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]
prob = ODEProblem(lotka!,u0,(0.0,10.0),p)
loss(sol) = sum(sol)
v = ones(4)

H  = second_order_sensitivities(loss,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)</code></pre><p><strong>Arguments</strong></p><p>The arguments for this function match <code>adjoint_sensitivities</code>. The only notable difference is <code>sensealg</code> which requires a second order sensitivity algorithm, of which currently the only choice is <code>ForwardDiffOverAdjoint</code> which uses forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a faster computation than either double forward or double reverse. <code>ForwardDiffOverAdjoint</code>&#39;s positional argument just accepts a first order sensitivity algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/eef7521ce15eb5f8f595967d54bff957ad91804a/src/sensitivity_interface.jl#L357">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="DiffEqSensitivity.second_order_sensitivity_product" href="#DiffEqSensitivity.second_order_sensitivity_product"><code>DiffEqSensitivity.second_order_sensitivity_product</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Hv = second<em>order</em>sensitivity_product(loss,v,prob,alg,args...;                                sensealg=ForwardDiffOverAdjoint(InterpolatingAdjoint(autojacvec=ReverseDiffVJP())),                                kwargs...)</p><p>Second order sensitivity analysis product is used for the fast calculation of Hessian-vector products <span>$Hv$</span> without requiring the construction of the Hessian matrix.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>Adjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct <code>p</code>. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with adjoint sensitivity analysis requires that <code>p</code> could be a valid   type for being the initial condition <code>u0</code> of an array. This means that   many simple types, such as <code>Tuple</code>s and <code>NamedTuple</code>s, will work as   parameters in normal contexts but will fail during adjoint differentiation.   To work around this issue for complicated cases like nested structs, look   into defining <code>p</code> using <code>AbstractArray</code> libraries such as RecursiveArrayTools.jl   or ComponentArrays.jl so that <code>p</code> is an <code>AbstractArray</code> with a concrete element type.</p></div></div><p><strong>Example second order sensitivity analysis calculation</strong></p><pre><code class="language-julia hljs">using DiffEqSensitivity, OrdinaryDiffEq, ForwardDiff
using Test

function lotka!(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end

p = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]
prob = ODEProblem(lotka!,u0,(0.0,10.0),p)
loss(sol) = sum(sol)
v = ones(4)

Hv = second_order_sensitivity_product(loss,v,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)</code></pre><p><strong>Arguments</strong></p><p>The arguments for this function match <code>adjoint_sensitivities</code>. The only notable difference is <code>sensealg</code> which requires a second order sensitivity algorithm, of which currently the only choice is <code>ForwardDiffOverAdjoint</code> which uses forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a faster computation than either double forward or double reverse. <code>ForwardDiffOverAdjoint</code>&#39;s positional argument just accepts a first order sensitivity algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/eef7521ce15eb5f8f595967d54bff957ad91804a/src/sensitivity_interface.jl#L412">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../direct_forward_sensitivity/">« Direct Forward Sensitivity Analysis of ODEs</a><a class="docs-footer-nextpage" href="../../Benchmark/">Benchmarks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Thursday 16 June 2022 02:20">Thursday 16 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
