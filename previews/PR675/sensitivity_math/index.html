<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sensitivity Math Details Â· SciMLSensitivity.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://sensitivity.sciml.ai/stable/sensitivity_math/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">SciMLSensitivity.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Differentiating Ordinary Differential Equations (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation</a></li><li><a class="tocitem" href="../ad_examples/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../ad_examples/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../ad_examples/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox"/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Fitting Ordinary Differential Equation (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../ode_fitting/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../ode_fitting/stiff_ode_fit/">Parameter Estimation on Highly Stiff Systems</a></li><li><a class="tocitem" href="../ode_fitting/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../ode_fitting/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../ode_fitting/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../ode_fitting/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../ode_fitting/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Neural Ordinary Differential Equation (Neural ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Stochastic Differential Equation (SDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../sde_fitting/optimization_sde/">Optimization of Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox"/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Delay Differential Equation (DDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../dde_fitting/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Differential-Algebraic Equation (DAE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../dae_fitting/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox"/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Partial Differential Equation (PDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../pde_fitting/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-9" type="checkbox"/><label class="tocitem" for="menuitem-2-9"><span class="docs-label">Hybrid and Jump Equation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../hybrid_jump_fitting/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-10" type="checkbox"/><label class="tocitem" for="menuitem-2-10"><span class="docs-label">Bayesian Estimation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-11" type="checkbox"/><label class="tocitem" for="menuitem-2-11"><span class="docs-label">Optimal and Model Predictive Control Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../optimal_control/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../Benchmark/">Benchmarks</a></li><li class="is-active"><a class="tocitem" href>Sensitivity Math Details</a><ul class="internal"><li><a class="tocitem" href="#Forward-Sensitivity-Analysis"><span>Forward Sensitivity Analysis</span></a></li><li><a class="tocitem" href="#Adjoint-Sensitivity-Analysis"><span>Adjoint Sensitivity Analysis</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Sensitivity Math Details</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sensitivity Math Details</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/SciMLSensitivity.jl/blob/master/docs/src/sensitivity_math.md" title="Edit on GitHub"><span class="docs-icon fab">ï</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sensitivity_math"><a class="docs-heading-anchor" href="#sensitivity_math">Mathematics of Sensitivity Analysis</a><a id="sensitivity_math-1"></a><a class="docs-heading-anchor-permalink" href="#sensitivity_math" title="Permalink"></a></h1><h2 id="Forward-Sensitivity-Analysis"><a class="docs-heading-anchor" href="#Forward-Sensitivity-Analysis">Forward Sensitivity Analysis</a><a id="Forward-Sensitivity-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Forward-Sensitivity-Analysis" title="Permalink"></a></h2><p>The local sensitivity is computed using the sensitivity ODE:</p><p class="math-container">\[\frac{d}{dt}\frac{\partial u}{\partial p_{j}}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial p_{j}}+\frac{\partial f}{\partial p_{j}}=J\cdot S_{j}+F_{j}\]</p><p>where</p><p class="math-container">\[J=\left(\begin{array}{cccc}
\frac{\partial f_{1}}{\partial u_{1}} &amp; \frac{\partial f_{1}}{\partial u_{2}} &amp; \cdots &amp; \frac{\partial f_{1}}{\partial u_{k}}\\
\frac{\partial f_{2}}{\partial u_{1}} &amp; \frac{\partial f_{2}}{\partial u_{2}} &amp; \cdots &amp; \frac{\partial f_{2}}{\partial u_{k}}\\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots\\
\frac{\partial f_{k}}{\partial u_{1}} &amp; \frac{\partial f_{k}}{\partial u_{2}} &amp; \cdots &amp; \frac{\partial f_{k}}{\partial u_{k}}
\end{array}\right)\]</p><p>is the Jacobian of the system,</p><p class="math-container">\[F_{j}=\left(\begin{array}{c}
\frac{\partial f_{1}}{\partial p_{j}}\\
\frac{\partial f_{2}}{\partial p_{j}}\\
\vdots\\
\frac{\partial f_{k}}{\partial p_{j}}
\end{array}\right)\]</p><p>are the parameter derivatives, and</p><p class="math-container">\[S_{j}=\left(\begin{array}{c}
\frac{\partial u_{1}}{\partial p_{j}}\\
\frac{\partial u_{2}}{\partial p_{j}}\\
\vdots\\
\frac{\partial u_{k}}{\partial p_{j}}
\end{array}\right)\]</p><p>is the vector of sensitivities. Since this ODE is dependent on the values of the independent variables themselves, this ODE is computed simultaneously with the actual ODE system.</p><p>Note that the Jacobian-vector product</p><p class="math-container">\[\frac{\partial f}{\partial u}\frac{\partial u}{\partial p_{j}}\]</p><p>can be computed without forming the Jacobian. With finite differences, this through using the following formula for the directional derivative</p><p class="math-container">\[Jv \approx \frac{f(x+v \epsilon) - f(x)}{\epsilon},\]</p><p>or, alternatively and without truncation error, by using a dual number with a single partial dimension, <span>$d = x + v \epsilon$</span> we get that</p><p class="math-container">\[f(d) = f(x) + Jv \epsilon\]</p><p>as a fast way to calcuate <span>$Jv$</span>. Thus, except when a sufficiently good function for <code>J</code> is given by the user, the Jacobian is never formed. For more details, consult the <a href="https://mitmath.github.io/18337/lecture8/automatic_differentiation.html">MIT 18.337 lecture notes on forward mode AD</a>.</p><h2 id="Adjoint-Sensitivity-Analysis"><a class="docs-heading-anchor" href="#Adjoint-Sensitivity-Analysis">Adjoint Sensitivity Analysis</a><a id="Adjoint-Sensitivity-Analysis-1"></a><a class="docs-heading-anchor-permalink" href="#Adjoint-Sensitivity-Analysis" title="Permalink"></a></h2><p>This adjoint requires the definition of some scalar functional <span>$g(u,p)$</span> where <span>$u(t,p)$</span> is the (numerical) solution to the differential equation <span>$d/dt u(t,p)=f(t,u,p)$</span> with <span>$t\in [0,T]$</span> and <span>$u(t_0,p)=u_0$</span>. Adjoint sensitivity analysis finds the gradient of</p><p class="math-container">\[G(u,p)=G(u(\cdot,p))=\int_{t_{0}}^{T}g(u(t,p),p)dt\]</p><p>some integral of the solution. It does so by solving the adjoint problem</p><p class="math-container">\[\frac{d\lambda^{\star}}{dt}=g_{u}(u(t,p),p)-\lambda^{\star}(t)f_{u}(t,u(t,p),p),\thinspace\thinspace\thinspace\lambda^{\star}(T)=0\]</p><p>where <span>$f_u$</span> is the Jacobian of the system with respect to the state <span>$u$</span> while <span>$f_p$</span> is the Jacobian with respect to the parameters. The adjoint problem&#39;s solution gives the sensitivities through the integral:</p><p class="math-container">\[\frac{dG}{dp}=\int_{t_{0}}^{T}\lambda^{\star}(t)f_{p}(t)+g_{p}(t)dt+\lambda^{\star}(t_{0})u_{p}(t_{0})\]</p><p>Notice that since the adjoints require the Jacobian of the system at the state, it requires the ability to evaluate the state at any point in time. Thus it requires the continuous forward solution in order to solve the adjoint solution, and the adjoint solution is required to be continuous in order to calculate the resulting integral.</p><p>There is one extra detail to consider. In many cases we would like to calculate the adjoint sensitivity of some discontinuous functional of the solution. One canonical function is the L2 loss against some data points, that is:</p><p class="math-container">\[L(u,p)=\sum_{i=1}^{n}\Vert\tilde{u}(t_{i})-u(t_{i},p)\Vert^{2}\]</p><p>In this case, we can reinterpret our summation as the distribution integral:</p><p class="math-container">\[G(u,p)=\int_{0}^{T}\sum_{i=1}^{n}\Vert\tilde{u}(t_{i})-u(t_{i},p)\Vert^{2}\delta(t_{i}-t)dt\]</p><p>where <span>$Î´$</span> is the Dirac distribution. In this case, the integral is continuous except at finitely many points. Thus it can be calculated between each <span>$t_i$</span>. At a given <span>$t_i$</span>, given that the <span>$t_i$</span> are unique, we have that</p><p class="math-container">\[g_{u}(t_{i})=2\left(\tilde{u}(t_{i})-u(t_{i},p)\right)\]</p><p>Thus the adjoint solution <span>$\lambda^{\star}(t)$</span> is given by integrating between the integrals and applying the jump function <span>$g_u$</span> at every data point <span>$t_i$</span>.</p><p>We note that</p><p class="math-container">\[\lambda^{\star}(t)f_{u}(t)\]</p><p>is a vector-transpose Jacobian product, also known as a <code>vjp</code>, which can be efficiently computed using the pullback of backpropogation on the user function <code>f</code> with a forward pass at <code>u</code> with a pullback vector <span>$\lambda^{\star}$</span>. For more information, consult the <a href="https://mitmath.github.io/18337/lecture10/estimation_identification">MIT 18.337 lecture notes on reverse mode AD</a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../Benchmark/">Â« Benchmarks</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Sunday 3 July 2022 21:58">Sunday 3 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
