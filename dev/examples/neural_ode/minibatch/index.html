<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training a Neural Ordinary Differential Equation with Mini-Batching · SciMLSensitivity.jl</title><meta name="title" content="Training a Neural Ordinary Differential Equation with Mini-Batching · SciMLSensitivity.jl"/><meta property="og:title" content="Training a Neural Ordinary Differential Equation with Mini-Batching · SciMLSensitivity.jl"/><meta property="twitter:title" content="Training a Neural Ordinary Differential Equation with Mini-Batching · SciMLSensitivity.jl"/><meta name="description" content="Documentation for SciMLSensitivity.jl."/><meta property="og:description" content="Documentation for SciMLSensitivity.jl."/><meta property="twitter:description" content="Documentation for SciMLSensitivity.jl."/><meta property="og:url" content="https://docs.sciml.ai/SciMLSensitivity/stable/examples/neural_ode/minibatch/"/><meta property="twitter:url" content="https://docs.sciml.ai/SciMLSensitivity/stable/examples/neural_ode/minibatch/"/><link rel="canonical" href="https://docs.sciml.ai/SciMLSensitivity/stable/examples/neural_ode/minibatch/"/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">SciMLSensitivity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">SciMLSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><a class="tocitem" href="../../../getting_started/">Getting Started with SciMLSensitivity: Differentiating ODE Solutions</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/parameter_estimation_ode/">Parameter Estimation of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../../tutorials/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../../tutorials/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../../tutorials/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../../tutorials/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li><li><input class="collapse-toggle" id="menuitem-3-6" type="checkbox"/><label class="tocitem" for="menuitem-3-6"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../tutorials/training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../../tutorials/training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../../tutorials/training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li></ul></li><li><a class="tocitem" href="../../../faq/">Frequently Asked Questions (FAQ)</a></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Ordinary Differential Equations (ODEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ode/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../../ode/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../../ode/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../../ode/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-2" type="checkbox" checked/><label class="tocitem" for="menuitem-5-2"><span class="docs-label">Neural Ordinary Differential Equations (Neural ODE)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../simplechains/">Faster Neural Ordinary Differential Equations with SimpleChains</a></li><li class="is-active"><a class="tocitem" href>Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">Stochastic Differential Equations (SDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sde/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../sde/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-4" type="checkbox"/><label class="tocitem" for="menuitem-5-4"><span class="docs-label">Delay Differential Equations (DDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dde/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-5" type="checkbox"/><label class="tocitem" for="menuitem-5-5"><span class="docs-label">Partial Differential Equations (PDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../pde/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-6" type="checkbox"/><label class="tocitem" for="menuitem-5-6"><span class="docs-label">Hybrid and Jump Equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../hybrid_jump/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../hybrid_jump/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-7" type="checkbox"/><label class="tocitem" for="menuitem-5-7"><span class="docs-label">Bayesian Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-8" type="checkbox"/><label class="tocitem" for="menuitem-5-8"><span class="docs-label">Optimal and Model Predictive Control</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../../../manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../../manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../../manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../../../manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li><a class="is-disabled">Neural Ordinary Differential Equations (Neural ODE)</a></li><li class="is-active"><a href>Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/SciMLSensitivity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/SciMLSensitivity.jl/blob/master/docs/src/examples/neural_ode/minibatch.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-a-Neural-Ordinary-Differential-Equation-with-Mini-Batching"><a class="docs-heading-anchor" href="#Training-a-Neural-Ordinary-Differential-Equation-with-Mini-Batching">Training a Neural Ordinary Differential Equation with Mini-Batching</a><a id="Training-a-Neural-Ordinary-Differential-Equation-with-Mini-Batching-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-Neural-Ordinary-Differential-Equation-with-Mini-Batching" title="Permalink"></a></h1><pre><code class="language-julia hljs">using SciMLSensitivity
using DifferentialEquations, Flux, Random, Plots, MLUtils
using IterTools: ncycle

rng = Random.default_rng()

function newtons_cooling(du, u, p, t)
    temp = u[1]
    k, temp_m = p
    du[1] = dT = -k * (temp - temp_m)
end

function true_sol(du, u, p, t)
    true_p = [log(2) / 8.0, 100.0]
    newtons_cooling(du, u, true_p, t)
end

ann = Chain(Dense(1, 8, tanh), Dense(8, 1, tanh))
θ, re = Flux.destructure(ann)

function dudt_(u, p, t)
    re(p)(u)[1] .* u
end

function predict_adjoint(time_batch)
    _prob = remake(prob, u0 = u0, p = θ)
    Array(solve(_prob, Tsit5(), saveat = time_batch))
end

function loss_adjoint(batch, time_batch)
    pred = predict_adjoint(time_batch)
    sum(abs2, batch - pred)#, pred
end

u0 = Float32[200.0]
datasize = 30
tspan = (0.0f0, 3.0f0)

t = range(tspan[1], tspan[2], length = datasize)
true_prob = ODEProblem(true_sol, u0, tspan)
ode_data = Array(solve(true_prob, Tsit5(), saveat = t))

prob = ODEProblem{false}(dudt_, u0, tspan, θ)

k = 10
train_loader = DataLoader((ode_data, t), batchsize = k)

for (x, y) in train_loader
    @show x
    @show y
end

numEpochs = 300
losses = []
function cb()
    begin
        l = loss_adjoint(ode_data, t)
        push!(losses, l)
        @show l
        pred = predict_adjoint(t)
        pl = scatter(t, ode_data[1, :], label = &quot;data&quot;, color = :black, ylim = (150, 200))
        scatter!(pl, t, pred[1, :], label = &quot;prediction&quot;, color = :darkgreen)
        display(plot(pl))
        false
    end
end

opt = Adam(0.05)
Flux.train!(loss_adjoint, Flux.params(θ), ncycle(train_loader, numEpochs), opt,
    cb = Flux.throttle(cb, 10))

#Now lets see how well it generalizes to new initial conditions

starting_temp = collect(10:30:250)
true_prob_func(u0) = ODEProblem(true_sol, [u0], tspan)
color_cycle = palette(:tab10)
pl = plot()
for (j, temp) in enumerate(starting_temp)
    ode_test_sol = solve(ODEProblem(true_sol, [temp], (0.0f0, 10.0f0)), Tsit5(),
        saveat = 0.0:0.5:10.0)
    ode_nn_sol = solve(ODEProblem{false}(dudt_, [temp], (0.0f0, 10.0f0), θ))
    scatter!(pl, ode_test_sol, var = (0, 1), label = &quot;&quot;, color = color_cycle[j])
    plot!(pl, ode_nn_sol, var = (0, 1), label = &quot;&quot;, color = color_cycle[j], lw = 2.0)
end
display(pl)
title!(&quot;Neural ODE for Newton&#39;s Law of Cooling: Test Data&quot;)
xlabel!(&quot;Time&quot;)
ylabel!(&quot;Temp&quot;)</code></pre><img src="94e7746f.svg" alt="Example block output"/><p>When training a neural network, we need to find the gradient with respect to our data set. There are three main ways to partition our data when using a training algorithm like gradient descent: stochastic, batching and mini-batching. Stochastic gradient descent trains on a single random data point each epoch. This allows for the neural network to better converge to the global minimum even on noisy data, but is computationally inefficient. Batch gradient descent trains on the whole data set each epoch and while computationally efficient is prone to converging to local minima. Mini-batching combines both of these advantages and by training on a small random &quot;mini-batch&quot; of the data each epoch can converge to the global minimum while remaining more computationally efficient than stochastic descent. Typically, we do this by randomly selecting subsets of the data each epoch and use this subset to train on. We can also pre-batch the data by creating an iterator holding these randomly selected batches before beginning to train. The proper size for the batch can be determined experimentally. Let us see how to do this with Julia.</p><p>For this example, we will use a very simple ordinary differential equation, newtons law of cooling. We can represent this in Julia like so.</p><pre><code class="language-julia hljs">using SciMLSensitivity, MLUtils
using DifferentialEquations, Flux, Random, Plots
using IterTools: ncycle

rng = Random.default_rng()
function newtons_cooling(du, u, p, t)
    temp = u[1]
    k, temp_m = p
    du[1] = dT = -k * (temp - temp_m)
end

function true_sol(du, u, p, t)
    true_p = [log(2) / 8.0, 100.0]
    newtons_cooling(du, u, true_p, t)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true_sol (generic function with 1 method)</code></pre><p>Now we define a neural-network using a linear approximation with 1 hidden layer of 8 neurons.</p><pre><code class="language-julia hljs">ann = Chain(Dense(1, 8, tanh), Dense(8, 1, tanh))
θ, re = Flux.destructure(ann)

function dudt_(u, p, t)
    re(p)(u)[1] .* u
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">dudt_ (generic function with 1 method)</code></pre><p>From here we build a loss function around it.</p><pre><code class="language-julia hljs">function predict_adjoint(time_batch)
    _prob = remake(prob, u0 = u0, p = θ)
    Array(solve(_prob, Tsit5(), saveat = time_batch))
end

function loss_adjoint(batch, time_batch)
    pred = predict_adjoint(time_batch)
    sum(abs2, batch - pred)#, pred
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_adjoint (generic function with 1 method)</code></pre><p>To add support for batches of size <code>k</code> we use <code>Flux.Data.DataLoader</code>. To use this we pass in the <code>ode_data</code> and <code>t</code> as the &#39;x&#39; and &#39;y&#39; data to batch respectively. The parameter <code>batchsize</code> controls the size of our batches. We check our implementation by iterating over the batched data.</p><pre><code class="language-julia hljs">u0 = Float32[200.0]
datasize = 30
tspan = (0.0f0, 3.0f0)

t = range(tspan[1], tspan[2], length = datasize)
true_prob = ODEProblem(true_sol, u0, tspan)
ode_data = Array(solve(true_prob, Tsit5(), saveat = t))

prob = ODEProblem{false}(dudt_, u0, tspan, θ)

k = 10
train_loader = DataLoader((ode_data, t), batchsize = k)

for (x, y) in train_loader
    @show x
    @show y
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">x = Float32[200.0 199.1077 198.22334 197.3469 196.47826 195.61739 194.76419 193.9186 193.08055 192.24998]
y = 0.0f0:0.10344828f0:0.9310345f0
x = Float32[191.42683 190.61102 189.8025 189.00119 188.20702 187.41994 186.6399 185.8668 185.1006 184.34125]
y = 1.0344827f0:0.10344828f0:1.9655173f0
x = Float32[183.58865 182.84279 182.1036 181.37097 180.64491 179.92528 179.21214 178.50533 177.8048 177.11053]
y = 2.0689654f0:0.10344828f0:3.0f0</code></pre><p>Now we train the neural network with a user-defined call back function to display loss and the graphs with a maximum of 300 epochs.</p><pre><code class="language-julia hljs">numEpochs = 300
losses = []
function cb()
    begin
        l = loss_adjoint(ode_data, t)
        push!(losses, l)
        @show l
        pred = predict_adjoint(t)
        pl = scatter(t, ode_data[1, :], label = &quot;data&quot;, color = :black, ylim = (150, 200))
        scatter!(pl, t, pred[1, :], label = &quot;prediction&quot;, color = :darkgreen)
        display(plot(pl))
        false
    end
end

opt = Adam(0.05)
Flux.train!(loss_adjoint, Flux.params(θ), ncycle(train_loader, numEpochs), opt,
    cb = Flux.throttle(cb, 10))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">l = 516389.38f0</code></pre><p>Finally, we can see how well our trained network will generalize to new initial conditions.</p><pre><code class="language-julia hljs">starting_temp = collect(10:30:250)
true_prob_func(u0) = ODEProblem(true_sol, [u0], tspan)
color_cycle = palette(:tab10)
pl = plot()
for (j, temp) in enumerate(starting_temp)
    ode_test_sol = solve(ODEProblem(true_sol, [temp], (0.0f0, 10.0f0)), Tsit5(),
        saveat = 0.0:0.5:10.0)
    ode_nn_sol = solve(ODEProblem{false}(dudt_, [temp], (0.0f0, 10.0f0), θ))
    scatter!(pl, ode_test_sol, var = (0, 1), label = &quot;&quot;, color = color_cycle[j])
    plot!(pl, ode_nn_sol, var = (0, 1), label = &quot;&quot;, color = color_cycle[j], lw = 2.0)
end
display(pl)
title!(&quot;Neural ODE for Newton&#39;s Law of Cooling: Test Data&quot;)
xlabel!(&quot;Time&quot;)
ylabel!(&quot;Temp&quot;)</code></pre><img src="00ed1dc9.svg" alt="Example block output"/></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../simplechains/">« Faster Neural Ordinary Differential Equations with SimpleChains</a><a class="docs-footer-nextpage" href="../../sde/optimization_sde/">Optimization of Stochastic Differential Equations »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Monday 18 November 2024 21:01">Monday 18 November 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
