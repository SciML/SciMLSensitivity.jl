<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Prediction error method (PEM) · SciMLSensitivity.jl</title><meta name="title" content="Prediction error method (PEM) · SciMLSensitivity.jl"/><meta property="og:title" content="Prediction error method (PEM) · SciMLSensitivity.jl"/><meta property="twitter:title" content="Prediction error method (PEM) · SciMLSensitivity.jl"/><meta name="description" content="Documentation for SciMLSensitivity.jl."/><meta property="og:description" content="Documentation for SciMLSensitivity.jl."/><meta property="twitter:description" content="Documentation for SciMLSensitivity.jl."/><meta property="og:url" content="https://docs.sciml.ai/SciMLSensitivity/stable/examples/ode/prediction_error_method/"/><meta property="twitter:url" content="https://docs.sciml.ai/SciMLSensitivity/stable/examples/ode/prediction_error_method/"/><link rel="canonical" href="https://docs.sciml.ai/SciMLSensitivity/stable/examples/ode/prediction_error_method/"/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img src="../../../assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../../">SciMLSensitivity.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">SciMLSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><a class="tocitem" href="../../../getting_started/">Getting Started with SciMLSensitivity: Differentiating ODE Solutions</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/parameter_estimation_ode/">Parameter Estimation of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../../tutorials/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../../tutorials/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../../tutorials/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../../tutorials/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li><li><input class="collapse-toggle" id="menuitem-3-6" type="checkbox"/><label class="tocitem" for="menuitem-3-6"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../tutorials/training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../../tutorials/training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../../tutorials/training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox" checked/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Ordinary Differential Equations (ODEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../exogenous_input/">Handling Exogenous Input Signals</a></li><li class="is-active"><a class="tocitem" href>Prediction error method (PEM)</a></li><li><a class="tocitem" href="../second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox"/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Neural Ordinary Differential Equations (Neural ODE)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../../neural_ode/simplechains/">Neural Ordinary Differential Equations with SimpleChains</a></li><li><a class="tocitem" href="../../neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../../neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Stochastic Differential Equations (SDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sde/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../sde/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Delay Differential Equations (DDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dde/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Differential-Algebraic Equations (DAEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dae/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-6" type="checkbox"/><label class="tocitem" for="menuitem-4-6"><span class="docs-label">Partial Differential Equations (PDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../pde/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-7" type="checkbox"/><label class="tocitem" for="menuitem-4-7"><span class="docs-label">Hybrid and Jump Equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../hybrid_jump/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../hybrid_jump/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-8" type="checkbox"/><label class="tocitem" for="menuitem-4-8"><span class="docs-label">Bayesian Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-9" type="checkbox"/><label class="tocitem" for="menuitem-4-9"><span class="docs-label">Optimal and Model Predictive Control</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../../../manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../../manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../../manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../../../manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li><a class="is-disabled">Ordinary Differential Equations (ODEs)</a></li><li class="is-active"><a href>Prediction error method (PEM)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Prediction error method (PEM)</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/DiffEqSensitivity.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/master/docs/src/examples/ode/prediction_error_method.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="pemethod"><a class="docs-heading-anchor" href="#pemethod">Prediction error method (PEM)</a><a id="pemethod-1"></a><a class="docs-heading-anchor-permalink" href="#pemethod" title="Permalink"></a></h1><p>When identifying linear systems from noisy data, the prediction-error method <sup class="footnote-reference"><a id="citeref-Ljung" href="#footnote-Ljung">[Ljung]</a></sup> is close to a gold standard when it comes to the quality of the models it produces, but is also one of the computationally more expensive methods due to its reliance on iterative, gradient-based estimation. When we are identifying nonlinear models, we typically do not have the luxury of closed-form, non-iterative solutions, while PEM is easier to adapt to the nonlinear setting.<sup class="footnote-reference"><a id="citeref-Larsson" href="#footnote-Larsson">[Larsson]</a></sup></p><p>Fundamentally, PEM changes the problem from minimizing a loss based on the simulation performance, to minimizing a loss based on shorter-term predictions. There are several benefits of doing so, and this example will highlight two:</p><ul><li>The loss is often easier to optimize.</li><li>In addition to an accurate simulator, you also obtain a prediction for the system.</li><li>With PEM, it&#39;s possible to estimate <em>disturbance models</em>.</li></ul><p>The last point will not be illustrated in this tutorial, but we will briefly expand upon it here. Gaussian, zero-mean measurement noise is usually not very hard to handle. Disturbances that affect the state of the system may, however, cause all sorts of havoc on the estimate. Consider wind affecting an aircraft, deriving a statistical and dynamical model of the wind may be doable, but unless you measure the exact wind affecting the aircraft, making use of the model during parameter estimation is impossible. The wind is an <em>unmeasured load disturbance</em> that affects the state of the system through its own dynamics model. Using the techniques illustrated in this tutorial, it&#39;s possible to estimate the influence of the wind during the experiment that generated the data and reduce or eliminate the bias it otherwise causes in the parameter estimates.</p><p>We will start by illustrating a common problem with simulation-error minimization. Imagine a pendulum with unknown length that is to be estimated. A small error in the pendulum length causes the frequency of oscillation to change. Over sufficiently large horizon, two sinusoidal signals with different frequencies become close to orthogonal to each other. If some form of squared-error loss is used, the loss landscape will be horribly non-convex in this case, indeed, we will illustrate exactly this below.</p><p>Another case that poses a problem for simulation-error estimation is when the system is unstable or chaotic. A small error in either the initial condition or the parameters may cause the simulation error to diverge and its gradient to become meaningless.</p><p>In both of these examples, we may make use of measurements we have of the evolution of the system to prevent the simulation error from diverging. For instance, if we have measured the angle of the pendulum, we can make use of this measurement to adjust the angle during the simulation to make sure it stays close to the measured angle. Instead of performing a pure simulation, we instead say that we <em>predict</em> the state a while forward in time, given all the measurements until the current time point. By minimizing this prediction rather than the pure simulation, we can often prevent the model error from diverging even though we have a poor initial guess.</p><p>We start by defining a model of the pendulum. The model takes a parameter <span>$L$</span> corresponding to the length of the pendulum.</p><pre><code class="language-julia hljs">using DifferentialEquations, Optimization, OptimizationPolyalgorithms, Plots, Statistics,
    DataInterpolations, ForwardDiff

tspan = (0.1, 20.0)
tsteps = range(tspan[1], tspan[2], length = 1000)

u0 = [0.0, 3.0] # Initial angle and angular velocity

function simulator(du, u, p, t) # Pendulum dynamics
    g = 9.82 # Gravitational constant
    L = p isa Number ? p : p[1] # Length of the pendulum
    gL = g / L
    θ = u[1]
    dθ = u[2]
    du[1] = dθ
    du[2] = -gL * sin(θ)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">simulator (generic function with 1 method)</code></pre><p>We assume that the true length of the pendulum is <span>$L = 1$</span>, and generate some data from this system.</p><pre><code class="language-julia hljs">prob = ODEProblem(simulator, u0, tspan, 1.0) # Simulate with L = 1
sol = solve(prob, Tsit5(), saveat = tsteps, abstol = 1e-8, reltol = 1e-8)
y = sol[1, :] # This is the data we have available for parameter estimation
plot(y, title = &quot;Pendulum simulation&quot;, label = &quot;angle&quot;)</code></pre><img src="d16c962b.svg" alt="Example block output"/><p>We also define functions that simulate the system and calculate the loss, given a parameter <code>p</code> corresponding to the length.</p><pre><code class="language-julia hljs">function simulate(p)
    _prob = remake(prob, p = p)
    solve(_prob, Tsit5(), saveat = tsteps, abstol = 1e-8, reltol = 1e-8)
end

function simloss(p)
    yh = simulate(p)
    if !SciMLBase.successful_retcode(yh.retcode)
        return Inf
    end
    e2 = yh[1, :]
    e2 .= abs2.(y .- e2)
    return mean(e2)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">simloss (generic function with 1 method)</code></pre><p>We now look at the loss landscape as a function of the pendulum length:</p><pre><code class="language-julia hljs">Ls = 0.01:0.01:2
simlosses = simloss.(Ls)
fig_loss = plot(Ls, simlosses, title = &quot;Loss landscape&quot;, xlabel = &quot;Pendulum length&quot;,
    ylabel = &quot;MSE loss&quot;, lab = &quot;Simulation loss&quot;)</code></pre><img src="76a62d56.svg" alt="Example block output"/><p>This figure is interesting, the loss is of course 0 for the true value <span>$L=1$</span>, but for values <span>$L &lt; 1$</span>, the overall slope actually points in the wrong direction! Moreover, the loss is oscillatory, indicating that this is a terrible function to optimize, and that we would need a very good initial guess for a local search to converge to the true value. Note, this example is chosen to be one-dimensional in order to allow these kinds of visualizations, and one-dimensional problems are typically not hard to solve, but the reasoning extends to higher-dimensional and harder problems.</p><p>We will now move on to defining a <em>predictor</em> model. Our predictor will be very simple, each time step, we will calculate the error <span>$e$</span> between the simulated angle <span>$\theta$</span> and the measured angle <span>$y$</span>. A part of this error will be used to correct the state of the pendulum. The correction we use is linear and looks like <span>$Ke = K(y - \theta)$</span>. We have formed what is commonly referred to as a (linear) <em>observer</em>. The <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a> is a particular kind of linear observer, where <span>$K$</span> is calculated based on a statistical model of the disturbances that act on the system. We will stay with a simple, fixed-gain observer here for simplicity.</p><p>To feed the sampled data into the continuous-time simulation, we make use of an interpolator. We also define new functions, <code>predictor</code> that contains the pendulum dynamics with the observer correction, a <code>prediction</code> function that performs the rollout (we&#39;re not using the word simulation to not confuse with the setting above) and a loss function.</p><pre><code class="language-julia hljs">y_int = LinearInterpolation(y, tsteps)

function predictor(du, u, p, t)
    g = 9.82
    L, K, y = p # pendulum length, observer gain and measurements
    gL = g / L
    θ = u[1]
    dθ = u[2]
    yt = y(t)
    e = yt - θ
    du[1] = dθ + K * e
    du[2] = -gL * sin(θ)
end

predprob = ODEProblem(predictor, u0, tspan, nothing)

function prediction(p)
    p_full = (p..., y_int)
    _prob = remake(predprob, u0 = eltype(p).(u0), p = p_full)
    solve(_prob, Tsit5(), saveat = tsteps, abstol = 1e-8, reltol = 1e-8)
end

function predloss(p)
    yh = prediction(p)
    if !SciMLBase.successful_retcode(yh.retcode)
        return Inf
    end
    e2 = yh[1, :]
    e2 .= abs2.(y .- e2)
    return mean(e2)
end

predlosses = map(Ls) do L
    p = (L, 1) # use K = 1
    predloss(p)
end

plot!(Ls, predlosses, lab = &quot;Prediction loss&quot;)</code></pre><img src="0bbb8a1d.svg" alt="Example block output"/><p>Once gain, we look at the loss as a function of the parameter, and this time it looks a lot better. The loss is not convex, but the gradient points in the right direction over a much larger interval. Here, we arbitrarily set the observer gain to <span>$K=1$</span>, we will later let the optimizer learn this parameter.</p><p>For completeness, we also perform estimation using both losses. We choose an initial guess we know will be hard for the simulation-error minimization just to drive home the point:</p><pre><code class="language-julia hljs">L0 = [0.7] # Initial guess of pendulum length
adtype = Optimization.AutoForwardDiff()
optf = Optimization.OptimizationFunction((x, p) -&gt; simloss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, L0)

ressim = Optimization.solve(optprob, PolyOpt(),
    maxiters = 5000)
ysim = simulate(ressim.u)[1, :]

plot(tsteps, [y ysim], label = [&quot;Data&quot; &quot;Simulation model&quot;])

p0 = [0.7, 1.0] # Initial guess of length and observer gain K
optf2 = Optimization.OptimizationFunction((p, _) -&gt; predloss(p), adtype)
optprob2 = Optimization.OptimizationProblem(optf2, p0)

respred = Optimization.solve(optprob2, PolyOpt(),
    maxiters = 5000)
ypred = simulate(respred.u)[1, :]

plot!(tsteps, ypred, label = &quot;Prediction model&quot;)</code></pre><img src="9b84dbce.svg" alt="Example block output"/><p>The estimated parameters <span>$(L, K)$</span> are</p><pre><code class="language-julia hljs">respred.u</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.000068857525122
 1.2905721812688222</code></pre><p>Now, we might ask ourselves why we used a correct on the form <span>$Ke$</span> and didn&#39;t instead set the angle in the simulation <em>equal</em> to the measurement. The reason is twofold</p><ol><li>If our prediction of the angle is 100% based on the measurements, the model parameters do not matter for the prediction, and we thus cannot hope to learn their values.</li><li>The measurement is usually noisy, and we thus want to <em>fuse</em> the predictive power of the model with the information of the measurements. The Kalman filter is an optimal approach to this information fusion under special circumstances (linear model, Gaussian noise).</li></ol><p>We thus let the optimization <em>learn</em> the best value of the observer gain in order to make the best predictions.</p><p>As a last step, we perform the estimation also with some measurement noise to verify that it does something reasonable:</p><pre><code class="language-julia hljs">yn = y .+ 0.1f0 .* randn.(Float32)
y_int = LinearInterpolation(yn, tsteps) # redefine the interpolator to contain noisy measurements

optf = Optimization.OptimizationFunction((x, p) -&gt; predloss(x), adtype)
optprob = Optimization.OptimizationProblem(optf, p0)

resprednoise = Optimization.solve(optprob, PolyOpt(),
    maxiters = 5000)

yprednoise = prediction(resprednoise.u)[1, :]
plot!(tsteps, yprednoise, label = &quot;Prediction model with noisy measurements&quot;)</code></pre><img src="01789d56.svg" alt="Example block output"/><pre><code class="language-julia hljs">resprednoise.u</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.9999999984983569
 3.963140471083158e-7</code></pre><p>This example has illustrated basic use of the prediction-error method for parameter estimation. In our example, the measurement we had corresponded directly to one of the states, and coming up with an observer/predictor that worked was not too hard. For more difficult cases, we may opt to use a nonlinear observer, such as an extended Kalman filter (EKF) or design a Kalman filter based on a linearization of the system around some operating point.</p><p>As a last note, there are several other methods available to improve the loss landscape and avoid local minima, such as multiple-shooting. The prediction-error method can easily be combined with most of those methods.</p><p>References:</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-Ljung"><a class="tag is-link" href="#citeref-Ljung">Ljung</a>Ljung, Lennart. &quot;System identification–-Theory for the user&quot;.</li><li class="footnote" id="footnote-Larsson"><a class="tag is-link" href="#citeref-Larsson">Larsson</a>Larsson, Roger, et al. &quot;Direct prediction-error identification of unstable nonlinear systems applied to flight test data.&quot;</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../exogenous_input/">« Handling Exogenous Input Signals</a><a class="docs-footer-nextpage" href="../second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.2 on <span class="colophon-date" title="Saturday 28 October 2023 13:51">Saturday 28 October 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
