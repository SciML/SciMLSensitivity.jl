<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sensitivity Analysis of Differential Equations · DiffEqSensitivity.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://sensitivity.sciml.ai/stable/manual/differential_equation_sensitivities/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DiffEqSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DiffEqSensitivity.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DiffEqSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><span class="tocitem">Differentiating Ordinary Differential Equations (ODE) Tutorials</span><ul><li><a class="tocitem" href="../../ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation</a></li><li><a class="tocitem" href="../../ad_examples/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li></ul></li><li><span class="tocitem">Fitting Ordinary Differential Equation (ODE) Tutorials</span><ul><li><a class="tocitem" href="../../ode_fitting/optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../ode_fitting/stiff_ode_fit/">Parameter Estimation on Highly Stiff Systems</a></li><li><a class="tocitem" href="../../ode_fitting/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../../ode_fitting/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../ode_fitting/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../../ode_fitting/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../../ode_fitting/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><span class="tocitem">Training Techniques and Tips</span><ul><li><a class="tocitem" href="../../training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li><li><span class="tocitem">Neural Ordinary Differential Equation (Neural ODE) Tutorials</span><ul><li><a class="tocitem" href="../../neural_ode/neural_ode_galacticoptim/">Neural Ordinary Differential Equations with GalacticOptim.jl</a></li><li><a class="tocitem" href="../../neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux.train!</a></li><li><a class="tocitem" href="../../neural_ode/mnist_neural_ode/">GPU-based MNIST Neural ODE Classifier</a></li><li><a class="tocitem" href="../../neural_ode/mnist_conv_neural_ode/">Convolutional Neural ODE MNIST Classifier on GPU</a></li><li><a class="tocitem" href="../../neural_ode/GPUs/">Neural ODEs on GPUs</a></li><li><a class="tocitem" href="../../neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../../neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><span class="tocitem">Stochastic Differential Equation (SDE) Tutorials</span><ul><li><a class="tocitem" href="../../sde_fitting/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../sde_fitting/neural_sde/">Neural Stochastic Differential Equations</a></li></ul></li><li><span class="tocitem">Delay Differential Equation (DDE) Tutorials</span><ul><li><a class="tocitem" href="../../dde_fitting/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><span class="tocitem">Differential-Algebraic Equation (DAE) Tutorials</span><ul><li><a class="tocitem" href="../../dae_fitting/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><span class="tocitem">Partial Differential Equation (PDE) Tutorials</span><ul><li><a class="tocitem" href="../../pde_fitting/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><span class="tocitem">Hybrid and Jump Equation Tutorials</span><ul><li><a class="tocitem" href="../../hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../hybrid_jump_fitting/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><span class="tocitem">Bayesian Estimation Tutorials</span><ul><li><a class="tocitem" href="../../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li><li><a class="tocitem" href="../../bayesian/BayesianNODE_NUTS/">Bayesian Neural ODEs: NUTS</a></li><li><a class="tocitem" href="../../bayesian/BayesianNODE_SGLD/">Bayesian Neural ODEs: SGLD</a></li></ul></li><li><span class="tocitem">Optimal and Model Predictive Control Tutorials</span><ul><li><a class="tocitem" href="../../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../../optimal_control/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li class="is-active"><a class="tocitem" href>Sensitivity Analysis of Differential Equations</a><ul class="internal"><li><a class="tocitem" href="#Sensitivity-Algorithms"><span>Sensitivity Algorithms</span></a></li><li><a class="tocitem" href="#Choosing-a-sensealg-in-a-Nutshell"><span>Choosing a sensealg in a Nutshell</span></a></li><li><a class="tocitem" href="#Additional-Details"><span>Additional Details</span></a></li><li><a class="tocitem" href="#Choices-of-Vector-Jacobian-Products-(autojacvec)"><span>Choices of Vector-Jacobian Products (autojacvec)</span></a></li><li><a class="tocitem" href="#Manual-VJPs"><span>Manual VJPs</span></a></li><li><a class="tocitem" href="#Optimize-then-Discretize"><span>Optimize-then-Discretize</span></a></li><li><a class="tocitem" href="#Discretize-then-Optimize"><span>Discretize-then-Optimize</span></a></li><li class="toplevel"><a class="tocitem" href="#Special-Notes-on-Equation-Types"><span>Special Notes on Equation Types</span></a></li><li><a class="tocitem" href="#Differential-Algebraic-Equations"><span>Differential-Algebraic Equations</span></a></li><li><a class="tocitem" href="#Stochastic-Differential-Equations"><span>Stochastic Differential Equations</span></a></li><li><a class="tocitem" href="#Delay-Differential-Equations"><span>Delay Differential Equations</span></a></li><li class="toplevel"><a class="tocitem" href="#Controlling-Automatic-Differentiation"><span>Controlling Automatic Differentiation</span></a></li><li><a class="tocitem" href="#Choosing-a-Differentiation-Method"><span>Choosing a Differentiation Method</span></a></li></ul></li><li><a class="tocitem" href="../nonlinear_solve_sensitivities/">-</a></li><li><a class="tocitem" href="../direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual and APIs</a></li><li class="is-active"><a href>Sensitivity Analysis of Differential Equations</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sensitivity Analysis of Differential Equations</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/master/docs/src/manual/differential_equation_sensitivities.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sensitivity_diffeq"><a class="docs-heading-anchor" href="#sensitivity_diffeq">Sensitivity Analysis of Differential Equations</a><a id="sensitivity_diffeq-1"></a><a class="docs-heading-anchor-permalink" href="#sensitivity_diffeq" title="Permalink"></a></h1><p>DiffEqFlux is capable of training neural networks embedded inside of differential equations with many different techniques. For all of the details, see the <a href="https://diffeq.sciml.ai/latest/analysis/sensitivity/">DifferentialEquations.jl local sensitivity analysis</a> documentation. Here we will summarize these methodologies in the context of neural differential equations and scientific machine learning.</p><h2 id="Sensitivity-Algorithms"><a class="docs-heading-anchor" href="#Sensitivity-Algorithms">Sensitivity Algorithms</a><a id="Sensitivity-Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Sensitivity-Algorithms" title="Permalink"></a></h2><p>The following algorithm choices exist for <code>sensealg</code>. See <a href="../../sensitivity_math/#sensitivity_math">the sensitivity mathematics page</a> for more details on the definition of the methods.</p><ul><li><code>ForwardSensitivity(;ADKwargs...)</code>: An implementation of continuous forward sensitivity analysis for propagating derivatives by solving the extended ODE. Only supports ODEs.</li><li><code>ForwardDiffSensitivity(;chunk_size=0,convert_tspan=true)</code>: An implementation of discrete forward sensitivity analysis through ForwardDiff.jl.</li><li><code>BacksolveAdjoint(;checkpointing=true,ADKwargs...)</code>: An implementation of adjoint sensitivity analysis using a backwards solution of the ODE. By default this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot, allowing reduced memory with stabilization. Only supports ODEs and SDEs.</li><li><code>InterpolatingAdjoint(;checkpointing=false;ADKwargs...)</code>: The default. An implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products. By default it requires a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation. When checkpointing is enabled it will only require the memory to interpolate between checkpoints. Only supports ODEs and SDEs.</li><li><code>QuadratureAdjoint(;abstol=1e-6,reltol=1e-3,compile=false,ADKwargs...)</code>: An implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature. This method requires the the dense solution and will ignore saving arguments during the gradient calculation. The tolerances in the constructor control the inner quadrature. The inner quadrature uses a ReverseDiff vjp if autojacvec, and <code>compile=false</code> by default but can compile the tape under the same circumstances as <code>ReverseDiffVJP</code>. Only supports ODEs.</li><li><code>ReverseDiffAdjoint()</code>: An implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.</li><li><code>TrackerAdjoint()</code>: An implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.</li><li><code>ZygoteAdjoint()</code>: An implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver. Currently fails.</li><li><code>SensitivityADPassThrough()</code>: Ignores all adjoint definitions and proceeds to do standard AD through the <code>solve</code> functions.</li><li><code>ForwardLSS()</code>, <code>AdjointLSS()</code>, <code>NILSS(nseg,nstep)</code>, <code>NILSAS(nseg,nstep,M)</code>: Implementation of shadowing methods for chaotic systems with a long-time averaged objective. See the <a href="../../ad_examples/chaotic_ode/#shadowing_methods">sensitivity analysis for chaotic systems (shadowing methods) section</a> for more details.</li></ul><p>The <code>ReverseDiffAdjoint()</code>, <code>TrackerAdjoint()</code>, <code>ZygoteAdjoint()</code>, and <code>SensitivityADPassThrough()</code> algorithms all offer differentiate-through-the-solver adjoints, each based on their respective automatic differentiation packages. If you&#39;re not sure which to use, <code>ReverseDiffAdjoint()</code> is generally a stable and performant best if using the CPU, while <code>TrackerAdjoint()</code> is required if you need GPUs. Note that <code>SensitivityADPassThrough()</code> is more or less an internal implementation detail. For example, <code>ReverseDiffAdjoint()</code> is implemented by invoking <code>ReverseDiff</code>&#39;s AD functionality on <code>solve(...; sensealg=SensitivityADPassThrough())</code>.</p><p><code>ForwardDiffSensitivity</code> can differentiate code with callbacks when <code>convert_tspan=true</code>, but will be faster when <code>convert_tspan=false</code>. All methods based on discrete adjoint sensitivity analysis via automatic differentiation, like <code>ReverseDiffAdjoint</code>, <code>TrackerAdjoint</code>, or <code>QuadratureAdjoint</code> are fully compatible with events. This applies to ODEs, SDEs, DAEs, and DDEs. The continuous adjoint sensitivities <code>BacksolveAdjoint</code>, <code>InterpolatingAdjoint</code>, and <code>QuadratureAdjoint</code> are compatible with events for ODEs. <code>BacksolveAdjoint</code> and <code>InterpolatingAdjoint</code> can also handle events for SDEs. Use <code>BacksolveAdjoint</code> if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point. The shadowing methods are not compatible with callbacks.</p><h3 id="Internal-Automatic-Differentiation-Options-(ADKwargs)"><a class="docs-heading-anchor" href="#Internal-Automatic-Differentiation-Options-(ADKwargs)">Internal Automatic Differentiation Options (ADKwargs)</a><a id="Internal-Automatic-Differentiation-Options-(ADKwargs)-1"></a><a class="docs-heading-anchor-permalink" href="#Internal-Automatic-Differentiation-Options-(ADKwargs)" title="Permalink"></a></h3><p>Many sensitivity algorithms share the same options for controlling internal use of automatic differentiation. The following arguments constitute the <code>ADKwargs</code>:</p><ul><li><code>autodiff</code>: Use automatic differentiation in the internal sensitivity algorithm computations. Default is <code>true</code>.</li><li><code>chunk_size</code>: Chunk size for forward mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</li><li><code>autojacvec</code>: Calculate the Jacobian-vector (forward sensitivity) or vector-Jacobian (adjoint sensitivity analysis) product via automatic differentiation with special seeding. For adjoint methods this option requires <code>autodiff=true</code>. If <code>autojacvec=false</code>, then a full Jacobian has to be computed, and this will default to using a <code>f.jac</code> function provided by the user from the problem of the forward pass. Otherwise, if <code>autodiff=true</code> and <code>autojacvec=false</code> then it will use forward-mode AD for the Jacobian, otherwise it will fall back to using a numerical approximation to the Jacobian. Additionally, if the method is an adjoint method, there are three choices which can be made explicitly. The default vjp choice is a polyalgorithm that uses a compiler analysis to choose the most efficient vjp for a given code.<ul><li><code>TrackerVJP</code>: Uses Tracker.jl for the vjp.</li><li><code>ZygoteVJP</code>: Uses Zygote.jl for the vjp.</li><li><code>EnzymeVJP</code>: Uses Enzyme.jl for the vjp.</li><li><code>ReverseDiffVJP(compile=false)</code>: Uses ReverseDiff.jl for the vjp. <code>compile</code> is a boolean for whether to precompile the tape, which should only be done if there are no branches (<code>if</code> or <code>while</code> statements) in the <code>f</code> function. When applicable, <code>ReverseDiffVJP(true)</code> is the fastest method, and then <code>ReverseDiffVJP(false)</code> is the second fastest, but this method is not compatible with third party libraries like Flux.jl, FFTW.jl, etc. (only linear algebra and basic mathematics is supported) so it should be swapped in only as an optimization.</li></ul></li></ul><p>Note that the Jacobian-vector products and vector-Jacobian products can be directly specified by the user using the <a href="manual/@ref performance_overloads">performance overloads</a>.</p><h3 id="Choosing-a-Sensitivity-Algorithm"><a class="docs-heading-anchor" href="#Choosing-a-Sensitivity-Algorithm">Choosing a Sensitivity Algorithm</a><a id="Choosing-a-Sensitivity-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-a-Sensitivity-Algorithm" title="Permalink"></a></h3><p>For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem, consult our analysis <a href="https://arxiv.org/abs/1812.01892">in this arxiv paper</a>. A general rule of thumb is:</p><ul><li><code>ForwardDiffSensitivity</code> is the fastest for differential equations with small numbers of parameters (&lt;100) and can be used on any differential equation solver that is native Julia.</li><li>Adjoint senstivity analysis is the fastest when the number of parameters is sufficiently large. There are three configurations of note. Using <code>QuadratureAdjoint</code> is the fastest for small systems, <code>BacksolveAdjoint</code> uses the least memory but on very stiff problems it may be unstable and require a lot of checkpoints, while <code>InterpolatingAdjoint</code> is in the middle, allowing checkpointing to control total memory use.</li><li>The methods which use automatic differentiation (<code>ReverseDiffAdjoint</code>, <code>TrackerAdjoint</code>, <code>ForwardDiffSensitivity</code>, and <code>ZygoteAdjoint</code>) support the full range of DifferentialEquations.jl features (SDEs, DDEs, events, etc.), but only work on native Julia solvers. The methods which utilize altered differential equation systems only work on ODEs (without events), but work on any ODE solver.</li><li>For non-ODEs with large numbers of parameters, <code>TrackerAdjoint</code> in out-of-place form may be the best performer.</li><li><code>TrackerAdjoint</code> is able to use a <code>TrackedArray</code> form with out-of-place functions <code>du = f(u,p,t)</code> but requires an <code>Array{TrackedReal}</code> form for <code>f(du,u,p,t)</code> mutating <code>du</code>. The latter has much more overhead, and should be avoided if possible. Thus if solving non-ODEs with lots of parameters, using <code>TrackerAdjoint</code> with an out-of-place definition may be the current best option.</li></ul><h2 id="Choosing-a-sensealg-in-a-Nutshell"><a class="docs-heading-anchor" href="#Choosing-a-sensealg-in-a-Nutshell">Choosing a sensealg in a Nutshell</a><a id="Choosing-a-sensealg-in-a-Nutshell-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-a-sensealg-in-a-Nutshell" title="Permalink"></a></h2><p>By default, a stable adjoint with an auto-adapting vjp choice is used. In many cases, a user can optimize the choice to compute more than an order of magnitude faster than the default. However, given the vast space to explore, use the following decision tree to help guide the choice:</p><ul><li>If you have 100 parameters or less, consider using forward-mode sensititivites. If the <code>f</code> function is not ForwardDiff-compatible, use <code>ForwardSensitivty</code>, otherwise use <code>ForwardDiffSensitivty</code> as its more efficient.</li><li>For larger equations, give <code>BacksolveAdjoint</code> and <code>InterpolatingAdjoint</code> a try. If the gradient of <code>BacksolveAdjoint</code> is correct, many times it&#39;s the faster choice so choose that (but it&#39;s not always faster!). If your equation is stiff or a DAE, skip this step as <code>BacksolveAdjoint</code> is almost certainly unstable.</li><li>If your equation does not use much memory and you&#39;re using a stiff solver, consider using <code>QuadratureAdjoint</code> as it is asymtopically more computationally efficient by trading off memory cost.</li><li>If the other methods are all unstable (check the gradients against each other!), then <code>ReverseDiffAdjoint</code> is a good fallback on CPU, while <code>TrackerAdjoint</code> is a good fallback on GPUs.</li><li>After choosing a general sensealg, if the choice is <code>InterpolatingAdjoint</code>, <code>QuadratureAdjoint</code>, or <code>BacksolveAdjoint</code>, then optimize the choice of vjp calculation next:<ul><li>If your function has no branching (no if statements), use <code>ReverseDiffVJP(true)</code>.</li><li>If you&#39;re on the CPU and your function is very scalarized in operations but has branches, choose <code>ReverseDiffVJP()</code>.</li><li>If your on the CPU or GPU and your function is very vectorized, choose <code>ZygoteVJP()</code>.</li><li>Else fallback to <code>TrackerVJP()</code> if Zygote does not support the function.</li></ul></li></ul><h2 id="Additional-Details"><a class="docs-heading-anchor" href="#Additional-Details">Additional Details</a><a id="Additional-Details-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-Details" title="Permalink"></a></h2><p>A sensitivity analysis method can be passed to a solver via the <code>sensealg</code> keyword argument. For example:</p><pre><code class="language-julia hljs">solve(prob,Tsit5(),sensealg=BacksolveAdjoint(autojacvec=ZygoteVJP()))</code></pre><p>sets the adjoint sensitivity analysis so that, when this call is encountered in the gradient calculation of any of the Julia reverse-mode AD frameworks, the differentiation will be replaced with the <code>BacksolveAdjoint</code> method where internal vector-Jacobian products are performed using Zygote.jl. From the <a href="https://diffeq.sciml.ai/latest/analysis/sensitivity/">DifferentialEquations.jl local sensitivity analysis</a> page, we note that the following choices for <code>sensealg</code> exist:</p><ul><li><code>BacksolveAdjoint</code></li><li><code>InterpolatingAdjoint</code> (with checkpoints)</li><li><code>QuadratureAdjoint</code></li><li><code>TrackerAdjoint</code></li><li><code>ReverseDiffAdjoint</code> (currently requires <code>using DistributionsAD</code>)</li><li><code>ZygoteAdjoint</code> (currently limited to special solvers)</li></ul><p>Additionally, there are methodologies for forward sensitivity analysis:</p><ul><li><code>ForwardSensitivty</code></li><li><code>ForwardDiffSensitivty</code></li></ul><p>These methods have very low overhead compared to adjoint methods but have poor scaling with respect to increased numbers of parameters. <a href="https://arxiv.org/abs/1812.01892">Our benchmarks demonstrate a cutoff of around 100 parameters</a>, where for models with less than 100 parameters these techniques are more efficient, but when there are more than 100 parameters (like in neural ODEs) these methods are less efficient than the adjoint methods.</p><h2 id="Choices-of-Vector-Jacobian-Products-(autojacvec)"><a class="docs-heading-anchor" href="#Choices-of-Vector-Jacobian-Products-(autojacvec)">Choices of Vector-Jacobian Products (autojacvec)</a><a id="Choices-of-Vector-Jacobian-Products-(autojacvec)-1"></a><a class="docs-heading-anchor-permalink" href="#Choices-of-Vector-Jacobian-Products-(autojacvec)" title="Permalink"></a></h2><p>With each of these solvers, <code>autojacvec</code> can be utilized to choose how the internal vector-Jacobian products of the <code>f</code> function are computed. The choices are:</p><ul><li><p><code>ReverseDiffVJP(compile::Bool)</code>: Usually the fastest when scalarized operations exist in the <code>f</code> function (like in scientific machine learning applications like Universal Differential Equations) and the boolean <code>compile</code> (i.e. <code>ReverseDiffVJP(true)</code>) is the absolute fastest but requires that the <code>f</code> function of the ODE/DAE/SDE/DDE has no branching. Does not support GPUs. </p></li><li><p><code>TrackerVJP</code>: Not as efficient as <code>ReverseDiffVJP</code>, but supports GPUs. </p></li><li><p><code>ZygoteVJP</code>: Tends to be the fastest VJP method if the ODE/DAE/SDE/DDE is written with mostly vectorized functions (like neural networks and other layers from <a href="https://fluxml.ai/">Flux.jl</a>). Bear in mind that Zygote does not allow mutation, making the solve more memory expensive and therefore slow.</p></li><li><p><code>nothing</code>: Default choice given characteristics of the types in your model.</p></li><li><p><code>true</code>: Forward-mode AD Jacobian-vector products. Should only be used on sufficiently small equations</p></li><li><p><code>false</code>: Numerical Jacobian-vector products. Should only be used if the <code>f</code> function is not differentiable (i.e. is a Fortran code).</p></li></ul><p>As other vector-Jacobian product systems become available in Julia they will be added to this system so that no user code changes are required to interface with these methodologies. </p><h2 id="Manual-VJPs"><a class="docs-heading-anchor" href="#Manual-VJPs">Manual VJPs</a><a id="Manual-VJPs-1"></a><a class="docs-heading-anchor-permalink" href="#Manual-VJPs" title="Permalink"></a></h2><p>Note that when defining your differential equation the vjp can be manually overwritten by providing a <code>vjp(u,p,t)</code> that returns a tuple <code>f(u,p,t),v-&gt;J*v</code> in the form of <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/">ChainRules.jl</a>. When this is done, the choice of <code>ZygoteVJP</code> will utilize your VJP function during the internal steps of the adjoint. This is useful for models where automatic differentiation may have trouble producing optimal code. This can be paired with <a href="https://github.com/SciML/ModelingToolkit.jl">ModelingToolkit.jl</a> for producing hyper-optimized, sparse, and parallel VJP functions utilizing the automated symbolic conversions.</p><h2 id="Optimize-then-Discretize"><a class="docs-heading-anchor" href="#Optimize-then-Discretize">Optimize-then-Discretize</a><a id="Optimize-then-Discretize-1"></a><a class="docs-heading-anchor-permalink" href="#Optimize-then-Discretize" title="Permalink"></a></h2><p><a href="https://arxiv.org/abs/1806.07366">The original neural ODE paper</a> popularized optimize-then-discretize with O(1) adjoints via backsolve. This is the methodology <code>BacksolveAdjoint</code> When training non-stiff neural ODEs, <code>BacksolveAdjoint</code> with <code>ZygoteVJP</code> is generally the fastest method. Additionally, this method does not require storing the values of any intermediate points and is thus the most memory efficient. However, <code>BacksolveAdjoint</code> is prone to instabilities whenever the Lipschitz constant is sufficiently large, like in stiff equations, PDE discretizations, and many other contexts, so it is not used by default. When training a neural ODE for machine learning applications, the user should try <code>BacksolveAdjoint</code> and see if it is sufficiently accurate on their problem.</p><p>Note that DiffEqFlux&#39;s implementation of <code>BacksolveAdjoint</code> includes an extra feature <code>BacksolveAdjoint(checkpointing=true)</code> which mixes checkpointing with <code>BacksolveAdjoint</code>. What this method does is that, at <code>saveat</code> points, values from the forward pass are saved. Since the reverse solve should numerically be the same as the forward pass, issues with divergence of the reverse pass are mitigated by restarting the reverse pass at the <code>saveat</code> value from the forward pass. This reduces the divergence and can lead to better gradients at the cost of higher memory usage due to having to save some values of the forward pass. This can stabilize the adjoint in some applications, but for highly stiff applications the divergence can be too fast for this to work in practice.</p><p>To avoid the issues of backwards solving the ODE, <code>InterpolatingAdjoint</code> and <code>QuadratureAdjoint</code> utilize information from the forward pass. By default these methods utilize the <a href="https://diffeq.sciml.ai/latest/basics/solution/#Interpolations-1">continuous solution</a> provided by DifferentialEquations.jl in the calculations of the adjoint pass. <code>QuadratureAdjoint</code> uses this to build a continuous function for the solution of adjoint equation and then performs an adaptive quadrature via <a href="https://github.com/SciML/Quadrature.jl">Quadrature.jl</a>, while <code>InterpolatingAdjoint</code> appends the integrand to the ODE so it&#39;s computed simultaneously to the Lagrange multiplier. When memory is not an issue, we find that the <code>QuadratureAdjoint</code> approach tends to be the most efficient as it has a significantly smaller adjoint differential equation and the quadrature converges very fast, but this form requires holding the full continuous solution of the adjoint which can be a significant burden for large parameter problems. The <code>InterpolatingAdjoint</code> is thus a compromise between memory efficiency and compute efficiency, and is in the same spirit as <a href="https://computing.llnl.gov/projects/sundials">CVODES</a>.</p><p>However, if the memory cost of the <code>InterpolatingAdjoint</code> is too high, checkpointing can be used via <code>InterpolatingAdjoint(checkpointing=true)</code>. When this is used, the checkpoints default to <code>sol.t</code> of the forward pass (i.e. the saved timepoints usually set by <code>saveat</code>). Then in the adjoint, intervals of <code>sol.t[i-1]</code> to <code>sol.t[i]</code> are re-solved in order to obtain a short interpolation which can be utilized in the adjoints. This at most results in two full solves of the forward pass, but dramatically reduces the computational cost while being a low-memory format. This is the preferred method for highly stiff equations when memory is an issue, i.e. stiff PDEs or large neural DAEs.</p><p>For forward-mode, the <code>ForwardSensitivty</code> is the version that performs the optimize-then-discretize approach. In this case, <code>autojacvec</code> corresponds to the method for computing <code>J*v</code> within the forward sensitivity equations, which is either <code>true</code> or <code>false</code> for whether to use Jacobian-free forward-mode AD (via ForwardDiff.jl) or Jacobian-free numerical differentiation.</p><h2 id="Discretize-then-Optimize"><a class="docs-heading-anchor" href="#Discretize-then-Optimize">Discretize-then-Optimize</a><a id="Discretize-then-Optimize-1"></a><a class="docs-heading-anchor-permalink" href="#Discretize-then-Optimize" title="Permalink"></a></h2><p>In this approach the discretization is done first and then optimization is done on the discretized system. While traditionally this can be done discrete sensitivity analysis, this is can be equivalently done by automatic differentiation on the solver itself. <code>ReverseDiffAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a>, <code>ZygoteAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>, and <code>TrackerAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a>. In addition, <code>ForwardDiffSensitivty</code> performs forward-mode automatic differentiation on the solver via <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a>.</p><p>We note that many studies have suggested that <a href="https://arxiv.org/abs/2005.13420">this approach produces more accurate gradients than the optimize-than-discretize approach</a></p><h1 id="Special-Notes-on-Equation-Types"><a class="docs-heading-anchor" href="#Special-Notes-on-Equation-Types">Special Notes on Equation Types</a><a id="Special-Notes-on-Equation-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Special-Notes-on-Equation-Types" title="Permalink"></a></h1><p>While all of the choices are compatible with ordinary differential equations, specific notices apply to other forms:</p><h2 id="Differential-Algebraic-Equations"><a class="docs-heading-anchor" href="#Differential-Algebraic-Equations">Differential-Algebraic Equations</a><a id="Differential-Algebraic-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-Algebraic-Equations" title="Permalink"></a></h2><p>We note that while all 3 are compatible with index-1 DAEs via the <a href="https://arxiv.org/abs/2001.04385">derivation in the universal differential equations paper</a> (note the reinitialization), we do not recommend <code>BacksolveAdjoint</code> one DAEs because the stiffness inherent in these problems tends to cause major difficulties with the accuracy of the backwards solution due to reinitialization of the algebraic variables.</p><h2 id="Stochastic-Differential-Equations"><a class="docs-heading-anchor" href="#Stochastic-Differential-Equations">Stochastic Differential Equations</a><a id="Stochastic-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Stochastic-Differential-Equations" title="Permalink"></a></h2><p>We note that all of the adjoints except <code>QuadratureAdjoint</code> are applicable to stochastic differential equations.</p><h2 id="Delay-Differential-Equations"><a class="docs-heading-anchor" href="#Delay-Differential-Equations">Delay Differential Equations</a><a id="Delay-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Delay-Differential-Equations" title="Permalink"></a></h2><p>We note that only the discretize-then-optimize methods are applicable to delay differential equations. Constant lag and variable lag delay differential equation parameters can be estimated, but the lag times themselves are unable to be estimated through these automatic differentiation techniques.</p><h1 id="Controlling-Automatic-Differentiation"><a class="docs-heading-anchor" href="#Controlling-Automatic-Differentiation">Controlling Automatic Differentiation</a><a id="Controlling-Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Controlling-Automatic-Differentiation" title="Permalink"></a></h1><p>One of the key features of DiffEqFlux.jl is the fact that it has many modes of differentiation which are available, allowing neural differential equations and universal differential equations to be fit in the manner that is most appropriate.</p><p>To use the automatic differentiation overloads, the differential equation just needs to be solved with <code>solve</code>. Thus, for example,</p><pre><code class="language-julia hljs">using DiffEqSensitivity, OrdinaryDiffEq, Zygote

function fiip(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end
p = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]
prob = ODEProblem(fiip,u0,(0.0,10.0),p)
sol = solve(prob,Tsit5())
loss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))
du0,dp = Zygote.gradient(loss,u0,p)</code></pre><p>will compute the gradient of the loss function &quot;sum of the values of the solution to the ODE at timepoints dt=0.1&quot; using an adjoint method, where <code>du0</code> is the derivative of the loss function with respect to the initial condition and <code>dp</code> is the derivative of the loss function with respect to the parameters.</p><h2 id="Choosing-a-Differentiation-Method"><a class="docs-heading-anchor" href="#Choosing-a-Differentiation-Method">Choosing a Differentiation Method</a><a id="Choosing-a-Differentiation-Method-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-a-Differentiation-Method" title="Permalink"></a></h2><p>The choice of the method for calculating the gradient is made by passing the keyword argument <code>sensealg</code> to <code>solve</code>. The default choice is dependent on the type of differential equation and the choice of neural network architecture.</p><p>The full listing of differentiation methods is described in the <a href="https://diffeq.sciml.ai/latest/analysis/sensitivity/#Sensitivity-Algorithms-1">DifferentialEquations.jl documentation</a>. That page also has guidelines on how to make the right choice.</p><h3 id="Applicability-of-Backsolve-and-Caution"><a class="docs-heading-anchor" href="#Applicability-of-Backsolve-and-Caution">Applicability of Backsolve and Caution</a><a id="Applicability-of-Backsolve-and-Caution-1"></a><a class="docs-heading-anchor-permalink" href="#Applicability-of-Backsolve-and-Caution" title="Permalink"></a></h3><p>When <code>BacksolveAdjoint</code> is applicable it is a fast method and requires the least memory. However, one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers. An example of such an equation is the Lorenz equation. Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator, then the backwards solution diverges from the forward solution. As a quick demonstration:</p><pre><code class="language-julia hljs">using Sundials
function lorenz(du,u,p,t)
 du[1] = 10.0*(u[2]-u[1])
 du[2] = u[1]*(28.0-u[3]) - u[2]
 du[3] = u[1]*u[2] - (8/3)*u[3]
end
u0 = [1.0;0.0;0.0]
tspan = (0.0,100.0)
prob = ODEProblem(lorenz,u0,tspan)
sol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)
prob2 = ODEProblem(lorenz,sol[end],(100.0,0.0))
sol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)
@show sol[end]-u0 #[-3.22091, -1.49394, 21.3435]</code></pre><p>Thus one should check the stability of the backsolve on their type of problem before enabling this method. Additionally, using checkpointing with backsolve can be a low memory way to stabilize it.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../optimal_control/SDE_control/">« Controlling Stochastic Differential Equations</a><a class="docs-footer-nextpage" href="../nonlinear_solve_sensitivities/">- »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.18 on <span class="colophon-date" title="Sunday 29 May 2022 18:54">Sunday 29 May 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
