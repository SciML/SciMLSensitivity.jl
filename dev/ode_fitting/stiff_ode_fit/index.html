<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parameter Estimation on Highly Stiff Systems · SciMLSensitivity.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://sensitivity.sciml.ai/stable/ode_fitting/stiff_ode_fit/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SciMLSensitivity.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><span class="tocitem">Tutorials</span><ul><li><input class="collapse-toggle" id="menuitem-2-1" type="checkbox"/><label class="tocitem" for="menuitem-2-1"><span class="docs-label">Differentiating Ordinary Differential Equations (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../ad_examples/differentiating_ode/">Differentiating an ODE Solution with Automatic Differentiation</a></li><li><a class="tocitem" href="../../ad_examples/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../ad_examples/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../ad_examples/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-2" type="checkbox" checked/><label class="tocitem" for="menuitem-2-2"><span class="docs-label">Fitting Ordinary Differential Equation (ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../optimization_ode/">Optimization of Ordinary Differential Equations</a></li><li class="is-active"><a class="tocitem" href>Parameter Estimation on Highly Stiff Systems</a><ul class="internal"><li><a class="tocitem" href="#Copy-Pasteable-Code"><span>Copy-Pasteable Code</span></a></li><li><a class="tocitem" href="#Explanation"><span>Explanation</span></a></li></ul></li><li><a class="tocitem" href="../exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-3" type="checkbox"/><label class="tocitem" for="menuitem-2-3"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Neural Ordinary Differential Equation (Neural ODE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../../neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../../neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-5" type="checkbox"/><label class="tocitem" for="menuitem-2-5"><span class="docs-label">Stochastic Differential Equation (SDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../sde_fitting/optimization_sde/">Optimization of Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-6" type="checkbox"/><label class="tocitem" for="menuitem-2-6"><span class="docs-label">Delay Differential Equation (DDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dde_fitting/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Differential-Algebraic Equation (DAE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../dae_fitting/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox"/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Partial Differential Equation (PDE) Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../pde_fitting/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-9" type="checkbox"/><label class="tocitem" for="menuitem-2-9"><span class="docs-label">Hybrid and Jump Equation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../hybrid_jump_fitting/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../hybrid_jump_fitting/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-10" type="checkbox"/><label class="tocitem" for="menuitem-2-10"><span class="docs-label">Bayesian Estimation Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-11" type="checkbox"/><label class="tocitem" for="menuitem-2-11"><span class="docs-label">Optimal and Model Predictive Control Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li><li><a class="tocitem" href="../../optimal_control/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li><a class="tocitem" href="../../manual/differential_equation_sensitivities/">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../manual/nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../../manual/direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../../manual/direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Fitting Ordinary Differential Equation (ODE) Tutorials</a></li><li class="is-active"><a href>Parameter Estimation on Highly Stiff Systems</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Parameter Estimation on Highly Stiff Systems</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/SciMLSensitivity.jl/blob/master/docs/src/ode_fitting/stiff_ode_fit.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Parameter-Estimation-on-Highly-Stiff-Systems"><a class="docs-heading-anchor" href="#Parameter-Estimation-on-Highly-Stiff-Systems">Parameter Estimation on Highly Stiff Systems</a><a id="Parameter-Estimation-on-Highly-Stiff-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Estimation-on-Highly-Stiff-Systems" title="Permalink"></a></h1><p>This tutorial goes into training a model on stiff chemical reaction system data.</p><h2 id="Copy-Pasteable-Code"><a class="docs-heading-anchor" href="#Copy-Pasteable-Code">Copy-Pasteable Code</a><a id="Copy-Pasteable-Code-1"></a><a class="docs-heading-anchor-permalink" href="#Copy-Pasteable-Code" title="Permalink"></a></h2><p>Before getting to the explanation, here&#39;s some code to start with. We will follow a full explanation of the definition and training process:</p><pre><code class="language-julia hljs">using DifferentialEquations, DiffEqFlux, Optimization, OptimizationOptimJL, LinearAlgebra
using ForwardDiff
using DiffEqBase: UJacobianWrapper
using Plots
function rober(du,u,p,t)
    y₁,y₂,y₃ = u
    k₁,k₂,k₃ = p
    du[1] = -k₁*y₁+k₃*y₂*y₃
    du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃
    du[3] =  k₂*y₂^2
    nothing
end

p = [0.04,3e7,1e4]
u0 = [1.0,0.0,0.0]
prob = ODEProblem(rober,u0,(0.0,1e5),p)
sol = solve(prob,Rosenbrock23())
ts = sol.t
Js = map(u-&gt;I + 0.1*ForwardDiff.jacobian(UJacobianWrapper(rober, 0.0, p), u), sol.u)

function predict_adjoint(p)
    p = exp.(p)
    _prob = remake(prob,p=p)
    Array(solve(_prob,Rosenbrock23(autodiff=false),saveat=ts,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))
end

function loss_adjoint(p)
    prediction = predict_adjoint(p)
    prediction = [prediction[:, i] for i in axes(prediction, 2)]
    diff = map((J,u,data) -&gt; J * (abs2.(u .- data)) , Js, prediction, sol.u)
    loss = sum(abs, sum(diff)) |&gt; sqrt
    loss, prediction
end

callback = function (p,l,pred) #callback function to observe training
    println(&quot;Loss: $l&quot;)
    println(&quot;Parameters: $(exp.(p))&quot;)
    # using `remake` to re-create our `prob` with current parameters `p`
    plot(solve(remake(prob, p=exp.(p)), Rosenbrock23())) |&gt; display
    return false # Tell it to not halt the optimization. If return true, then optimization stops
end

initp = ones(3)
# Display the ODE with the initial parameter values.
callback(initp,loss_adjoint(initp)...)

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x,p)-&gt;loss_adjoint(x), adtype)
optprob = Optimization.OptimizationProblem(optf, initp)

res = Optimization.solve(optprob, ADAM(0.01), callback = callback, maxiters = 300)

optprob2 = Optimization.OptimizationProblem(optf, res.u)

res2 = Optimization.solve(optprob2, BFGS(), callback = callback, maxiters = 30, allow_f_increases=true)
println(&quot;Ground truth: $(p)\nFinal parameters: $(round.(exp.(res2.u), sigdigits=5))\nError: $(round(norm(exp.(res2.u) - p) ./ norm(p) .* 100, sigdigits=3))%&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Loss: 20.01949298336545
Parameters: [2.718281828459045, 2.718281828459045, 2.718281828459045]
Loss: 20.01949298336545
Parameters: [2.718281828459045, 2.718281828459045, 2.718281828459045]
Loss: 19.90315002541611
Parameters: [2.6912344724089694, 2.745601014972752, 2.7456010147114345]
Loss: 19.787180914007898
Parameters: [2.6644581879208498, 2.7731909218495896, 2.7731985845921403]
Loss: 19.671593793932505
Parameters: [2.637950413885148, 2.8010516034866, 2.801080608817312]
Loss: 19.556398258246098
Parameters: [2.6117100893283522, 2.829182971626174, 2.8292522164216063]
Loss: 19.441598147135497
Parameters: [2.5857346807591366, 2.8575848904621224, 2.8577197088293365]
Loss: 19.32720235897085
Parameters: [2.5600228826933593, 2.8862571440187423, 2.8864896626851797]
Loss: 19.213151304431104
Parameters: [2.534573034531476, 2.9151993920102974, 2.91556621126119]
Loss: 19.09959647861446
Parameters: [2.509383566943848, 2.94441120134869, 2.9449549911118464]
Loss: 18.986465595909447
Parameters: [2.4844523677392307, 2.973892129012557, 2.9746624682998974]
Loss: 18.873766421069362
Parameters: [2.4597772423749587, 3.0036416236021166, 3.004694803914675]
Loss: 18.761505510715907
Parameters: [2.435356513101248, 3.0336590942649226, 3.035057858837295]
Loss: 18.64968162230453
Parameters: [2.411189000169689, 3.0639438351714516, 3.065757944610309]
Loss: 18.538188793801286
Parameters: [2.3872724187522607, 3.094495177710607, 3.096803365233681]
Loss: 18.42707018985014
Parameters: [2.363605024667837, 3.125312258864148, 3.1281971232858865]
Loss: 18.316510633393705
Parameters: [2.3401855536482863, 3.1563942101149767, 3.159945419755074]
Loss: 18.20651589669402
Parameters: [2.317011663832731, 3.1877401322261516, 3.192054308999206]
Loss: 18.097324581730604
Parameters: [2.294081322439154, 3.2193491187384664, 3.224530899124114]
Loss: 17.988327094130643
Parameters: [2.271392089316687, 3.251220296818449, 3.2573833385583817]
Loss: 17.879833010765154
Parameters: [2.2489428526139963, 3.283352626496544, 3.290616830885663]
Loss: 17.771841868105366
Parameters: [2.226732044211292, 3.3157450322186857, 3.324235867835148]
Loss: 17.664364316005038
Parameters: [2.204757021021563, 3.348396503212521, 3.3582484727781337]
Loss: 17.55740381688823
Parameters: [2.183017184456474, 3.381305883126803, 3.3926588705960947]
Loss: 17.450961553168284
Parameters: [2.161510101414458, 3.4144720883610256, 3.4274736608978658]
Loss: 17.345041985685796
Parameters: [2.1402337932050877, 3.4478940101468942, 3.4626992669947994]
Loss: 17.239650480079945
Parameters: [2.119186186123539, 3.4815705158702412, 3.4983426743447326]
Loss: 17.13479214652886
Parameters: [2.0983651976828037, 3.515500482458361, 3.534410872143311]
Loss: 17.030473087398825
Parameters: [2.077769260736369, 3.5496826541911304, 3.5709076968599702]
Loss: 16.926694933496854
Parameters: [2.057395926286257, 3.584115885640534, 3.6078401410807803]
Loss: 16.823459335498068
Parameters: [2.0372433905934937, 3.6187990524127964, 3.645216119031926]
Loss: 16.720768740285198
Parameters: [2.0173098792539683, 3.653730896935483, 3.6830403607010043]
Loss: 16.618553848023595
Parameters: [1.997594039715136, 3.688910186586728, 3.7213186708826917]
Loss: 16.516822132339087
Parameters: [1.9780929076892995, 3.7243359052923775, 3.7600612537550266]
Loss: 16.415645448873686
Parameters: [1.9588040026364775, 3.7600068329112286, 3.7992748550909923]
Loss: 16.31499218045339
Parameters: [1.9397252393676065, 3.7959217244567993, 3.838964929763697]
Loss: 16.214868810224814
Parameters: [1.9208554756471259, 3.832079194851886, 3.879134669446242]
Loss: 16.115276268279846
Parameters: [1.9021924310588847, 3.868478069989949, 3.9197919033624746]
Loss: 16.016208006818708
Parameters: [1.8837339646594549, 3.9051170956797785, 3.960942138471699]
Loss: 15.917626317707404
Parameters: [1.8654788333260817, 3.9419950132783117, 4.002592046228786]
Loss: 15.819546748431582
Parameters: [1.847424216612172, 3.9791106025243566, 4.044749565713798]
Loss: 15.721950673920368
Parameters: [1.8295684884431047, 4.016462563485045, 4.087420970413996]
Loss: 15.624827603801462
Parameters: [1.8119088977062687, 4.054049643935541, 4.130613411752461]
Loss: 15.528868539670103
Parameters: [1.7944437610074107, 4.091870520207014, 4.174333162802234]
Loss: 15.432973723429257
Parameters: [1.7771712070418004, 4.129924144348721, 4.218589500253516]
Loss: 15.337593111574513
Parameters: [1.760089210957356, 4.168209223369143, 4.263389107118475]
Loss: 15.242734501209426
Parameters: [1.7431947536401022, 4.206724571472351, 4.308740905882061]
Loss: 15.148439159759128
Parameters: [1.7264865908836968, 4.2454688278512975, 4.354651100485178]
Loss: 15.054691121183142
Parameters: [1.7099628269998635, 4.284440510113874, 4.401122476874762]
Loss: 14.961494776524706
Parameters: [1.6936215792383809, 4.323638249206662, 4.448161921714319]
Loss: 14.86888346306763
Parameters: [1.6774608275945642, 4.363060729839257, 4.495776515259491]
Loss: 14.776828547944008
Parameters: [1.6614782621055701, 4.402706646426681, 4.543974097899611]
Loss: 14.685328702162709
Parameters: [1.6456722068175447, 4.442574694136281, 4.592761890947306]
Loss: 14.594388910882195
Parameters: [1.6300403133808643, 4.482663552281425, 4.642148268920989]
Loss: 14.504005454701153
Parameters: [1.6145808227664962, 4.522971850788752, 4.692139951452451]
Loss: 14.414183014715592
Parameters: [1.5992923986242438, 4.563498206870182, 4.742744047418106]
Loss: 14.324893092778495
Parameters: [1.584173069549015, 4.604241275968151, 4.793968064396652]
Loss: 14.23609952386867
Parameters: [1.5692210846608408, 4.6451996472301635, 4.845819076538365]
Loss: 14.147830119149507
Parameters: [1.554434495191737, 4.68637189394648, 4.898304412009419]
Loss: 14.06008081429482
Parameters: [1.539811237448421, 4.727756692756646, 4.951432342048553]
Loss: 13.972850483402537
Parameters: [1.5253495845296434, 4.769352615703186, 5.005210467871246]
Loss: 13.886139042771164
Parameters: [1.5110475293930228, 4.811158250339726, 5.059646791053684]
Loss: 13.799920168911207
Parameters: [1.4969031194720634, 4.853172145069958, 5.114749161504264]
Loss: 13.714254222484227
Parameters: [1.4829145849071583, 4.895392672045474, 5.1705251766227995]
Loss: 13.629102048630104
Parameters: [1.4690798976361263, 4.937818373652024, 5.226982250943643]
Loss: 13.54446234624756
Parameters: [1.4553968157589783, 4.9804479654162765, 5.28413286649404]
Loss: 13.460334199340314
Parameters: [1.4418642855740855, 5.023279796797639, 5.341983095770457]
Loss: 13.376718739777
Parameters: [1.4284806676394628, 5.06631227573108, 5.400540922860218]
Loss: 13.293599775878219
Parameters: [1.4152438619158576, 5.109543783691448, 5.459814548658963]
Loss: 13.210953839345763
Parameters: [1.4021522528201418, 5.1529727921180895, 5.5198137567608825]
Loss: 13.12888335394599
Parameters: [1.3892042837937724, 5.196597453883104, 5.580543380869991]
Loss: 13.047216367352942
Parameters: [1.376397945665558, 5.24041635797514, 5.642015398653236]
Loss: 12.966025451679853
Parameters: [1.3637318222519386, 5.284427622772592, 5.70423426680232]
Loss: 12.885302419949554
Parameters: [1.3512041451044665, 5.328629582866198, 5.767208309495938]
Loss: 12.80503421908898
Parameters: [1.3388138213099565, 5.3730202751310765, 5.830945654057362]
Loss: 12.725291181764566
Parameters: [1.3265589957021895, 5.417598024897108, 5.895455028081921]
Loss: 12.645954365517381
Parameters: [1.3144377261830302, 5.46236144926554, 5.960749973065013]
Loss: 12.567053883782265
Parameters: [1.3024488114597512, 5.507308577211413, 6.026836629231574]
Loss: 12.488584431785972
Parameters: [1.2905897320427486, 5.552437818211354, 6.093729313634082]
Loss: 12.410585945070412
Parameters: [1.278860193351468, 5.5977468757466475, 6.161429067089324]
Loss: 12.333107277018774
Parameters: [1.2672583815123835, 5.64323400028258, 6.229949354892322]
Loss: 12.256681744205991
Parameters: [1.2557832181210955, 5.688897299934093, 6.299297278623152]
Loss: 12.179796139259883
Parameters: [1.24443191269594, 5.734735476963659, 6.369485045177065]
Loss: 12.10393507392717
Parameters: [1.2332039929129919, 5.780746177670829, 6.440517729642942]
Loss: 12.028564424432794
Parameters: [1.2220973268411615, 5.826927616856703, 6.512405739399504]
Loss: 11.953659755399016
Parameters: [1.2111104688780636, 5.87327853279422, 6.585164517737372]
Loss: 11.879199712763429
Parameters: [1.2002424032664998, 5.919796677066465, 6.6588002420272225]
Loss: 11.805089901683745
Parameters: [1.1894909855831293, 5.966480623260681, 6.733327556058351]
Loss: 11.731499299991391
Parameters: [1.178855649307981, 6.013328253863915, 6.8087515760323365]
Loss: 11.658355598412856
Parameters: [1.1683354961679842, 6.060337238022144, 6.8850772555284925]
Loss: 11.5856557534458
Parameters: [1.1579291306040334, 6.107505871866692, 6.962316440078032]
Loss: 11.513388827674428
Parameters: [1.1476344781627197, 6.1548326685259, 7.0404843927654355]
Loss: 11.441555852806193
Parameters: [1.1374503984799242, 6.202315485136508, 7.1195887943863765]
Loss: 11.36994370127623
Parameters: [1.1273755091620137, 6.2499520841976, 7.199638563531739]
Loss: 11.299031256919932
Parameters: [1.117408591605729, 6.297740481120987, 7.280643912300166]
Loss: 11.228490835077906
Parameters: [1.1075485330754855, 6.345678895478967, 7.362616860489423]
Loss: 11.158266030504251
Parameters: [1.0977935759024287, 6.39376551045612, 7.445572236313866]
Loss: 11.088498918706405
Parameters: [1.088143145461951, 6.44199790466993, 7.529514026400205]
Loss: 11.019185868832254
Parameters: [1.078595142665775, 6.490374420574281, 7.614459846664797]
Loss: 10.950401284577225
Parameters: [1.069148616513989, 6.538892766614594, 7.700417000605256]
Loss: 10.881879750047423
Parameters: [1.059802083330188, 6.587551213525778, 7.787402286329253]
Loss: 10.81378816811526
Parameters: [1.050554611291532, 6.636347461852168, 7.875421925525916]
Loss: 10.746129611445033
Parameters: [1.0414050693888897, 6.685279389294759, 7.964485033638806]
Loss: 10.678894065589287
Parameters: [1.0323518902747613, 6.734344836239764, 8.054604771714772]
Loss: 10.61205036946669
Parameters: [1.02339419111994, 6.783541778027812, 8.145792936664765]
Loss: 10.54561668164447
Parameters: [1.0145311315837737, 6.8328676290005115, 8.238055601747323]
Loss: 10.479578232758454
Parameters: [1.0057618845676888, 6.882320165149526, 8.331402018479128]
Loss: 10.413856598257643
Parameters: [0.9970854077187199, 6.931897429438505, 8.425846031006207]
Loss: 10.348654558728596
Parameters: [0.9885002268867278, 6.981597258551857, 8.521400651919459]
Loss: 10.283763443838673
Parameters: [0.9800052412809783, 7.031417777091219, 8.618076938178579]
Loss: 10.219260497417038
Parameters: [0.9715999160520732, 7.081356375112083, 8.715880132497743]
Loss: 10.155147901290619
Parameters: [0.9632824506820811, 7.131411285487322, 8.814826092934586]
Loss: 10.09143237095695
Parameters: [0.9550522491292539, 7.1815804626927635, 8.914927327721731]
Loss: 10.028107909202125
Parameters: [0.9469078003652598, 7.231861786942729, 9.016194758023133]
Loss: 9.965166140120239
Parameters: [0.9388488756604038, 7.282252810730588, 9.118635884889477]
Loss: 9.902598492238596
Parameters: [0.9308744423340634, 7.332751503924967, 9.222260639229562]
Loss: 9.840397827982088
Parameters: [0.9229830757765882, 7.383355988257737, 9.327086998761343]
Loss: 9.778499097551435
Parameters: [0.9151740984918145, 7.43406366311597, 9.433122041003474]
Loss: 9.71708630071971
Parameters: [0.9074465085400317, 7.48487266397351, 9.54038137255151]
Loss: 9.656018519644766
Parameters: [0.899799356138786, 7.535780897183519, 9.648872958663185]
Loss: 9.595295646787338
Parameters: [0.8922309867960968, 7.586786789499132, 9.758617977010521]
Loss: 9.53494091820902
Parameters: [0.8847412832557898, 7.637887552458458, 9.869618323185962]
Loss: 9.47493063451952
Parameters: [0.8773295022980845, 7.689081269588721, 9.981884957241995]
Loss: 9.415268243371028
Parameters: [0.869994039047573, 7.7403665135768405, 10.095441021514226]
Loss: 9.355958276441488
Parameters: [0.8627337337783875, 7.791741063339157, 10.210298655958724]
Loss: 9.29691279798201
Parameters: [0.8555479637193505, 7.843202684722609, 10.32646561858036]
Loss: 9.238344814078127
Parameters: [0.8484367407314983, 7.894749065200872, 10.443950517136349]
Loss: 9.180119754183902
Parameters: [0.8413989744283445, 7.946378146976613, 10.562764264382068]
Loss: 9.122227048802976
Parameters: [0.834433861330808, 7.998088004382965, 10.68291830240925]
Loss: 9.0646478605621
Parameters: [0.8275407960608999, 8.04987634144269, 10.80442229340942]
Loss: 9.007388500446549
Parameters: [0.8207183525166355, 8.10174184733788, 10.927300712618823]
Loss: 8.950473960867798
Parameters: [0.813965405314136, 8.1536828558345, 11.051567377412907]
Loss: 8.894096925763382
Parameters: [0.8072811027611659, 8.205696720759427, 11.177227697416429]
Loss: 8.837899347159047
Parameters: [0.8006650806755007, 8.25778178185232, 11.304293393421581]
Loss: 8.782027914363788
Parameters: [0.7941163129081552, 8.309935752770203, 11.43277830909351]
Loss: 8.726450106746213
Parameters: [0.7876343424169959, 8.362156750093273, 11.562690365343915]
Loss: 8.671190082109431
Parameters: [0.7812181923296815, 8.41444289189803, 11.694048153710156]
Loss: 8.616236762829722
Parameters: [0.7748671128819621, 8.466791919121086, 11.826860426484245]
Loss: 8.561666470477459
Parameters: [0.7685806788189657, 8.519202256493909, 11.961138727367683]
Loss: 8.5073744232146
Parameters: [0.7623581191768787, 8.57167214468149, 12.096898702511398]
Loss: 8.453377264163501
Parameters: [0.7561983931581724, 8.624199834741326, 12.23415789264824]
Loss: 8.399682959515651
Parameters: [0.750100743066681, 8.67678312983083, 12.372927539403829]
Loss: 8.346532615646073
Parameters: [0.7440646647157282, 8.729419994675167, 12.513218081217284]
Loss: 8.293551986493924
Parameters: [0.7380893568596824, 8.782109435275576, 12.655050077794618]
Loss: 8.240876124414912
Parameters: [0.7321739393214174, 8.83484938651733, 12.79843292145028]
Loss: 8.188499083971678
Parameters: [0.7263181149577816, 8.88763813099035, 12.943375258997682]
Loss: 8.136404027205282
Parameters: [0.7205210832789877, 8.940473432644978, 13.08988886155118]
Loss: 8.084692896302162
Parameters: [0.7147820941473032, 8.993354248850137, 13.237993981182024]
Loss: 8.033281935407162
Parameters: [0.7091003773207205, 9.046279000898746, 13.387704094581704]
Loss: 7.98214828950881
Parameters: [0.7034754815021528, 9.099245299320053, 13.539018337996664]
Loss: 7.9313001859358465
Parameters: [0.6979066638733185, 9.15225189729395, 13.691957031414038]
Loss: 7.880746036541219
Parameters: [0.6923935403339657, 9.20529686707623, 13.846527422903375]
Loss: 7.830397048988343
Parameters: [0.6869352054668825, 9.258378930528647, 14.00274836909611]
Loss: 7.7804758445358955
Parameters: [0.6815310525677468, 9.311496499345376, 14.160634242818972]
Loss: 7.730843349762052
Parameters: [0.6761804885906245, 9.364647963732782, 14.320195868151657]
Loss: 7.681477952662171
Parameters: [0.6708831091489382, 9.417832427832419, 14.481447951018536]
Loss: 7.6323539471554245
Parameters: [0.6656382135696275, 9.471047997984526, 14.644397917387687]
Loss: 7.583572046847954
Parameters: [0.6604454780737831, 9.52429310431012, 14.809053083906349]
Loss: 7.535064315634589
Parameters: [0.6553042807156979, 9.577566913460299, 14.97543200055887]
Loss: 7.4868391671942005
Parameters: [0.6502138454604182, 9.630867918980767, 15.14354889743424]
Loss: 7.438897836214706
Parameters: [0.6451735652850972, 9.68419466461698, 15.313416704601549]
Loss: 7.391194823150465
Parameters: [0.6401830302436443, 9.737545602712311, 15.485040510881776]
Loss: 7.343834399021511
Parameters: [0.6352416710319916, 9.790919437977905, 15.658433983394575]
Loss: 7.2967475331402225
Parameters: [0.6303489540136135, 9.844314997816195, 15.833609772467273]
Loss: 7.249926654122606
Parameters: [0.625504739436648, 9.897731176587866, 16.010555313839703]
Loss: 7.203372326728988
Parameters: [0.6207080669159306, 9.95116671612208, 16.18931041611404]
Loss: 7.157093548551751
Parameters: [0.6159585724421935, 10.004620462854968, 16.369865926342058]
Loss: 7.111088566955243
Parameters: [0.6112557994763304, 10.058091569059593, 16.552237386304054]
Loss: 7.065488542257262
Parameters: [0.6065993797015424, 10.11157907116754, 16.73643455812717]
Loss: 7.020065595188836
Parameters: [0.6019884279835486, 10.165082324149358, 16.92248427117076]
Loss: 6.974912530368077
Parameters: [0.5974226238842276, 10.21859989680258, 17.110391272085185]
Loss: 6.930025274946444
Parameters: [0.5929013868438539, 10.272131278515586, 17.30017361672844]
Loss: 6.885410561047759
Parameters: [0.5884244676833587, 10.325674844619126, 17.491828474493268]
Loss: 6.841064067337674
Parameters: [0.5839912870229321, 10.379229568274466, 17.685368741357628]
Loss: 6.796978343741817
Parameters: [0.5796014696373348, 10.432795275143514, 17.88081037912941]
Loss: 6.753150635500176
Parameters: [0.5752548009808748, 10.48637063023744, 18.07815132954897]
Loss: 6.709609691819845
Parameters: [0.5709505427875724, 10.53995456631742, 18.277408033764175]
Loss: 6.666413759606902
Parameters: [0.566688220426314, 10.593546478947067, 18.478591773958883]
Loss: 6.623424669668867
Parameters: [0.5624673521279784, 10.647145879659163, 18.681716345828047]
Loss: 6.58070147512545
Parameters: [0.5582875123595037, 10.700752090723059, 18.886790897235706]
Loss: 6.538240358588774
Parameters: [0.5541483089544268, 10.7543646464668, 19.093825569087606]
Loss: 6.496033306107512
Parameters: [0.5500491580157173, 10.807982803251189, 19.30283575801466]
Loss: 6.454064656862749
Parameters: [0.5459897538906965, 10.861605902184241, 19.513822769029726]
Loss: 6.4123581482092975
Parameters: [0.5419696098014977, 10.915233316407662, 19.726798815477775]
Loss: 6.370932213968043
Parameters: [0.5379884068764662, 10.968865191714098, 19.94177689615538]
Loss: 6.329753281738432
Parameters: [0.5340456485004654, 11.02250070614932, 20.158764708005958]
Loss: 6.2888385947538055
Parameters: [0.5301410751109079, 11.076139959041555, 20.37776850899189]
Loss: 6.248179046019718
Parameters: [0.5262742248197314, 11.129782375166167, 20.598797636839944]
Loss: 6.207757521086349
Parameters: [0.522444732074266, 11.1834275184087, 20.821856453284184]
Loss: 6.167533050183406
Parameters: [0.5186520829838344, 11.237075535784676, 21.046964605842636]
Loss: 6.127645899301364
Parameters: [0.5148960389738044, 11.290725956744817, 21.274122315680515]
Loss: 6.088001842430179
Parameters: [0.5111762159817853, 11.344378286923636, 21.503330981903606]
Loss: 6.048605838758357
Parameters: [0.5074922107700569, 11.398032309472647, 21.734596682267064]
Loss: 6.009437770014544
Parameters: [0.5038435643034265, 11.451688461638014, 21.96793575215303]
Loss: 5.970533242707161
Parameters: [0.5002299471026614, 11.50534677711641, 22.203354122324782]
Loss: 5.931875433901593
Parameters: [0.4966511472492104, 11.559006910558356, 22.440846363453044]
Loss: 5.8934628969479075
Parameters: [0.49310674222276124, 11.612669273522421, 22.680425513014143]
Loss: 5.8552957237718815
Parameters: [0.48959636888541824, 11.666334329986348, 22.922102316421828]
Loss: 5.817343460493531
Parameters: [0.48611971372498997, 11.720001744679166, 23.165875798192307]
Loss: 5.779679495597613
Parameters: [0.4826764799353604, 11.773671722807283, 23.411747867926575]
Loss: 5.742262158654489
Parameters: [0.4792662263140424, 11.827344753153282, 23.659732004335734]
Loss: 5.705076369121355
Parameters: [0.4758887399396608, 11.881020711757971, 23.909819459486126]
Loss: 5.668131753354695
Parameters: [0.47254373444273234, 11.934700005601883, 24.162013300030537]
Loss: 5.631438804361988
Parameters: [0.4692308845504193, 11.988383002756915, 24.41631699925622]
Loss: 5.594996552408651
Parameters: [0.46594988944058063, 12.042069906594628, 24.672729852365073]
Loss: 5.5588578412277405
Parameters: [0.46270037621956955, 12.095761403949714, 24.931262013967967]
Loss: 5.522909331743542
Parameters: [0.4594821190113624, 12.149457886205507, 25.191905550056525]
Loss: 5.487207300908225
Parameters: [0.45629478122710915, 12.20316010903206, 25.454664774409164]
Loss: 5.451749009294369
Parameters: [0.45313805498698045, 12.256868689775265, 25.719543478900572]
Loss: 5.416538612095729
Parameters: [0.45001159354966147, 12.31058395735736, 25.986541721167274]
Loss: 5.381579758746305
Parameters: [0.4469151247601215, 12.364306672551114, 26.255658543323623]
Loss: 5.346867619902268
Parameters: [0.4438483838940312, 12.418037281457428, 26.526889377220893]
Loss: 5.312403415255618
Parameters: [0.44081112651046234, 12.47177679797511, 26.80023262522822]
Loss: 5.278169317981674
Parameters: [0.4378029983449218, 12.525526008312694, 27.07569253395206]
Loss: 5.244168673902885
Parameters: [0.4348237003968505, 12.579285941144413, 27.353273878090356]
Loss: 5.2103905547697975
Parameters: [0.43187305043347596, 12.633057034271342, 27.63295780288595]
Loss: 5.177079686472311
Parameters: [0.4289507192463534, 12.686840246120724, 27.91474761816965]
Loss: 5.143875079804636
Parameters: [0.4260563981321567, 12.740637104349139, 28.19864774431489]
Loss: 5.110923159310044
Parameters: [0.42319062652202466, 12.794448336836698, 28.484519951930796]
Loss: 5.078222145073397
Parameters: [0.420353031095915, 12.848275305438259, 28.77237289014726]
Loss: 5.045756503707543
Parameters: [0.4175432500373289, 12.902119146920183, 29.062210485969015]
Loss: 5.013517464770963
Parameters: [0.4147610254031077, 12.955980832234902, 29.354023983150878]
Loss: 4.981523307816336
Parameters: [0.4120060432059303, 13.009861757122552, 29.64781410887861]
Loss: 4.949775223909369
Parameters: [0.4092779928544476, 13.063763117264365, 29.943580390022547]
Loss: 4.918273937569227
Parameters: [0.40657664800860227, 13.117686197814363, 30.241310575277385]
Loss: 4.887019273907724
Parameters: [0.4039017188091557, 13.171632536147097, 30.541002176417205]
Loss: 4.856011198109763
Parameters: [0.4012529582398937, 13.225603439273465, 30.842644123857244]
Loss: 4.825248343693454
Parameters: [0.3986300920906001, 13.27960044112468, 31.14622962521078]
Loss: 4.794729575060461
Parameters: [0.39603286554521866, 13.33362496464118, 31.451748634071443]
Loss: 4.7643781396823695
Parameters: [0.3934609874128672, 13.387678651174717, 31.759198745473803]
Loss: 4.734371440426984
Parameters: [0.390914296979619, 13.441762762089338, 32.06855434182058]
Loss: 4.70460495408233
Parameters: [0.3883925268725169, 13.495879024763747, 32.37980672052467]
Loss: 4.675073256183398
Parameters: [0.3858954422503239, 13.550028998895082, 32.69294106021197]
Loss: 4.64578183134144
Parameters: [0.3834228142843216, 13.604214418124924, 33.00794175493509]
Loss: 4.616727876290827
Parameters: [0.38097441051435715, 13.658437358726305, 33.32479796141847]
Loss: 4.587914382722771
Parameters: [0.37855007954903463, 13.712699312781815, 33.64347761837912]
Loss: 4.559340128080891
Parameters: [0.37614951217482484, 13.767002277713232, 33.96398000093203]
Loss: 4.531005031977008
Parameters: [0.37377249704302906, 13.821348051849656, 34.28628511923674]
Loss: 4.502909994547264
Parameters: [0.3714188858246516, 13.875738555829239, 34.61036326483836]
Loss: 4.475006263148272
Parameters: [0.3690884729941346, 13.930175831894722, 34.93619362679492]
Loss: 4.447403184201636
Parameters: [0.36678103694028996, 13.984661437613493, 35.26375305055202]
Loss: 4.420035550025836
Parameters: [0.36449638263057726, 14.03919743600056, 35.593018195949405]
Loss: 4.392894495107459
Parameters: [0.36223430103631177, 14.093786025134293, 35.92396824336907]
Loss: 4.366000666883839
Parameters: [0.3599946616112207, 14.148429210746551, 36.25656609040971]
Loss: 4.339351771070572
Parameters: [0.3577772051811687, 14.203129054583828, 36.59079705067085]
Loss: 4.312946054564679
Parameters: [0.35558177474493075, 14.257887727107567, 36.92662878833987]
Loss: 4.286857018261829
Parameters: [0.3534081936255531, 14.312707257754457, 37.264029479549855]
Loss: 4.260938286166103
Parameters: [0.3512562799604856, 14.367589977483549, 37.602968430040754]
Loss: 4.23525825342289
Parameters: [0.3491257962624064, 14.422538341327055, 37.94343055986576]
Loss: 4.209823915605326
Parameters: [0.3470166196495974, 14.4775545570695, 38.28537258403173]
Loss: 4.184634408242083
Parameters: [0.344928577556404, 14.53264086769623, 38.62875945633444]
Loss: 4.159689963173335
Parameters: [0.3428615037330546, 14.587799438074935, 38.973553530016325]
Loss: 4.134988905909441
Parameters: [0.3408151986521528, 14.643032587180125, 39.31972443011819]
Loss: 4.110528712715811
Parameters: [0.33878951658605266, 14.69834273481892, 39.66723304752161]
Loss: 4.086352766898192
Parameters: [0.33678429743404803, 14.753732218509613, 40.016038704073935]
Loss: 4.062380292038692
Parameters: [0.3347994376262953, 14.80920347434931, 40.36609095485926]
Loss: 4.038643967460637
Parameters: [0.33283467126988975, 14.86475905118073, 40.71737372235692]
Loss: 4.01514379384021
Parameters: [0.33088986678474364, 14.920401497017963, 41.069841685652065]
Loss: 3.9918837976911536
Parameters: [0.328964868983011, 14.976133144099618, 41.42344973679044]
Loss: 3.9690448930350954
Parameters: [0.327059509663258, 15.03195666906741, 41.7781595279341]
Loss: 3.946319683781317
Parameters: [0.32517366785737545, 15.087875045154066, 42.13392097242835]
Loss: 3.923835103110746
Parameters: [0.32330718687000126, 15.143890835515531, 42.49068926509369]
Loss: 3.9015902907816344
Parameters: [0.3214599142277418, 15.20000647562849, 42.84841581778719]
Loss: 3.879584903066231
Parameters: [0.3196317313565602, 15.25622484623594, 43.20704830068004]
Loss: 3.8578188808305045
Parameters: [0.3178224940712379, 15.312548326864865, 43.56653374992435]
Loss: 3.836291829349607
Parameters: [0.3160320222565486, 15.368979732472726, 43.92683333524297]
Loss: 3.81500342411428
Parameters: [0.31426016964198333, 15.425521757342098, 44.28789408236987]
Loss: 3.7939531311245327
Parameters: [0.3125067991850957, 15.482176973058825, 44.649661500527955]
Loss: 3.7731400664814547
Parameters: [0.31077178861860105, 15.538948283139355, 45.012078208889235]
Loss: 3.7525479940210245
Parameters: [0.3090549492758504, 15.59583852471078, 45.3751056215914]
Loss: 3.7321868971507013
Parameters: [0.3073561888255486, 15.65285028590049, 45.73867200073951]
Loss: 3.7120605207944615
Parameters: [0.3056753538972503, 15.709986146004846, 46.10272450904014]
Loss: 3.6921682285062536
Parameters: [0.3040123136728693, 15.767248859317416, 46.4672038047629]
Loss: 3.672509378618996
Parameters: [0.30236696104527716, 15.824641333427355, 46.83204532125555]
Loss: 3.653018847202332
Parameters: [0.3007391374800708, 15.882166232919579, 47.19719443915558]
Loss: 3.6338455931359905
Parameters: [0.2991280742100031, 15.939826275930967, 47.56284250784571]
Loss: 3.6148937396856455
Parameters: [0.2975337103352433, 15.997624320521183, 47.928910613088206]
Loss: 3.5961635783011023
Parameters: [0.29595597775749116, 16.055563389168544, 48.29532070956147]
Loss: 3.5776542673426954
Parameters: [0.2943948078957452, 16.113646007910834, 48.66198893807252]
Loss: 3.5593549073198014
Parameters: [0.2928501261961101, 16.17187516241797, 49.02883801794625]
Loss: 3.541277120767473
Parameters: [0.29132186558087425, 16.230253509710703, 49.39578207850688]
Loss: 3.5234290492270444
Parameters: [0.2898105232248573, 16.288783978981225, 49.76249291517233]
Loss: 3.5058096284226257
Parameters: [0.28831595240421115, 16.34746957141335, 50.12891126795632]
Loss: 3.488416017713428
Parameters: [0.2868379530430843, 16.406312761739482, 50.494989928828495]
Loss: 3.471248036637714
Parameters: [0.2853763979366004, 16.465316494214857, 50.86066435802281]
Loss: 3.454304059390206
Parameters: [0.283931152609266, 16.524483282828157, 51.22586390044405]
Loss: 3.437582871020583
Parameters: [0.28250206994927235, 16.58381601593563, 51.59052834161875]
Loss: 3.4210565797109416
Parameters: [0.2810890058908676, 16.643317423652256, 51.95459589677385]
Loss: 3.4047798627970614
Parameters: [0.27969177749054785, 16.702990383899717, 52.31801804101997]
Loss: 3.3887211937782578
Parameters: [0.27831026770549677, 16.76283765800013, 52.680722721778785]
Loss: 3.372868712599326
Parameters: [0.27694432779792966, 16.822862054382117, 53.042651096737636]
Loss: 3.357225707890483
Parameters: [0.27559383512526137, 16.88306628759106, 53.403729516274375]
Loss: 3.3417933587283835
Parameters: [0.2742586250063233, 16.943453196232603, 53.763906385168355]
Loss: 3.3265698891141384
Parameters: [0.27293859473440635, 17.004025398741817, 54.12310278384605]
Loss: 3.31156152459182
Parameters: [0.27163360958824817, 17.064785698240094, 54.481253419350764]
Loss: 3.2967631026009996
Parameters: [0.27034351827020775, 17.125736739029794, 54.83830056813468]
Loss: 3.2821736465317053
Parameters: [0.26906818436295854, 17.186881171785625, 55.19417668509968]
Loss: 3.267883387261535
Parameters: [0.267807506071606, 17.248221790284617, 55.54880513346756]
Loss: 3.2537031339007267
Parameters: [0.26656134461649755, 17.309761322696456, 55.9021226946454]
Loss: 3.2397225839159303
Parameters: [0.26532957088344555, 17.37150240713504, 56.254062941474075]
Loss: 3.225939375779284
Parameters: [0.26411204650992176, 17.433447802264794, 56.60456393239953]
Loss: 3.225939375779284
Parameters: [0.26411204650992176, 17.433447802264794, 56.60456393239953]
Loss: 3.225939375779284
Parameters: [0.26411204650992176, 17.433447802264794, 56.60456393239953]
Loss: 2.209401226378694
Parameters: [0.10973466515077766, 36.92691913008635, 89.15233993402771]
Loss: 1.157831913911209
Parameters: [0.09470957941503927, 351.86362754704226, 45.2244075250296]
Loss: 0.7822699445259665
Parameters: [0.05605531896296803, 310.4453136719521, 64.76493297132257]
Loss: 0.48836560332758566
Parameters: [0.046062381046894134, 862.19051898818, 50.391777837944254]
Loss: 0.3460433640966141
Parameters: [0.02084039787612439, 2514.967256937777, 54.95813971036602]
Loss: 0.22188115852547527
Parameters: [0.022518747104163038, 2838.6587025757617, 52.196329759517766]
Loss: 0.2079830777712263
Parameters: [0.021921348366137513, 4451.6501493763935, 60.316913158577826]
Loss: 0.1377847073159872
Parameters: [0.03550944921394394, 47746.95296159734, 385.28173921672345]
Loss: 0.053125217430499846
Parameters: [0.03881536048757893, 54278.225785961, 416.7218513528914]
Loss: 0.04441231016845508
Parameters: [0.04201786197741417, 93802.96600256175, 580.9872964292658]
Loss: 0.028726021878718817
Parameters: [0.038762444645377606, 387191.3856308896, 1115.2360426825173]
Loss: 0.01617365797437167
Parameters: [0.039164728136279435, 539337.3644419437, 1308.0696166864052]
Loss: 0.014967219313284824
Parameters: [0.041530128892343005, 806029.2908588017, 1701.3199831834515]
Loss: 0.010228153954460088
Parameters: [0.038065821415852824, 6.415055797738003e6, 4378.9219097710275]
Loss: 0.003924588683059752
Parameters: [0.039676879653206175, 5.890056147315492e6, 4394.660329496387]
Loss: 0.0030856287403033733
Parameters: [0.0401408685173047, 6.690991620394971e6, 4737.150878294387]
Loss: 0.002041158762251882
Parameters: [0.04022474102808858, 5.369266745105831e7, 13469.521302785448]
Loss: 0.001115232728374516
Parameters: [0.04003648583865376, 7.583657040378626e7, 15911.32074872758]
Loss: 0.000642616979891354
Parameters: [0.040008737339517396, 4.68523576486815e7, 12499.82412372187]
Loss: 0.0006426175594930998
Parameters: [0.04000873737332541, 4.685187834379431e7, 12499.760238380046]
Loss: 0.0006426181921973958
Parameters: [0.04000873739041076, 4.6851636098880395e7, 12499.727950040187]
Loss: 0.0006426186347867049
Parameters: [0.04000873743204068, 4.6851045754540086e7, 12499.649263871786]
Loss: 0.0006426191055517402
Parameters: [0.04002586943920275, 4.590183246898599e7, 12378.139382883222]
Loss: 0.0006426195370774916
Parameters: [0.04002586970469905, 4.590180509597179e7, 12378.135774848632]
Loss: 0.0006171023325572471
Parameters: [0.04007970502377367, 4.573457040805872e7, 12372.183752499217]
Loss: 0.0005672332525415651
Parameters: [0.04003076582254943, 4.5136192447738975e7, 12272.493086090015]
Loss: 0.0005576903424079657
Parameters: [0.04005023038685145, 4.500822404641245e7, 12260.925118345911]
Loss: 0.0005523315934248628
Parameters: [0.040045045640134024, 4.470785523317776e7, 12217.51026880116]
Loss: 0.0005523320311223211
Parameters: [0.040018815669871734, 4.328241987035641e7, 12012.771389453323]
Loss: 0.0005523321942745898
Parameters: [0.04001881443072911, 4.328234290960748e7, 12012.76028853786]
Ground truth: [0.04, 3.0e7, 10000.0]
Final parameters: [0.040019, 4.3282e7, 12013.0]
Error: 44.3%</code></pre><p>Output:</p><pre><code class="nohighlight hljs">Ground truth: [0.04, 3.0e7, 10000.0]
Final parameters: [0.040002, 3.0507e7, 10084.0]
Error: 1.69%</code></pre><h2 id="Explanation"><a class="docs-heading-anchor" href="#Explanation">Explanation</a><a id="Explanation-1"></a><a class="docs-heading-anchor-permalink" href="#Explanation" title="Permalink"></a></h2><p>First, let&#39;s get a time series array from the Robertson&#39;s equation as data.</p><pre><code class="language-julia hljs">using DifferentialEquations, DiffEqFlux, Optimization, OptimizationOptimJL, LinearAlgebra
using ForwardDiff
using DiffEqBase: UJacobianWrapper
using Plots
function rober(du,u,p,t)
    y₁,y₂,y₃ = u
    k₁,k₂,k₃ = p
    du[1] = -k₁*y₁+k₃*y₂*y₃
    du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃
    du[3] =  k₂*y₂^2
    nothing
end

p = [0.04,3e7,1e4]
u0 = [1.0,0.0,0.0]
prob = ODEProblem(rober,u0,(0.0,1e5),p)
sol = solve(prob,Rosenbrock23())
ts = sol.t
Js = map(u-&gt;I + 0.1*ForwardDiff.jacobian(UJacobianWrapper(rober, 0.0, p), u), sol.u)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">61-element Vector{Matrix{Float64}}:
 [0.996 0.0 0.0; 0.004 1.0 0.0; 0.0 0.0 1.0]
 [0.996 3.9181897521319503e-7 0.0012780900152625978; 0.004 -6.668540483394562 -0.0012780900152625978; 0.0 7.668540091575586 1.0]
 [0.996 4.175663804739006e-5 0.005718510461294757; 0.004 -33.31110452440659 -0.005718510461294757; 0.0 34.31106276776854 1.0]
 [0.996 0.00024992454904057104 0.009992106612572492; 0.004 -58.95288959998399 -0.009992106612572492; 0.0 59.952639675434945 1.0]
 [0.996 0.0016037077316934775 0.017833623941038088; 0.004 -106.00334735396024 -0.017833623941038088; 0.0 107.00174364622853 1.0]
 [0.996 0.004682453587410618 0.02403488562731424; 0.004 -143.21399621747284 -0.02403488562731424; 0.0 144.20931376388543 1.0]
 [0.996 0.01288429926109498 0.030390689334989115; 0.004 -181.3570203091958 -0.030390689334989115; 0.0 182.3441360099347 1.0]
 [0.996 0.02531711709494679 0.03388427339038224; 0.004 -202.3309574593884 -0.03388427339038224; 0.0 203.30564034229346 1.0]
 [0.996 0.046868882246842165 0.03583508669306405; 0.004 -214.0573890406311 -0.03583508669306405; 0.0 215.01052015838428 1.0]
 [0.996 0.077296222064754 0.03641240161925743; 0.004 -217.5517059376093 -0.03641240161925743; 0.0 218.47440971554457 1.0]
 ⋮
 [0.996 944.3646828226427 0.0002354632239450543; 0.004 -944.777462166313 -0.0002354632239450543; 0.0 1.412779343670326 1.0]
 [0.996 952.0744466258238 0.0002012149653947109; 0.004 -952.2817364181922 -0.0002012149653947109; 0.0 1.2072897923682655 1.0]
 [0.996 958.7664043966821 0.00017192789116847056; 0.004 -958.7979717436929 -0.00017192789116847056; 0.0 1.0315673470108233 1.0]
 [0.996 964.5628529142934 0.00014688362762022163; 0.004 -964.4441546800149 -0.00014688362762022163; 0.0 0.88130176572133 1.0]
 [0.996 969.5745014322464 0.00012546809864160562; 0.004 -969.327310024096 -0.00012546809864160562; 0.0 0.7528085918496338 1.0]
 [0.996 973.9007597174125 0.00010715608821695005; 0.004 -973.5436962467143 -0.00010715608821695005; 0.0 0.6429365293017003 1.0]
 [0.996 977.6302161336016 9.149845157822576e-5; 0.004 -977.179206843071 -9.149845157822576e-5; 0.0 0.5489907094693546 1.0]
 [0.996 980.8413583945702 7.81109645534633e-5; 0.004 -980.310024181891 -7.81109645534633e-5; 0.0 0.4686657873207798 1.0]
 [0.996 982.1720335649057 7.258919980139007e-5; 0.004 -981.6075687637142 -7.258919980139007e-5; 0.0 0.43553519880834046 1.0]</code></pre><p>Note that we also computed a shifted and scaled Jacobian along with the solution. We will use this matrix to scale the loss later.</p><p>We fit the parameters in log space, so we need to compute <code>exp.(p)</code> to get back the original parameters.</p><pre><code class="language-julia hljs">function predict_adjoint(p)
    p = exp.(p)
    _prob = remake(prob,p=p)
    Array(solve(_prob,Rosenbrock23(autodiff=false),saveat=ts,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))
end

function loss_adjoint(p)
    prediction = predict_adjoint(p)
    prediction = [prediction[:, i] for i in axes(prediction, 2)]
    diff = map((J,u,data) -&gt; J * (abs2.(u .- data)) , Js, prediction, sol.u)
    loss = sum(abs, sum(diff)) |&gt; sqrt
    loss, prediction
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">loss_adjoint (generic function with 1 method)</code></pre><p>The difference between the data and the prediction is weighted by the transformed Jacobian to do a relative scaling of the loss.</p><p>We define a callback function.</p><pre><code class="language-julia hljs">callback = function (p,l,pred) #callback function to observe training
    println(&quot;Loss: $l&quot;)
    println(&quot;Parameters: $(exp.(p))&quot;)
    # using `remake` to re-create our `prob` with current parameters `p`
    plot(solve(remake(prob, p=exp.(p)), Rosenbrock23())) |&gt; display
    return false # Tell it to not halt the optimization. If return true, then optimization stops
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">#7 (generic function with 1 method)</code></pre><p>We then use a combination of <code>ADAM</code> and <code>BFGS</code> to minimize the loss function to accelerate the optimization. The initial guess of the parameters are chosen to be <code>[1, 1, 1.0]</code>.</p><pre><code class="language-julia hljs">initp = ones(3)
# Display the ODE with the initial parameter values.
callback(initp,loss_adjoint(initp)...)

adtype = Optimization.AutoZygote()
optf = Optimization.OptimizationFunction((x,p)-&gt;loss_adjoint(x), adtype)

optprob = Optimization.OptimizationProblem(optf, initp)
res = Optimization.solve(optprob, ADAM(0.01), callback = callback, maxiters = 300)

optprob2 = Optimization.OptimizationProblem(optf, res.u)
res2 = Optimization.solve(optprob2, BFGS(), callback = callback, maxiters = 30, allow_f_increases=true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u: 3-element Vector{Float64}:
 -3.218405574684924
 17.583255324808288
  9.393724721184306</code></pre><p>Finally, we can analyze the difference between the fitted parameters and the ground truth.</p><pre><code class="language-julia hljs">println(&quot;Ground truth: $(p)\nFinal parameters: $(round.(exp.(res2.u), sigdigits=5))\nError: $(round(norm(exp.(res2.u) - p) ./ norm(p) .* 100, sigdigits=3))%&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Ground truth: [0.04, 3.0e7, 10000.0]
Final parameters: [0.040019, 4.3282e7, 12013.0]
Error: 44.3%</code></pre><p>It gives the output</p><pre><code class="nohighlight hljs">Ground truth: [0.04, 3.0e7, 10000.0]
Final parameters: [0.040002, 3.0507e7, 10084.0]
Error: 1.69%</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimization_ode/">« Optimization of Ordinary Differential Equations</a><a class="docs-footer-nextpage" href="../exogenous_input/">Handling Exogenous Input Signals »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Wednesday 27 July 2022 21:02">Wednesday 27 July 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
