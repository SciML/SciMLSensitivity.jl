<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD) · SciMLSensitivity.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://docs.sciml.ai/SciMLSensitivity/stable/manual/differential_equation_sensitivities/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="SciMLSensitivity.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">SciMLSensitivity.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">SciMLSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started with SciMLSensitivity: Differentiating ODE Solutions</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/parameter_estimation_ode/">Parameter Estimation of Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/direct_sensitivity/">Direct Sensitivity Analysis Functionality</a></li><li><a class="tocitem" href="../../tutorials/adjoint_continuous_functional/">Adjoint Sensitivity Analysis of Continuous Functionals</a></li><li><a class="tocitem" href="../../tutorials/data_parallel/">Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching</a></li><li><a class="tocitem" href="../../tutorials/chaotic_ode/">Sensitivity analysis for chaotic systems (shadowing methods)</a></li><li><input class="collapse-toggle" id="menuitem-3-6" type="checkbox"/><label class="tocitem" for="menuitem-3-6"><span class="docs-label">Training Techniques and Tips</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../tutorials/training_tips/local_minima/">Strategies to Avoid Local Minima</a></li><li><a class="tocitem" href="../../tutorials/training_tips/divergence/">Handling Divergent and Unstable Trajectories</a></li><li><a class="tocitem" href="../../tutorials/training_tips/multiple_nn/">Simultaneous Fitting of Multiple Neural Networks</a></li></ul></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Ordinary Differential Equations (ODEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/ode/exogenous_input/">Handling Exogenous Input Signals</a></li><li><a class="tocitem" href="../../examples/ode/prediction_error_method/">Prediction error method (PEM)</a></li><li><a class="tocitem" href="../../examples/ode/second_order_adjoints/">Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis</a></li><li><a class="tocitem" href="../../examples/ode/second_order_neural/">Neural Second Order Ordinary Differential Equation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox"/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Neural Ordinary Differential Equations (Neural ODE)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/neural_ode/neural_ode_flux/">Neural Ordinary Differential Equations with Flux</a></li><li><a class="tocitem" href="../../examples/neural_ode/simplechains/">Neural Ordinary Differential Equations with SimpleChains</a></li><li><a class="tocitem" href="../../examples/neural_ode/neural_gde/">Neural Graph Differential Equations</a></li><li><a class="tocitem" href="../../examples/neural_ode/minibatch/">Training a Neural Ordinary Differential Equation with Mini-Batching</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Stochastic Differential Equations (SDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/sde/optimization_sde/">Optimization of Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../examples/sde/SDE_control/">Controlling Stochastic Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox"/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Delay Differential Equations (DDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/dde/delay_diffeq/">Delay Differential Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Differential-Algebraic Equations (DAEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/dae/physical_constraints/">Enforcing Physical Constraints via Universal Differential-Algebraic Equations</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-6" type="checkbox"/><label class="tocitem" for="menuitem-4-6"><span class="docs-label">Partial Differential Equations (PDEs)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/pde/pde_constrained/">Partial Differential Equation (PDE) Constrained Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-7" type="checkbox"/><label class="tocitem" for="menuitem-4-7"><span class="docs-label">Hybrid and Jump Equations</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/hybrid_jump/hybrid_diffeq/">Training Neural Networks in Hybrid Differential Equations</a></li><li><a class="tocitem" href="../../examples/hybrid_jump/bouncing_ball/">Bouncing Ball Hybrid ODE Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-8" type="checkbox"/><label class="tocitem" for="menuitem-4-8"><span class="docs-label">Bayesian Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/bayesian/turing_bayesian/">Bayesian Estimation of Differential Equations with Probabilistic Programming</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-9" type="checkbox"/><label class="tocitem" for="menuitem-4-9"><span class="docs-label">Optimal and Model Predictive Control</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/optimal_control/optimal_control/">Solving Optimal Control Problems with Universal Differential Equations</a></li><li><a class="tocitem" href="../../examples/optimal_control/feedback_control/">Universal Differential Equations for Neural Feedback Control</a></li></ul></li></ul></li><li><span class="tocitem">Manual and APIs</span><ul><li class="is-active"><a class="tocitem" href>Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a><ul class="internal"><li><a class="tocitem" href="#Using-and-Controlling-Sensitivity-Algorithms-within-AD"><span>Using and Controlling Sensitivity Algorithms within AD</span></a></li><li><a class="tocitem" href="#Choosing-a-Sensitivity-Algorithm"><span>Choosing a Sensitivity Algorithm</span></a></li><li><a class="tocitem" href="#Special-Notes-on-Non-ODE-Differential-Equation-Problems"><span>Special Notes on Non-ODE Differential Equation Problems</span></a></li><li><a class="tocitem" href="#Manual-VJPs"><span>Manual VJPs</span></a></li><li><a class="tocitem" href="#Sensitivity-Algorithms"><span>Sensitivity Algorithms</span></a></li><li><a class="tocitem" href="#Vector-Jacobian-Product-(VJP)-Choices"><span>Vector-Jacobian Product (VJP) Choices</span></a></li><li><a class="tocitem" href="#More-Details-on-Sensitivity-Algorithm-Choices"><span>More Details on Sensitivity Algorithm Choices</span></a></li></ul></li><li><a class="tocitem" href="../nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)</a></li><li><a class="tocitem" href="../direct_forward_sensitivity/">Direct Forward Sensitivity Analysis of ODEs</a></li><li><a class="tocitem" href="../direct_adjoint_sensitivities/">Direct Adjoint Sensitivities of Differential Equations</a></li></ul></li><li><a class="tocitem" href="../../Benchmark/">Benchmarks</a></li><li><a class="tocitem" href="../../sensitivity_math/">Sensitivity Math Details</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual and APIs</a></li><li class="is-active"><a href>Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqSensitivity.jl/blob/master/docs/src/manual/differential_equation_sensitivities.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sensitivity_diffeq"><a class="docs-heading-anchor" href="#sensitivity_diffeq">Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)</a><a id="sensitivity_diffeq-1"></a><a class="docs-heading-anchor-permalink" href="#sensitivity_diffeq" title="Permalink"></a></h1><p>SciMLSensitivity.jl&#39;s high-level interface allows for specifying a sensitivity algorithm (<code>sensealg</code>) to control the method by which <code>solve</code> is differentiated in an automatic differentiation (AD) context by a compatible AD library. The underlying algorithms then use the direct interface methods, like <code>ODEForwardSensitivityProblem</code> and <code>adjoint_sensitivities</code>, to compute the derivatives without requiring the user to do any of the setup.</p><p>Current AD libraries whose calls are captured by the sensitivity system are:</p><ul><li><a href="https://fluxml.ai/Zygote.jl/stable/">Zygote.jl</a></li><li><a href="https://github.com/JuliaDiff/Diffractor.jl">Diffractor.jl</a></li></ul><h2 id="Using-and-Controlling-Sensitivity-Algorithms-within-AD"><a class="docs-heading-anchor" href="#Using-and-Controlling-Sensitivity-Algorithms-within-AD">Using and Controlling Sensitivity Algorithms within AD</a><a id="Using-and-Controlling-Sensitivity-Algorithms-within-AD-1"></a><a class="docs-heading-anchor-permalink" href="#Using-and-Controlling-Sensitivity-Algorithms-within-AD" title="Permalink"></a></h2><p>Take for example this simple differential equation solve on Lotka-Volterra:</p><pre><code class="language-julia hljs">using SciMLSensitivity, OrdinaryDiffEq, Zygote

function fiip(du, u, p, t)
    du[1] = dx = p[1] * u[1] - p[2] * u[1] * u[2]
    du[2] = dy = -p[3] * u[2] + p[4] * u[1] * u[2]
end
p = [1.5, 1.0, 3.0, 1.0];
u0 = [1.0; 1.0];
prob = ODEProblem(fiip, u0, (0.0, 10.0), p)
sol = solve(prob, Tsit5())
loss(u0, p) = sum(solve(prob, Tsit5(), u0 = u0, p = p, saveat = 0.1))
du0, dp = Zygote.gradient(loss, u0, p)</code></pre><p>This will compute the gradient of the loss function &quot;sum of the values of the solution to the ODE at timepoints dt=0.1&quot; using an adjoint method, where <code>du0</code> is the derivative of the loss function with respect to the initial condition and <code>dp</code> is the derivative of the loss function with respect to the parameters.</p><p>Because the gradient is calculated by <code>Zygote.gradient</code> and Zygote.jl is one of the compatible AD libraries, this derivative calculation will be captured by the <code>sensealg</code> system, and one of SciMLSensitivity.jl&#39;s adjoint overloads will be used to compute the derivative. By default, if the <code>sensealg</code> keyword argument is not defined, then a smart polyalgorithm is used to automatically determine the most appropriate method for a given equation.</p><p>Likewise, the <code>sensealg</code> argument can be given to directly control the method by which the derivative is computed. For example:</p><pre><code class="language-julia hljs">function loss(u0, p)
    sum(solve(prob, Tsit5(), u0 = u0, p = p, saveat = 0.1, sensealg = ForwardSensitivity()))
end
du0, dp = Zygote.gradient(loss, u0, p)</code></pre><p>would do reverse-mode automatic differentiation of the loss function, but when reversing over the ODE solve, it would do forward sensitivity analysis to compute the required pullbacks, effectively creating an algorithm that mixes forward and reverse differentiation.</p><h2 id="Choosing-a-Sensitivity-Algorithm"><a class="docs-heading-anchor" href="#Choosing-a-Sensitivity-Algorithm">Choosing a Sensitivity Algorithm</a><a id="Choosing-a-Sensitivity-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-a-Sensitivity-Algorithm" title="Permalink"></a></h2><p>There are two classes of algorithms: the continuous sensitivity analysis methods, and the discrete sensitivity analysis methods (direct automatic differentiation). Generally:</p><ul><li><p><a href="https://arxiv.org/abs/2001.04385">Continuous sensitivity analysis are more efficient while the discrete sensitivity analysis is more stable</a> (full discussion is in the appendix of that paper)</p></li><li><p>Continuous sensitivity analysis methods only support a subset of equations, which currently includes:</p><ul><li>ODEProblem (with mass matrices for differential-algebraic equations (DAEs)</li><li>SDEProblem</li><li>SteadyStateProblem / NonlinearProblem</li></ul></li><li><p>Discrete sensitivity analysis methods only support a subset of algorithms, namely, the pure Julia solvers which are written generically.</p></li></ul><p>For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem, consult our analysis <a href="https://arxiv.org/abs/1812.01892">in this arXiv paper</a>. A general rule of thumb is:</p><ul><li><code>ForwardDiffSensitivity</code> is the fastest for differential equations with small numbers of parameters (&lt;100) and can be used on any differential equation solver that is native Julia. If the chosen ODE solver is incompatible with direct automatic differentiation, <code>ForwardSensitivty</code> may be used instead.</li><li>Adjoint sensitivity analysis is the fastest when the number of parameters is sufficiently large. There are three configurations of note. Using <code>QuadratureAdjoint</code> is the fastest but uses the most memory, <code>BacksolveAdjoint</code> uses the least memory but on very stiff problems it may be unstable and requires many checkpoints, while <code>InterpolatingAdjoint</code> is in the middle, allowing checkpointing to control total memory use.</li><li>The methods which use direct automatic differentiation (<code>ReverseDiffAdjoint</code>, <code>TrackerAdjoint</code>, <code>ForwardDiffSensitivity</code>, and <code>ZygoteAdjoint</code>) support the full range of DifferentialEquations.jl features (SDEs, DDEs, events, etc.), but only work on native Julia solvers.</li><li>For non-ODEs with large numbers of parameters, <code>TrackerAdjoint</code> in out-of-place form may be the best performer on GPUs, and <code>ReverseDiffAdjoint</code></li><li><code>TrackerAdjoint</code> is able to use a <code>TrackedArray</code> form with out-of-place functions <code>du = f(u,p,t)</code> but requires an <code>Array{TrackedReal}</code> form for <code>f(du,u,p,t)</code> mutating <code>du</code>. The latter has much more overhead, and should be avoided if possible. When solving non-ODEs with lots of parameters, using <code>TrackerAdjoint</code> with an out-of-place definition may currently be the best option.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Compatibility with direct automatic differentiation algorithms (<code>ForwardDiffSensitivity</code>, <code>ReverseDiffAdjoint</code>, etc.) can be queried using the <code>SciMLBase.isautodifferentiable(::SciMLAlgorithm)</code> trait function.</p></div></div><p>If the chosen algorithm is a continuous sensitivity analysis algorithm, then an <code>autojacvec</code> argument can be given for choosing how the Jacobian-vector product (<code>J*v</code>) or vector-Jacobian product (<code>J&#39;*v</code>) calculation is computed. For the forward sensitivity methods, <code>autojacvec=true</code> is the most efficient, though <code>autojacvec=false</code> is slightly less accurate but very close in efficiency. For adjoint methods, it&#39;s more complicated and dependent on the way that the user&#39;s <code>f</code> function is implemented:</p><ul><li><code>EnzymeVJP()</code> is the most efficient if it&#39;s applicable on your equation.</li><li>If your function has no branching (no if statements) but uses mutation, <code>ReverseDiffVJP(true)</code> will be the most efficient after Enzyme. Otherwise, <code>ReverseDiffVJP()</code>, but you may wish to proceed with eliminating mutation as without compilation enabled this can be slow.</li><li>If you are on the CPU or GPU and your function is very vectorized and has no mutation, choose <code>ZygoteVJP()</code>.</li><li>Else fallback to <code>TrackerVJP()</code> if Zygote does not support the function.</li></ul><h2 id="Special-Notes-on-Non-ODE-Differential-Equation-Problems"><a class="docs-heading-anchor" href="#Special-Notes-on-Non-ODE-Differential-Equation-Problems">Special Notes on Non-ODE Differential Equation Problems</a><a id="Special-Notes-on-Non-ODE-Differential-Equation-Problems-1"></a><a class="docs-heading-anchor-permalink" href="#Special-Notes-on-Non-ODE-Differential-Equation-Problems" title="Permalink"></a></h2><p>While all of the choices are compatible with ordinary differential equations, specific notices apply to other forms:</p><h3 id="Differential-Algebraic-Equations"><a class="docs-heading-anchor" href="#Differential-Algebraic-Equations">Differential-Algebraic Equations</a><a id="Differential-Algebraic-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Differential-Algebraic-Equations" title="Permalink"></a></h3><p>We note that while all 3 are compatible with index-1 DAEs via the <a href="https://arxiv.org/abs/2001.04385">derivation in the universal differential equations paper</a> (note the reinitialization), we do not recommend <code>BacksolveAdjoint</code> on DAEs because the stiffness inherent in these problems tends to cause major difficulties with the accuracy of the backwards solution due to reinitialization of the algebraic variables.</p><h3 id="Stochastic-Differential-Equations"><a class="docs-heading-anchor" href="#Stochastic-Differential-Equations">Stochastic Differential Equations</a><a id="Stochastic-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Stochastic-Differential-Equations" title="Permalink"></a></h3><p>We note that all of the adjoints except <code>QuadratureAdjoint</code> are applicable to stochastic differential equations.</p><h3 id="Delay-Differential-Equations"><a class="docs-heading-anchor" href="#Delay-Differential-Equations">Delay Differential Equations</a><a id="Delay-Differential-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Delay-Differential-Equations" title="Permalink"></a></h3><p>We note that only the discretize-then-optimize methods are applicable to delay differential equations. Constant lag and variable lag delay differential equation parameters can be estimated, but the lag times themselves are unable to be estimated through these automatic differentiation techniques.</p><h3 id="Hybrid-Equations-(Equations-with-events/callbacks)-and-Jump-Equations"><a class="docs-heading-anchor" href="#Hybrid-Equations-(Equations-with-events/callbacks)-and-Jump-Equations">Hybrid Equations (Equations with events/callbacks) and Jump Equations</a><a id="Hybrid-Equations-(Equations-with-events/callbacks)-and-Jump-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Hybrid-Equations-(Equations-with-events/callbacks)-and-Jump-Equations" title="Permalink"></a></h3><p><code>ForwardDiffSensitivity</code> can differentiate code with callbacks when <code>convert_tspan=true</code>. <code>ForwardSensitivity</code> is incompatible with hybrid equations. The shadowing methods are incompatible with callbacks. All methods based on discrete adjoint sensitivity analysis via automatic differentiation, like <code>ReverseDiffAdjoint</code>, <code>TrackerAdjoint</code>, or <code>QuadratureAdjoint</code> are fully compatible with events. This applies to ODEs, SDEs, DAEs, and DDEs. The continuous adjoint sensitivities <code>BacksolveAdjoint</code>, <code>InterpolatingAdjoint</code>, and <code>QuadratureAdjoint</code> are compatible with events for ODEs. <code>BacksolveAdjoint</code> and <code>InterpolatingAdjoint</code> can also handle events for SDEs. Use <code>BacksolveAdjoint</code> if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point.</p><h2 id="Manual-VJPs"><a class="docs-heading-anchor" href="#Manual-VJPs">Manual VJPs</a><a id="Manual-VJPs-1"></a><a class="docs-heading-anchor-permalink" href="#Manual-VJPs" title="Permalink"></a></h2><p>Note that when defining your differential equation, the vjp can be manually overwritten by providing the <code>AbstractSciMLFunction</code> definition with  a <code>vjp(u,p,t)</code> that returns a tuple <code>f(u,p,t),v-&gt;J*v</code> in the form of <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/">ChainRules.jl</a>. When this is done, the choice of <code>ZygoteVJP</code> will utilize your VJP function during the internal steps of the adjoint. This is useful for models where automatic differentiation may have trouble producing optimal code. This can be paired with <a href="https://docs.sciml.ai/ModelingToolkit/stable/">ModelingToolkit.jl</a> for producing hyper-optimized, sparse, and parallel VJP functions utilizing the automated symbolic conversions.</p><h2 id="Sensitivity-Algorithms"><a class="docs-heading-anchor" href="#Sensitivity-Algorithms">Sensitivity Algorithms</a><a id="Sensitivity-Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Sensitivity-Algorithms" title="Permalink"></a></h2><p>The following algorithm choices exist for <code>sensealg</code>. See <a href="../../sensitivity_math/#sensitivity_math">the sensitivity mathematics page</a> for more details on the definition of the methods.</p><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ForwardSensitivity" href="#SciMLSensitivity.ForwardSensitivity"><code>SciMLSensitivity.ForwardSensitivity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ForwardSensitivity{CS, AD, FDT} &lt;: AbstractForwardSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of continuous forward sensitivity analysis for propagating derivatives by solving the extended ODE. When used within adjoint differentiation (i.e. via Zygote), this will cause forward differentiation of the <code>solve</code> call within the reverse-mode automatic differentiation environment.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ForwardSensitivity(;
                   chunk_size = 0, autodiff = true,
                   diff_type = Val{:central},
                   autojacvec = autodiff,
                   autojacmat = false)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><code>autodiff</code>: Use automatic differentiation in the internal sensitivity algorithm computations. Default is <code>true</code>.</li><li><code>chunk_size</code>: Chunk size for forward mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</li><li><code>autojacvec</code>: Calculate the Jacobian-vector product via automatic differentiation with special seeding.</li><li><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</li></ul><p>Further details:</p><ul><li>If <code>autodiff=true</code> and <code>autojacvec=true</code>, then the one chunk <code>J*v</code> forward-mode directional derivative calculation trick is used to compute the product without constructing the Jacobian (via ForwardDiff.jl).</li><li>If <code>autodiff=false</code> and <code>autojacvec=true</code>, then the numerical direction derivative trick <code>(f(x+epsilon*v)-f(x))/epsilon</code> is used to compute <code>J*v</code> without constructing the Jacobian.</li><li>If <code>autodiff=true</code> and <code>autojacvec=false</code>, then the Jacobian is constructed via chunked forward-mode automatic differentiation (via ForwardDiff.jl).</li><li>If <code>autodiff=false</code> and <code>autojacvec=false</code>, then the Jacobian is constructed via finite differences via FiniteDiff.jl.</li></ul><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s without callbacks (events).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ForwardDiffSensitivity" href="#SciMLSensitivity.ForwardDiffSensitivity"><code>SciMLSensitivity.ForwardDiffSensitivity</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ForwardDiffSensitivity{CS, CTS} &lt;: AbstractForwardSensitivityAlgorithm{CS, Nothing, Nothing}</code></pre><p>An implementation of discrete forward sensitivity analysis through ForwardDiff.jl. When used within adjoint differentiation (i.e. via Zygote), this will cause forward differentiation of the <code>solve</code> call within the reverse-mode automatic differentiation environment.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ForwardDiffSensitivity(; chunk_size = 0, convert_tspan = nothing)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><code>chunk_size</code>: the chunk size used by ForwardDiff for computing the Jacobian, i.e. the number of simultaneous columns computed.</li><li><code>convert_tspan</code>: whether to convert time to also be <code>Dual</code> valued. By default this is <code>nothing</code> which will only convert if callbacks are found. Conversion is required in order to accurately differentiate callbacks (hybrid equations).</li></ul><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> supports any <code>SciMLProblem</code>s, provided that the solver algorithms is <code>SciMLBase.isautodifferentiable</code>. Note that <code>ForwardDiffSensitivity</code> can accurately differentiate code with callbacks only when <code>convert_tspan=true</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.BacksolveAdjoint" href="#SciMLSensitivity.BacksolveAdjoint"><code>SciMLSensitivity.BacksolveAdjoint</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BacksolveAdjoint{CS, AD, FDT, VJP} &lt;: AbstractAdjointSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of adjoint sensitivity analysis using a backwards solution of the ODE. By default, this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot, allowing reduced memory (O(1) memory). Checkpointing stabilization is included for additional numerical stability over the naive implementation.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">BacksolveAdjoint(; chunk_size = 0, autodiff = true,
                 diff_type = Val{:central},
                 autojacvec = nothing,
                 checkpointing = true, noisemixing = false)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><p><code>autodiff</code>: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to <code>true</code>.</p></li><li><p><code>chunk_size</code>: Chunk size for forward-mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</p></li><li><p><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</p></li><li><p><code>autojacvec</code>: Calculate the vector-Jacobian product (<code>J&#39;*v</code>) via automatic differentiation with special seeding. The default is <code>true</code>. The total set of choices are:</p><ul><li><code>false</code>: the Jacobian is constructed via FiniteDiff.jl</li><li><code>true</code>: the Jacobian is constructed via ForwardDiff.jl</li><li><code>TrackerVJP</code>: Uses Tracker.jl for the vjp.</li><li><code>ZygoteVJP</code>: Uses Zygote.jl for the vjp.</li><li><code>EnzymeVJP</code>: Uses Enzyme.jl for the vjp.</li><li><code>ReverseDiffVJP(compile=false)</code>: Uses ReverseDiff.jl for the vjp. <code>compile</code> is a boolean for whether to precompile the tape, which should only be done if there are no branches (<code>if</code> or <code>while</code> statements) in the <code>f</code> function.</li></ul></li><li><p><code>checkpointing</code>: whether checkpointing is enabled for the reverse pass. Defaults to <code>true</code>.</p></li><li><p><code>noisemixing</code>: Handle noise processes that are not of the form <code>du[i] = f(u[i])</code>. For example, to compute the sensitivities of an SDE with diagonal diffusion</p><pre><code class="language-julia hljs">function g_mixing!(du, u, p, t)
    du[1] = p[3] * u[1] + p[4] * u[2]
    du[2] = p[3] * u[1] + p[4] * u[2]
    nothing
end</code></pre><p>correctly, <code>noisemixing=true</code> must be enabled. The default is <code>false</code>.</p></li></ul><p>For more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.</p><p><strong>Applicability of Backsolve and Caution</strong></p><p>When <code>BacksolveAdjoint</code> is applicable, it is a fast method, and requires the least memory. However, one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers. An example of such an equation is the Lorenz equation. Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator, then the backwards solution diverges from the forward solution. As a quick demonstration:</p><pre><code class="language-julia hljs">using Sundials
function lorenz(du, u, p, t)
    du[1] = 10.0 * (u[2] - u[1])
    du[2] = u[1] * (28.0 - u[3]) - u[2]
    du[3] = u[1] * u[2] - (8 / 3) * u[3]
end
u0 = [1.0; 0.0; 0.0]
tspan = (0.0, 100.0)
prob = ODEProblem(lorenz, u0, tspan)
sol = solve(prob, Tsit5(), reltol = 1e-12, abstol = 1e-12)
prob2 = ODEProblem(lorenz, sol[end], (100.0, 0.0))
sol = solve(prob, Tsit5(), reltol = 1e-12, abstol = 1e-12)
@show sol[end] - u0 #[-3.22091, -1.49394, 21.3435]</code></pre><p>Thus, one should check the stability of the backsolve on their type of problem before enabling this method. Additionally, using checkpointing with backsolve can be a low memory way to stabilize it.</p><p>For more details on this topic, see <a href="https://aip.scitation.org/doi/10.1063/5.0060697">Stiff Neural Ordinary Differential Equations</a>.</p><p><strong>Checkpointing</strong></p><p>To improve the numerical stability of the reverse pass, <code>BacksolveAdjoint</code> includes a checkpointing feature. If <code>sol.u</code> is a time series, then whenever a time <code>sol.t</code> is hit while reversing, a callback will replace the reversing ODE portion with <code>sol.u[i]</code>. This nudges the solution back onto the appropriate trajectory and reduces the numerical caused by drift.</p><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s, <code>SDEProblem</code>s, and <code>RODEProblem</code>s. This <code>sensealg</code> supports callback functions (events).</p><p><strong>References</strong></p><p>ODE: Rackauckas, C. and Ma, Y. and Martensen, J. and Warner, C. and Zubov, K. and Supekar, R. and Skinner, D. and Ramadhana, A. and Edelman, A., Universal Differential Equations for Scientific Machine Learning,	arXiv:2001.04385</p><p>Hindmarsh, A. C. and Brown, P. N. and Grant, K. E. and Lee, S. L. and Serban, R. and Shumaker, D. E. and Woodward, C. S., SUNDIALS: Suite of nonlinear and differential/algebraic equation solvers, ACM Transactions on Mathematical Software (TOMS), 31, pp:363–396 (2005)</p><p>Chen, R.T.Q. and Rubanova, Y. and Bettencourt, J. and Duvenaud, D. K., Neural ordinary differential equations. In Advances in neural information processing systems, pp. 6571–6583 (2018)</p><p>Pontryagin, L. S. and Mishchenko, E.F. and Boltyanskii, V.G. and Gamkrelidze, R.V. The mathematical theory of optimal processes. Routledge, (1962)</p><p>Rackauckas, C. and Ma, Y. and Dixit, V. and Guo, X. and Innes, M. and Revels, J. and Nyberg, J. and Ivaturi, V., A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions, arXiv:1812.01892</p><p>DAE: Cao, Y. and Li, S. and Petzold, L. and Serban, R., Adjoint sensitivity analysis for differential-algebraic equations: The adjoint DAE system and its numerical solution, SIAM journal on scientific computing 24 pp: 1076-1089 (2003)</p><p>SDE: Gobet, E. and Munos, R., Sensitivity Analysis Using Ito-Malliavin Calculus and Martingales, and Application to Stochastic Optimal Control, SIAM Journal on control and optimization, 43, pp. 1676-1713 (2005)</p><p>Li, X. and Wong, T.-K. L.and Chen, R. T. Q. and Duvenaud, D., Scalable Gradients for Stochastic Differential Equations, PMLR 108, pp. 3870-3882 (2020), http://proceedings.mlr.press/v108/li20i.html</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.InterpolatingAdjoint" href="#SciMLSensitivity.InterpolatingAdjoint"><code>SciMLSensitivity.InterpolatingAdjoint</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InterpolatingAdjoint{CS, AD, FDT, VJP} &lt;: AbstractAdjointSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products. By default it requires, a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation. When checkpointing is enabled, it will only require the memory to interpolate between checkpoints.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">InterpolatingAdjoint(; chunk_size = 0, autodiff = true,
                     diff_type = Val{:central},
                     autojacvec = nothing,
                     checkpointing = false, noisemixing = false)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><p><code>autodiff</code>: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to <code>true</code>.</p></li><li><p><code>chunk_size</code>: Chunk size for forward-mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</p></li><li><p><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</p></li><li><p><code>autojacvec</code>: Calculate the vector-Jacobian product (<code>J&#39;*v</code>) via automatic differentiation with special seeding. The default is <code>true</code>. The total set of choices are:</p><ul><li><code>false</code>: the Jacobian is constructed via FiniteDiff.jl</li><li><code>true</code>: the Jacobian is constructed via ForwardDiff.jl</li><li><code>TrackerVJP</code>: Uses Tracker.jl for the vjp.</li><li><code>ZygoteVJP</code>: Uses Zygote.jl for the vjp.</li><li><code>EnzymeVJP</code>: Uses Enzyme.jl for the vjp.</li><li><code>ReverseDiffVJP(compile=false)</code>: Uses ReverseDiff.jl for the vjp. <code>compile</code> is a boolean for whether to precompile the tape, which should only be done if there are no branches (<code>if</code> or <code>while</code> statements) in the <code>f</code> function.</li></ul></li><li><p><code>checkpointing</code>: whether checkpointing is enabled for the reverse pass. Defaults to <code>false</code>.</p></li><li><p><code>noisemixing</code>: Handle noise processes that are not of the form <code>du[i] = f(u[i])</code>. For example, to compute the sensitivities of an SDE with diagonal diffusion</p><pre><code class="language-julia hljs">function g_mixing!(du, u, p, t)
    du[1] = p[3] * u[1] + p[4] * u[2]
    du[2] = p[3] * u[1] + p[4] * u[2]
    nothing
end</code></pre><p>correctly, <code>noisemixing=true</code> must be enabled. The default is <code>false</code>.</p></li></ul><p>For more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.</p><p><strong>Checkpointing</strong></p><p>To reduce the memory usage of the reverse pass, <code>InterpolatingAdjoint</code> includes a checkpointing feature. If <code>sol</code> is <code>dense</code>, checkpointing is ignored and the continuous solution is used for calculating <code>u(t)</code> at arbitrary time points. If <code>checkpointing=true</code> and <code>sol</code> is not <code>dense</code>, then dense intervals between <code>sol.t[i]</code> and <code>sol.t[i+1]</code> are reconstructed on-demand for calculating <code>u(t)</code> at arbitrary time points. This reduces the total memory requirement to only the cost of holding the dense solution over the largest time interval (in terms of number of required steps). The total compute cost is no more than double the original forward compute cost.</p><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s, <code>SDEProblem</code>s, and <code>RODEProblem</code>s. This <code>sensealg</code> supports callbacks (events).</p><p><strong>References</strong></p><p>Rackauckas, C. and Ma, Y. and Martensen, J. and Warner, C. and Zubov, K. and Supekar, R. and Skinner, D. and Ramadhana, A. and Edelman, A., Universal Differential Equations for Scientific Machine Learning,	arXiv:2001.04385</p><p>Hindmarsh, A. C. and Brown, P. N. and Grant, K. E. and Lee, S. L. and Serban, R. and Shumaker, D. E. and Woodward, C. S., SUNDIALS: Suite of nonlinear and differential/algebraic equation solvers, ACM Transactions on Mathematical Software (TOMS), 31, pp:363–396 (2005)</p><p>Rackauckas, C. and Ma, Y. and Dixit, V. and Guo, X. and Innes, M. and Revels, J. and Nyberg, J. and Ivaturi, V., A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions, arXiv:1812.01892</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.QuadratureAdjoint" href="#SciMLSensitivity.QuadratureAdjoint"><code>SciMLSensitivity.QuadratureAdjoint</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuadratureAdjoint{CS, AD, FDT, VJP} &lt;: AbstractAdjointSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature. This method requires the dense solution and will ignore saving arguments during the gradient calculation. The tolerances in the constructor control the inner quadrature.</p><p>This method is O(n^3 + p) for stiff / implicit equations (as opposed to the O((n+p)^3) scaling of BacksolveAdjoint and InterpolatingAdjoint), and thus is much more compute efficient. However, it requires holding a dense reverse pass and is thus memory intensive.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">QuadratureAdjoint(; chunk_size = 0, autodiff = true,
                  diff_type = Val{:central},
                  autojacvec = nothing, abstol = 1e-6,
                  reltol = 1e-3)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><p><code>autodiff</code>: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to <code>true</code>.</p></li><li><p><code>chunk_size</code>: Chunk size for forward-mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</p></li><li><p><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</p></li><li><p><code>autojacvec</code>: Calculate the vector-Jacobian product (<code>J&#39;*v</code>) via automatic differentiation with special seeding. The default is <code>true</code>. The total set of choices are:</p><ul><li><code>false</code>: the Jacobian is constructed via FiniteDiff.jl</li><li><code>true</code>: the Jacobian is constructed via ForwardDiff.jl</li><li><code>TrackerVJP</code>: Uses Tracker.jl for the vjp.</li><li><code>ZygoteVJP</code>: Uses Zygote.jl for the vjp.</li><li><code>EnzymeVJP</code>: Uses Enzyme.jl for the vjp.</li><li><code>ReverseDiffVJP(compile=false)</code>: Uses ReverseDiff.jl for the vjp. <code>compile</code> is a boolean for whether to precompile the tape, which should only be done if there are no branches (<code>if</code> or <code>while</code> statements) in the <code>f</code> function.</li></ul></li><li><p><code>abstol</code>: absolute tolerance for the quadrature calculation</p></li><li><p><code>reltol</code>: relative tolerance for the quadrature calculation</p></li></ul><p>For more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.</p><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s. This <code>sensealg</code> supports events (callbacks).</p><p><strong>References</strong></p><p>Rackauckas, C. and Ma, Y. and Martensen, J. and Warner, C. and Zubov, K. and Supekar, R. and Skinner, D. and Ramadhana, A. and Edelman, A., Universal Differential Equations for Scientific Machine Learning,	arXiv:2001.04385</p><p>Hindmarsh, A. C. and Brown, P. N. and Grant, K. E. and Lee, S. L. and Serban, R. and Shumaker, D. E. and Woodward, C. S., SUNDIALS: Suite of nonlinear and differential/algebraic equation solvers, ACM Transactions on Mathematical Software (TOMS), 31, pp:363–396 (2005)</p><p>Rackauckas, C. and Ma, Y. and Dixit, V. and Guo, X. and Innes, M. and Revels, J. and Nyberg, J. and Ivaturi, V., A comparison of automatic differentiation and continuous sensitivity analysis for derivatives of differential equation solutions, arXiv:1812.01892</p><p>Kim, S., Ji, W., Deng, S., Ma, Y., &amp; Rackauckas, C. (2021). Stiff neural ordinary differential equations. Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(9), 093122.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ReverseDiffAdjoint" href="#SciMLSensitivity.ReverseDiffAdjoint"><code>SciMLSensitivity.ReverseDiffAdjoint</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ReverseDiffAdjoint &lt;: AbstractAdjointSensitivityAlgorithm{nothing, true, nothing}</code></pre><p>An implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ReverseDiffAdjoint()</code></pre><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> supports any <code>DEProblem</code> if the algorithm is <code>SciMLBase.isautodifferentiable</code>. Requires that the state variables are CPU-based <code>Array</code> types.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.TrackerAdjoint" href="#SciMLSensitivity.TrackerAdjoint"><code>SciMLSensitivity.TrackerAdjoint</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TrackerAdjoint &lt;: AbstractAdjointSensitivityAlgorithm{nothing, true, nothing}</code></pre><p>An implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">TrackerAdjoint()</code></pre><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> supports any <code>DEProblem</code> if the algorithm is <code>SciMLBase.isautodifferentiable</code> Compatible with a limited subset of <code>AbstractArray</code> types for <code>u0</code>, including <code>CuArrays</code>.</p><div class="admonition is-category-warn"><header class="admonition-header">Warn</header><div class="admonition-body"><p>TrackerAdjoint is incompatible with Stiff ODE solvers using forward-mode automatic differentiation for the Jacobians. Thus, for example, <code>TRBDF2()</code> will error. Instead, use <code>autodiff=false</code>, i.e. <code>TRBDF2(autodiff=false)</code>. This will only remove the forward-mode automatic differentiation of the Jacobian construction, not the reverse-mode AD usage, and thus performance will still be nearly the same, though Jacobian accuracy may suffer which could cause more steps to be required.</p></div></div></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ZygoteAdjoint" href="#SciMLSensitivity.ZygoteAdjoint"><code>SciMLSensitivity.ZygoteAdjoint</code></a> — <span class="docstring-category">Type</span></header><section><div><p>ZygoteAdjoint &lt;: AbstractAdjointSensitivityAlgorithm{nothing,true,nothing}</p><p>An implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ZygoteAdjoint()</code></pre><p><strong>SciMLProblem Support</strong></p><p>Currently fails on almost every solver.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ForwardLSS" href="#SciMLSensitivity.ForwardLSS"><code>SciMLSensitivity.ForwardLSS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ForwardLSS{CS, AD, FDT, RType, gType} &lt;: AbstractShadowingSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of the discrete, forward-mode <a href="https://arxiv.org/abs/1204.0159">least squares shadowing</a> (LSS) method. LSS replaces the ill-conditioned initial value problem (<code>ODEProblem</code>) for chaotic systems by a well-conditioned least-squares problem. This allows for computing sensitivities of long-time averaged quantities with respect to the parameters of the <code>ODEProblem</code>. The computational cost of LSS scales as (number of states x number of time steps). Converges to the correct sensitivity at a rate of <code>T^(-1/2)</code>, where <code>T</code> is the time of the trajectory. See <code>NILSS()</code> and <code>NILSAS()</code> for a more efficient non-intrusive formulation.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ForwardLSS(;
           chunk_size = 0, autodiff = true,
           diff_type = Val{:central},
           LSSregularizer = TimeDilation(10.0, 0.0, 0.0),
           g = nothing)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><p><code>autodiff</code>: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to <code>true</code>.</p></li><li><p><code>chunk_size</code>: Chunk size for forward-mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</p></li><li><p><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</p></li><li><p><code>LSSregularizer</code>: Using <code>LSSregularizer</code>, one can choose between three different regularization routines. The default choice is <code>TimeDilation(10.0,0.0,0.0)</code>.</p><ul><li><code>CosWindowing()</code>: cos windowing of the time grid, i.e. the time grid (saved time steps) is transformed using a cosine.</li><li><code>Cos2Windowing()</code>: cos^2 windowing of the time grid.</li><li><code>TimeDilation(alpha::Number,t0skip::Number,t1skip::Number)</code>: Corresponds to a time dilation. <code>alpha</code> controls the weight. <code>t0skip</code> and <code>t1skip</code> indicate the times truncated at the beginning and end of the trajectory, respectively.</li></ul></li><li><p><code>g</code>: instantaneous objective function of the long-time averaged objective.</p></li></ul><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s. This <code>sensealg</code> does not support events (callbacks). This <code>sensealg</code> assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.</p><p><strong>References</strong></p><p>Wang, Q., Hu, R., and Blonigan, P. Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations. Journal of Computational Physics, 267, 210-224 (2014).</p><p>Wang, Q., Convergence of the Least Squares Shadowing Method for Computing Derivative of Ergodic Averages, SIAM Journal on Numerical Analysis, 52, 156–170 (2014).</p><p>Blonigan, P., Gomez, S., Wang, Q., Least Squares Shadowing for sensitivity analysis of turbulent fluid flows, in: 52nd Aerospace Sciences Meeting, 1–24 (2014).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.AdjointLSS" href="#SciMLSensitivity.AdjointLSS"><code>SciMLSensitivity.AdjointLSS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdjointLSS{CS, AD, FDT, RType, gType} &lt;: AbstractShadowingSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of the discrete, adjoint-mode <a href="https://arxiv.org/abs/1204.0159">least square shadowing</a> method. LSS replaces the ill-conditioned initial value problem (<code>ODEProblem</code>) for chaotic systems by a well-conditioned least-squares problem. This allows for computing sensitivities of long-time averaged quantities with respect to the parameters of the <code>ODEProblem</code>. The computational cost of LSS scales as (number of states x number of time steps). Converges to the correct sensitivity at a rate of <code>T^(-1/2)</code>, where <code>T</code> is the time of the trajectory. See <code>NILSS()</code> and <code>NILSAS()</code> for a more efficient non-intrusive formulation.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">AdjointLSS(;
           chunk_size = 0, autodiff = true,
           diff_type = Val{:central},
           LSSRegularizer = TimeDilation(10.0, 0.0, 0.0),
           g = nothing)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><p><code>autodiff</code>: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to <code>true</code>.</p></li><li><p><code>chunk_size</code>: Chunk size for forward-mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</p></li><li><p><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</p></li><li><p><code>LSSregularizer</code>: Using <code>LSSregularizer</code>, one can choose between different regularization routines. The default choice is <code>TimeDilation(10.0,0.0,0.0)</code>.</p><ul><li><code>TimeDilation(alpha::Number,t0skip::Number,t1skip::Number)</code>: Corresponds to a time dilation. <code>alpha</code> controls the weight. <code>t0skip</code> and <code>t1skip</code> indicate the times truncated at the beginning and end of the trajectory, respectively. The default value for <code>t0skip</code> and <code>t1skip</code> is <code>zero(alpha)</code>.</li></ul></li><li><p><code>g</code>: instantaneous objective function of the long-time averaged objective.</p></li></ul><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s. This <code>sensealg</code> does not support events (callbacks). This <code>sensealg</code> assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.</p><p><strong>References</strong></p><p>Wang, Q., Hu, R., and Blonigan, P. Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations. Journal of Computational Physics, 267, 210-224 (2014).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.NILSS" href="#SciMLSensitivity.NILSS"><code>SciMLSensitivity.NILSS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct NILSS{CS,AD,FDT,RNG,nType,gType} &lt;: AbstractShadowingSensitivityAlgorithm{CS,AD,FDT}</code></pre><p>An implementation of the forward-mode, continuous <a href="https://arxiv.org/abs/1611.00880">non-intrusive least squares shadowing</a> method. <code>NILSS</code> allows for computing sensitivities of long-time averaged quantities with respect to the parameters of an <code>ODEProblem</code> by constraining the computation to the unstable subspace. <code>NILSS</code> employs the continuous-time <code>ForwardSensitivity</code> method as tangent solver. To avoid an exponential blow-up of the (homogeneous and inhomogeneous) tangent solutions, the trajectory should be divided into sufficiently small segments, where the tangent solutions are rescaled on the interfaces. The computational and memory cost of NILSS scale with the number of unstable (positive) Lyapunov exponents (instead of the number of states, as in the LSS method). <code>NILSS</code> avoids the explicit construction of the Jacobian at each time step, and thus should generally be preferred (for large system sizes) over <code>ForwardLSS</code>.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">NILSS(nseg, nstep; nus = nothing,
      rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),
      chunk_size = 0, autodiff = true,
      diff_type = Val{:central},
      autojacvec = autodiff,
      g = nothing)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nseg</code>: Number of segments on full time interval on the attractor.</li><li><code>nstep</code>: number of steps on each segment.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>nus</code>: Dimension of the unstable subspace. Default is <code>nothing</code>. <code>nus</code> must be smaller or equal to the state dimension (<code>length(u0)</code>). With the default choice, <code>nus = length(u0) - 1</code> will be set at compile time.</li><li><code>rng</code>: (Pseudo) random number generator. Used for initializing the homogeneous tangent states (<code>w</code>). Default is <code>Xorshifts.Xoroshiro128Plus(rand(UInt64))</code>.</li><li><code>autodiff</code>: Use automatic differentiation in the internal sensitivity algorithm computations. Default is <code>true</code>.</li><li><code>chunk_size</code>: Chunk size for forward mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</li><li><code>autojacvec</code>: Calculate the Jacobian-vector product via automatic differentiation with special seeding.</li><li><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</li><li><code>g</code>: instantaneous objective function of the long-time averaged objective.</li></ul><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s. This <code>sensealg</code> does not support events (callbacks). This <code>sensealg</code> assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.</p><p><strong>References</strong></p><p>Ni, A., Blonigan, P. J., Chater, M., Wang, Q., Zhang, Z., Sensitivity analy- sis on chaotic dynamical system by Non-Intrusive Least Square Shadowing (NI-LSS), in: 46th AIAA Fluid Dynamics Conference, AIAA AVIATION Forum (AIAA 2016-4399), American Institute of Aeronautics and Astronautics, 1–16 (2016).</p><p>Ni, A., and Wang, Q. Sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Shadowing (NILSS). Journal of Computational Physics 347, 56-77 (2017).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.NILSAS" href="#SciMLSensitivity.NILSAS"><code>SciMLSensitivity.NILSAS</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NILSAS{CS, AD, FDT, RNG, SENSE, gType} &lt;: AbstractShadowingSensitivityAlgorithm{CS, AD, FDT}</code></pre><p>An implementation of the adjoint-mode, continuous <a href="https://arxiv.org/abs/1801.08674">non-intrusive adjoint least squares shadowing</a> method. <code>NILSAS</code> allows for computing sensitivities of long-time averaged quantities with respect to the parameters of an <code>ODEProblem</code> by constraining the computation to the unstable subspace. <code>NILSAS</code> employs SciMLSensitivity.jl&#39;s continuous adjoint sensitivity methods on each segment to compute (homogeneous and inhomogeneous) adjoint solutions. To avoid an exponential blow-up of the adjoint solutions, the trajectory should be divided into sufficiently small segments, where the adjoint solutions are rescaled on the interfaces. The computational and memory cost of NILSAS scale with the number of unstable, adjoint Lyapunov exponents (instead of the number of states as in the LSS method). <code>NILSAS</code> avoids the explicit construction of the Jacobian at each time step, and thus should generally be preferred (for large system sizes) over <code>AdjointLSS</code>. <code>NILSAS</code> is favorable over <code>NILSS</code> for many parameters because NILSAS computes the gradient with respect to multiple parameters with negligible additional cost.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">NILSAS(nseg, nstep, M = nothing; rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),
       adjoint_sensealg = BacksolveAdjoint(autojacvec = ReverseDiffVJP()),
       chunk_size = 0, autodiff = true,
       diff_type = Val{:central},
       g = nothing)</code></pre><p><strong>Arguments</strong></p><ul><li><code>nseg</code>: Number of segments on full time interval on the attractor.</li><li><code>nstep</code>: number of steps on each segment.</li><li><code>M</code>: number of homogeneous adjoint solutions. This number must be bigger or equal than the number of (positive, adjoint) Lyapunov exponents. Default is <code>nothing</code>.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><p><code>rng</code>: (Pseudo) random number generator. Used for initializing the terminate conditions of the homogeneous adjoint states (<code>w</code>). Default is <code>Xorshifts.Xoroshiro128Plus(rand(UInt64))</code>.</p></li><li><p><code>adjoint_sensealg</code>: Continuous adjoint sensitivity method to compute homogeneous and inhomogeneous adjoint solutions on each segment. Default is <code>BacksolveAdjoint(autojacvec=ReverseDiffVJP())</code>.</p><ul><li><p><code>autojacvec</code>: Calculate the vector-Jacobian product (<code>J&#39;*v</code>) via automatic differentiation with special seeding. The default is <code>true</code>. The total set of choices are:</p><ul><li><code>false</code>: the Jacobian is constructed via FiniteDiff.jl</li><li><code>true</code>: the Jacobian is constructed via ForwardDiff.jl</li><li><code>TrackerVJP</code>: Uses Tracker.jl for the vjp.</li><li><code>ZygoteVJP</code>: Uses Zygote.jl for the vjp.</li><li><code>EnzymeVJP</code>: Uses Enzyme.jl for the vjp.</li><li><code>ReverseDiffVJP(compile=false)</code>: Uses ReverseDiff.jl for the vjp. <code>compile</code> is a boolean for whether to precompile the tape, which should only be done if there are no branches (<code>if</code> or <code>while</code> statements) in the <code>f</code> function.</li></ul></li></ul></li><li><p><code>autodiff</code>: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to <code>true</code>.</p></li><li><p><code>chunk_size</code>: Chunk size for forward-mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</p></li><li><p><code>diff_type</code>: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with <code>autodiff=false</code>.</p></li><li><p><code>g</code>: instantaneous objective function of the long-time averaged objective.</p></li></ul><p><strong>SciMLProblem Support</strong></p><p>This <code>sensealg</code> only supports <code>ODEProblem</code>s. This <code>sensealg</code> does not support events (callbacks). This <code>sensealg</code> assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.</p><p><strong>References</strong></p><p>Ni, A., and Talnikar, C., Adjoint sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Adjoint Shadowing (NILSAS). Journal of Computational Physics 395, 690-709 (2019).</p></div></section></article><h2 id="Vector-Jacobian-Product-(VJP)-Choices"><a class="docs-heading-anchor" href="#Vector-Jacobian-Product-(VJP)-Choices">Vector-Jacobian Product (VJP) Choices</a><a id="Vector-Jacobian-Product-(VJP)-Choices-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Jacobian-Product-(VJP)-Choices" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ZygoteVJP" href="#SciMLSensitivity.ZygoteVJP"><code>SciMLSensitivity.ZygoteVJP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZygoteVJP &lt;: VJPChoice</code></pre><p>Uses Zygote.jl to compute vector-Jacobian products. Tends to be the fastest VJP method if the ODE/DAE/SDE/DDE is written with mostly vectorized  functions (like neural networks and other layers from Flux.jl) and the <code>f</code> function is given out-of-place. If the <code>f</code> function is in-place, then <code>Zygote.Buffer</code> arrays are used internally, which can greatly reduce the performance of the VJP method.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ZygoteVJP(; allow_nothing = false)</code></pre><p>Keyword arguments:</p><ul><li><code>allow_nothing</code>: whether <code>nothing</code>s should be implicitly converted to zeros. In Zygote, the derivative of a function with respect to <code>p</code> which does not use <code>p</code> in any possible calculation is given a derivative of <code>nothing</code> instead of zero. By default, this <code>nothing</code> is caught in order to throw an informative error message about a potentially unintentional misdefined function. However, if this was intentional, setting <code>allow_nothing=true</code> will remove the error message.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.EnzymeVJP" href="#SciMLSensitivity.EnzymeVJP"><code>SciMLSensitivity.EnzymeVJP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EnzymeVJP &lt;: VJPChoice</code></pre><p>Uses Enzyme.jl to compute vector-Jacobian products. Is the fastest VJP whenever applicable, though Enzyme.jl currently has low coverage over the Julia programming language, for example restricting the user&#39;s defined <code>f</code> function to not do things like require garbage collection or calls to BLAS/LAPACK. However, mutation is supported, meaning that in-place <code>f</code> with fully mutating non-allocating code will work with Enzyme (provided no high-level calls to C like BLAS/LAPACK are used) and this will be the most efficient adjoint implementation.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">EnzymeVJP(; chunksize = 0)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><code>chunksize</code>: the default chunk size for the temporary variables inside the vjp&#39;s right hand side definition. This is used for compatibility with ODE solves that default to using ForwardDiff.jl for the Jacobian of the stiff ODE solve, such as OrdinaryDiffEq.jl. This should be set to the maximum chunksize that can occur during an integration to preallocate the <code>DualCaches</code> for PreallocationTools.jl. It defaults to 0, using <code>ForwardDiff.pickchunksize</code> but could be decreased if this value is known to be lower to conserve memory.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.TrackerVJP" href="#SciMLSensitivity.TrackerVJP"><code>SciMLSensitivity.TrackerVJP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TrackerVJP &lt;: VJPChoice</code></pre><p>Uses Tracker.jl to compute the vector-Jacobian products. If <code>f</code> is in-place, then it uses a array of structs formulation to do scalarized reverse mode, while if <code>f</code> is out-of-place then it uses an array-based reverse mode.</p><p>Not as efficient as <code>ReverseDiffVJP</code>, but supports GPUs when doing array-based reverse mode.</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">TrackerVJP(; allow_nothing = false)</code></pre><p>Keyword arguments:</p><ul><li><code>allow_nothing</code>: whether non-tracked values should be implicitly converted to zeros. In Tracker, the derivative of a function with respect to <code>p</code> which does not use <code>p</code> in any possible calculation is given an untracked return instead of zero. By default, this <code>nothing</code> Trackedness is caught in order to throw an informative error message about a potentially unintentional misdefined function. However, if this was intentional, setting <code>allow_nothing=true</code> will remove the error message.</li></ul></div></section></article><article class="docstring"><header><a class="docstring-binding" id="SciMLSensitivity.ReverseDiffVJP" href="#SciMLSensitivity.ReverseDiffVJP"><code>SciMLSensitivity.ReverseDiffVJP</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ReverseDiffVJP{compile} &lt;: VJPChoice</code></pre><p>Uses ReverseDiff.jl to compute the vector-Jacobian products. If <code>f</code> is in-place, then it uses a array of structs formulation to do scalarized reverse mode, while if <code>f</code> is out-of-place, then it uses an array-based reverse mode.</p><p>Usually, the fastest when scalarized operations exist in the f function (like in scientific machine learning applications like Universal Differential Equations) and the boolean compilation is enabled (i.e. ReverseDiffVJP(true)), if EnzymeVJP fails on a given choice of <code>f</code>.</p><p>Does not support GPUs (CuArrays).</p><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">ReverseDiffVJP(compile = false)</code></pre><p><strong>Keyword Arguments</strong></p><ul><li><code>compile</code>: Whether to cache the compilation of the reverse tape. This heavily increases the performance of the method, but requires that the <code>f</code> function of the ODE/DAE/SDE/DDE has no branching.</li></ul></div></section></article><h2 id="More-Details-on-Sensitivity-Algorithm-Choices"><a class="docs-heading-anchor" href="#More-Details-on-Sensitivity-Algorithm-Choices">More Details on Sensitivity Algorithm Choices</a><a id="More-Details-on-Sensitivity-Algorithm-Choices-1"></a><a class="docs-heading-anchor-permalink" href="#More-Details-on-Sensitivity-Algorithm-Choices" title="Permalink"></a></h2><p>The following section describes a bit more details to consider when choosing a sensitivity algorithm.</p><h3 id="Optimize-then-Discretize"><a class="docs-heading-anchor" href="#Optimize-then-Discretize">Optimize-then-Discretize</a><a id="Optimize-then-Discretize-1"></a><a class="docs-heading-anchor-permalink" href="#Optimize-then-Discretize" title="Permalink"></a></h3><p><a href="https://arxiv.org/abs/1806.07366">The original neural ODE paper</a> popularized optimize-then-discretize with O(1) adjoints via backsolve. This is the methodology <code>BacksolveAdjoint</code> When training non-stiff neural ODEs, <code>BacksolveAdjoint</code> with <code>ZygoteVJP</code> is generally the fastest method. Additionally, this method does not require storing the values of any intermediate points and is thus the most memory efficient. However, <code>BacksolveAdjoint</code> is prone to instabilities whenever the Lipschitz constant is sufficiently large, like in stiff equations, PDE discretizations, and many other contexts, so it is not used by default. When training a neural ODE for machine learning applications, the user should try <code>BacksolveAdjoint</code> and see if it is sufficiently accurate on their problem. More details on this topic can be found in <a href="https://aip.scitation.org/doi/10.1063/5.0060697">Stiff Neural Ordinary Differential Equations</a></p><p>Note that DiffEqFlux&#39;s implementation of <code>BacksolveAdjoint</code> includes an extra feature <code>BacksolveAdjoint(checkpointing=true)</code> which mixes checkpointing with <code>BacksolveAdjoint</code>. What this method does is that, at <code>saveat</code> points, values from the forward pass are saved. Since the reverse solve should numerically be the same as the forward pass, issues with divergence of the reverse pass are mitigated by restarting the reverse pass at the <code>saveat</code> value from the forward pass. This reduces the divergence and can lead to better gradients at the cost of higher memory usage due to having to save some values of the forward pass. This can stabilize the adjoint in some applications, but for highly stiff applications the divergence can be too fast for this to work in practice.</p><p>To avoid the issues of backwards solving the ODE, <code>InterpolatingAdjoint</code> and <code>QuadratureAdjoint</code> utilize information from the forward pass. By default, these methods utilize the <a href="https://docs.sciml.ai/DiffEqDocs/stable/basics/solution/#Interpolations-1">continuous solution</a> provided by DifferentialEquations.jl in the calculations of the adjoint pass. <code>QuadratureAdjoint</code> uses this to build a continuous function for the solution of the adjoint equation and then performs an adaptive quadrature via <a href="https://docs.sciml.ai/Integrals/stable/">Integrals.jl</a>, while <code>InterpolatingAdjoint</code> appends the integrand to the ODE, so it&#39;s computed simultaneously to the Lagrange multiplier. When memory is not an issue, we find that the <code>QuadratureAdjoint</code> approach tends to be the most efficient as it has a significantly smaller adjoint differential equation and the quadrature converges very fast, but this form requires holding the full continuous solution of the adjoint which can be a significant burden for large parameter problems. The <code>InterpolatingAdjoint</code> is thus a compromise between memory efficiency and compute efficiency, and is in the same spirit as <a href="https://computing.llnl.gov/projects/sundials">CVODES</a>.</p><p>However, if the memory cost of the <code>InterpolatingAdjoint</code> is too high, checkpointing can be used via <code>InterpolatingAdjoint(checkpointing=true)</code>. When this is used, the checkpoints default to <code>sol.t</code> of the forward pass (i.e. the saved timepoints usually set by <code>saveat</code>). Then in the adjoint, intervals of <code>sol.t[i-1]</code> to <code>sol.t[i]</code> are re-solved in order to obtain a short interpolation which can be utilized in the adjoints. This at most results in two full solves of the forward pass, but dramatically reduces the computational cost while being a low-memory format. This is the preferred method for highly stiff equations when memory is an issue, i.e. stiff PDEs or large neural DAEs.</p><p>For forward-mode, the <code>ForwardSensitivty</code> is the version that performs the optimize-then-discretize approach. In this case, <code>autojacvec</code> corresponds to the method for computing <code>J*v</code> within the forward sensitivity equations, which is either <code>true</code> or <code>false</code> for whether to use Jacobian-free forward-mode AD (via ForwardDiff.jl) or Jacobian-free numerical differentiation.</p><h3 id="Discretize-then-Optimize"><a class="docs-heading-anchor" href="#Discretize-then-Optimize">Discretize-then-Optimize</a><a id="Discretize-then-Optimize-1"></a><a class="docs-heading-anchor-permalink" href="#Discretize-then-Optimize" title="Permalink"></a></h3><p>In this approach, the discretization is done first and then optimization is done on the discretized system. While traditionally this can be done discrete sensitivity analysis, this can equivalently be done by automatic differentiation on the solver itself. <code>ReverseDiffAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://juliadiff.org/ReverseDiff.jl/">ReverseDiff.jl</a>, <code>ZygoteAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://fluxml.ai/Zygote.jl/latest/">Zygote.jl</a>, and <code>TrackerAdjoint</code> performs reverse-mode automatic differentiation on the solver via <a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a>. In addition, <code>ForwardDiffSensitivty</code> performs forward-mode automatic differentiation on the solver via <a href="https://juliadiff.org/ForwardDiff.jl/stable/">ForwardDiff.jl</a>.</p><p>We note that many studies have suggested that <a href="https://arxiv.org/abs/2005.13420">this approach produces more accurate gradients than the optimize-than-discretize approach</a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../examples/optimal_control/feedback_control/">« Universal Differential Equations for Neural Feedback Control</a><a class="docs-footer-nextpage" href="../nonlinear_solve_sensitivities/">Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 29 January 2023 02:17">Sunday 29 January 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
